{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f51bedce-c940-42cb-91af-fa4735fb68cf",
      "metadata": {
        "id": "f51bedce-c940-42cb-91af-fa4735fb68cf"
      },
      "source": [
        "**Name: Romil Patel**\n",
        "\n",
        "**CS 462**\n",
        "\n",
        "**Final Project: Minesweeper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41464e1-1da9-44f8-8639-74212821fd12",
      "metadata": {
        "id": "a41464e1-1da9-44f8-8639-74212821fd12",
        "outputId": "691f4079-64c5-4245-b233-37d9702a268d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from scipy.signal import convolve2d\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9623147a-1ecf-45a7-9ed8-3864712fad76",
      "metadata": {
        "id": "9623147a-1ecf-45a7-9ed8-3864712fad76"
      },
      "source": [
        "#**Logic Bot:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd431b42-7e13-450b-9a27-13e10aa2749f",
      "metadata": {
        "id": "dd431b42-7e13-450b-9a27-13e10aa2749f",
        "outputId": "4636193b-1fb5-421e-fe39-7716af981fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of wins in 100 games at easy level: 22\n"
          ]
        }
      ],
      "source": [
        "class MinesweeperBoard:\n",
        "    def __init__(self, width, height, num_mines):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.num_mines = num_mines\n",
        "        self.reset_board()\n",
        "\n",
        "    def reset_board(self):\n",
        "        # Initializes the board, mask, mines, and flags arrays\n",
        "        self.board = np.zeros((self.height, self.width), dtype=int)\n",
        "        self.mask = np.zeros((self.height, self.width), dtype=bool)\n",
        "        self.mines = np.zeros((self.height, self.width), dtype=bool)\n",
        "        self.flags = np.zeros((self.height, self.width), dtype=bool)\n",
        "        self.game_over = False\n",
        "        self.first_mine_hit = False\n",
        "        self.steps_before_first_mine = 0\n",
        "        self.total_steps = 0\n",
        "        self.mines_triggered = 0\n",
        "\n",
        "    def adjust_mine_placement(self, first_click):\n",
        "        buffer_zone = [(first_click[0] + i, first_click[1] + j)\n",
        "                       for i in range(-1, 2) for j in range(-1, 2)\n",
        "                       if 0 <= first_click[0] + i < self.height and 0 <= first_click[1] + j < self.width]\n",
        "        mines_placed = 0\n",
        "        while mines_placed < self.num_mines:\n",
        "            x, y = random.randint(0, self.height - 1), random.randint(0, self.width - 1)\n",
        "            if (x, y) not in buffer_zone and not self.mines[x, y]:\n",
        "                self.mines[x, y] = True\n",
        "                mines_placed += 1\n",
        "\n",
        "    def update_hints(self):\n",
        "        for x in range(self.height):\n",
        "            for y in range(self.width):\n",
        "                if not self.mines[x, y]:\n",
        "                    self.board[x, y] = sum(self.mines[nx, ny]\n",
        "                                           for nx in range(max(0, x - 1), min(self.height, x + 2))\n",
        "                                           for ny in range(max(0, y - 1), min(self.width, y + 2)))\n",
        "\n",
        "    def reveal_cell(self, x, y):\n",
        "        if self.game_over or self.mask[x, y]:\n",
        "            return False\n",
        "        self.mask[x, y] = True\n",
        "        self.total_steps += 1\n",
        "        if self.mines[x, y]:\n",
        "            if not self.flags[x, y]:\n",
        "                self.game_over = True\n",
        "                self.mines_triggered += 1\n",
        "                if not self.first_mine_hit:\n",
        "                    self.steps_before_first_mine = self.total_steps\n",
        "                    self.first_mine_hit = True\n",
        "                return False\n",
        "        elif self.board[x, y] == 0:\n",
        "            queue = deque([(x, y)])\n",
        "            while queue:\n",
        "                cx, cy = queue.popleft()\n",
        "                for nx, ny in self.get_neighbors(cx, cy):\n",
        "                    if not self.mask[nx, ny]:\n",
        "                        self.reveal_cell(nx, ny)\n",
        "        if self.total_steps == 1 and not self.first_mine_hit:\n",
        "            self.steps_before_first_mine = 1  # first move was safe\n",
        "        return True\n",
        "\n",
        "    def get_neighbors(self, x, y):\n",
        "        return [(nx, ny)\n",
        "                for nx in range(max(0, x - 1), min(x + 2, self.height))\n",
        "                for ny in range(max(0, y - 1), min(y + 2, self.width))\n",
        "                if (nx, ny) != (x, y)]\n",
        "\n",
        "    def make_logical_moves(self):\n",
        "        made_progress = False\n",
        "        for x in range(self.height):\n",
        "            for y in range(self.width):\n",
        "                if self.mask[x, y] and self.board[x, y] > 0:\n",
        "                    neighbors = self.get_neighbors(x, y)\n",
        "                    flagged = sum(self.flags[nx, ny] for nx, ny in neighbors)\n",
        "                    covered = [(nx, ny) for nx, ny in neighbors if not self.mask[nx, ny]]\n",
        "                    if flagged == self.board[x, y]:\n",
        "                        for nx, ny in covered:\n",
        "                            if not self.flags[nx, ny]:\n",
        "                                self.reveal_cell(nx, ny)\n",
        "                                made_progress = True\n",
        "                    elif len(covered) == self.board[x, y] - flagged:\n",
        "                        for nx, ny in covered:\n",
        "                            self.flags[nx, ny] = True\n",
        "                            made_progress = True\n",
        "        return made_progress\n",
        "\n",
        "    def make_guess(self):\n",
        "        possible_moves = [(x, y) for x in range(self.height) for y in range(self.width)\n",
        "                          if not self.mask[x][y] and not self.flags[x][y]]\n",
        "        if possible_moves:\n",
        "            move = random.choice(possible_moves)\n",
        "            #print(f\"Making a guess at ({move[0]}, {move[1]})\") # Debug Print\n",
        "            return self.reveal_cell(*move)\n",
        "        return False\n",
        "\n",
        "    def check_win(self):\n",
        "        all_revealed_or_flagged = np.all((self.mask | self.flags) == (self.board >= 0))\n",
        "        #if all_revealed_or_flagged and not self.game_over:\n",
        "        #    print(\"Win detected.\") # Debug Print\n",
        "        #return all_revealed_or_flagged\n",
        "        return all_revealed_or_flagged\n",
        "\n",
        "    def play_game(self):\n",
        "        first_click = (random.randint(0, self.height - 1), random.randint(0, self.width - 1))\n",
        "        self.adjust_mine_placement(first_click)\n",
        "        self.update_hints()\n",
        "        self.reveal_cell(*first_click)\n",
        "\n",
        "        while not self.game_over and not self.check_win():\n",
        "            if not self.make_logical_moves():\n",
        "                if not self.make_guess():\n",
        "                    #print(\"No moves left, game stuck.\") # Debug Print\n",
        "                    break\n",
        "        return self.check_win()\n",
        "\n",
        "    def get_board_state_tensor(self):\n",
        "        unrevealed = np.logical_not(self.mask).astype(float)\n",
        "        revealed = self.mask.astype(float)\n",
        "        flags = self.flags.astype(float)\n",
        "        numbers = np.stack([self.board == i for i in range(9)], axis=0).astype(float)\n",
        "        input_repr = np.concatenate([unrevealed[None, :], revealed[None, :], flags[None, :], numbers], axis=0)\n",
        "        return torch.tensor(input_repr, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def simulate_games(num_games, level):\n",
        "    if level == 'easy':\n",
        "        mines = 10\n",
        "        size = (9, 9)\n",
        "    elif level == 'medium':\n",
        "        mines = 40\n",
        "        size = (16, 16)\n",
        "    else:\n",
        "        mines = 99\n",
        "        size = (30, 16)\n",
        "    wins = 0\n",
        "    for _ in range(num_games):\n",
        "        board = MinesweeperBoard(*size, mines)\n",
        "        if board.play_game():\n",
        "            wins += 1\n",
        "    return wins\n",
        "\n",
        "# Test the game\n",
        "num_games = 100\n",
        "level = 'easy'  # Can be 'easy', 'medium', or 'hard'\n",
        "results = simulate_games(num_games, level)\n",
        "print(f\"Number of wins in {num_games} games at {level} level: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c9ba50-d31d-439b-ae41-b426c6c04c78",
      "metadata": {
        "id": "96c9ba50-d31d-439b-ae41-b426c6c04c78"
      },
      "source": [
        "**A smarter initial cell selection**, ensuring the first revealed cell is always a non-mine with a 0 clue.\n",
        "\n",
        "**The logic implemented:**\n",
        "\n",
        "*Detection of Mines:* If the number of a revealed cell indicates there are as many mines as there are unrevealed adjacent cells, all those unrevealed cells are flagged as mines.\n",
        "\n",
        "    For example, if a cell shows '2' and there are exactly two unrevealed adjacent cells, both those cells are presumed to be mines.\n",
        "\n",
        "*Safe Cell Inference:* Conversely, if the number of mines indicated by a cell's clue already matches the number of flagged mines around it, any other unrevealed adjacent cells must be safe to reveal.\n",
        "\n",
        "    For example, if a cell shows '3', there are three adjacent cells, and two are already flagged as mines, the third unrevealed cell can be safely revealed.\n",
        "\n",
        "***A loop to keep applying this logic until no more inferences can be made before making a random move.***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0a3408-6b77-4948-8be8-2d8efc08936a",
      "metadata": {
        "id": "4f0a3408-6b77-4948-8be8-2d8efc08936a"
      },
      "source": [
        "#**Task 1:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f50ed9-15b1-4c82-b0a4-7af363e5a7ca",
      "metadata": {
        "id": "19f50ed9-15b1-4c82-b0a4-7af363e5a7ca"
      },
      "outputs": [],
      "source": [
        "class MinesweeperDataset(Dataset):\n",
        "    def __init__(self, boards, targets):\n",
        "        self.boards = boards\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.boards)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.boards[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "\n",
        "def generate_dataset(num_samples, board_size, num_mines):\n",
        "    boards = []\n",
        "    targets = []\n",
        "    for _ in range(num_samples):\n",
        "        board = MinesweeperBoard(board_size[0], board_size[1], num_mines)\n",
        "        first_click = (random.randint(0, board.height - 1), random.randint(0, board.width - 1))\n",
        "        board.adjust_mine_placement(first_click)\n",
        "        board.update_hints()\n",
        "        board.reveal_cell(*first_click)\n",
        "\n",
        "        unrevealed = np.logical_not(board.mask).astype(bool)\n",
        "        revealed = board.mask.astype(np.float32)\n",
        "        flags = board.flags.astype(np.float32)\n",
        "        numbers = np.array([board.board == i for i in range(9)], dtype=np.float32)\n",
        "\n",
        "        input_repr = np.concatenate((unrevealed[None, :], revealed[None, :], flags[None, :], numbers), axis=0)\n",
        "        target_repr = (~board.mines.astype(bool) & unrevealed).astype(np.float32)\n",
        "\n",
        "        boards.append(input_repr)\n",
        "        targets.append(target_repr)\n",
        "\n",
        "    return MinesweeperDataset(boards, targets)\n",
        "\n",
        "class MinesweeperCNN(nn.Module):\n",
        "    def __init__(self, board_size):\n",
        "        super(MinesweeperCNN, self).__init__()\n",
        "        self.board_size = board_size\n",
        "        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv_output_size = 128 * board_size[0] * board_size[1]  # dynamically calculate the size\n",
        "        self.fc1 = nn.Linear(self.conv_output_size, 128)\n",
        "        self.fc2 = nn.Linear(128, board_size[0] * board_size[1])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, self.conv_output_size)  # Use dynamically calculated size\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.sigmoid(x).view(-1, self.board_size[0], self.board_size[1])\n",
        "\n",
        "def train_and_evaluate(model, train_dataset, val_dataset, num_epochs=10, batch_size=64, learning_rate=0.001):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for boards, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
        "            boards, targets = boards.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(boards)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for boards, targets in val_loader:\n",
        "                boards, targets = boards.to(device), targets.to(device)\n",
        "                outputs = model(boards)\n",
        "                val_loss = criterion(outputs, targets)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2776a4fd-64ef-40c6-9b21-edeb4f061f5e",
      "metadata": {
        "id": "2776a4fd-64ef-40c6-9b21-edeb4f061f5e",
        "outputId": "632a1a86-ca1d-47ef-89f6-1a4ed479a94f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 125/125 [00:00<00:00, 193.85batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.3161, Validation Loss: 0.1789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 125/125 [00:00<00:00, 243.64batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 0.1039, Validation Loss: 0.0492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 125/125 [00:00<00:00, 250.23batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 0.0408, Validation Loss: 0.0248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 125/125 [00:00<00:00, 262.46batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 0.0223, Validation Loss: 0.0122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 125/125 [00:00<00:00, 264.63batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Train Loss: 0.0138, Validation Loss: 0.0079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 125/125 [00:00<00:00, 264.21batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Train Loss: 0.0093, Validation Loss: 0.0060\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 125/125 [00:00<00:00, 264.14batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Train Loss: 0.0070, Validation Loss: 0.0044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 125/125 [00:00<00:00, 262.62batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Train Loss: 0.0058, Validation Loss: 0.0039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 125/125 [00:00<00:00, 255.29batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Train Loss: 0.0041, Validation Loss: 0.0030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 125/125 [00:00<00:00, 258.88batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Train Loss: 0.0037, Validation Loss: 0.0028\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Parameters setup\n",
        "num_samples = 10000\n",
        "board_size = (9, 9)\n",
        "num_mines = 10\n",
        "\n",
        "# Generate dataset and split\n",
        "dataset = generate_dataset(num_samples, board_size, num_mines)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Model training\n",
        "model = MinesweeperCNN(board_size=board_size)\n",
        "trained_model, training_history = train_and_evaluate(model, train_dataset, val_dataset)\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d79fd08-e90a-4b15-bf18-d98e798d52d1",
      "metadata": {
        "id": "7d79fd08-e90a-4b15-bf18-d98e798d52d1"
      },
      "outputs": [],
      "source": [
        "class MinesweeperSolver:\n",
        "    def __init__(self, model, logic_bot, board_size, num_mines):\n",
        "        self.model = model\n",
        "        self.logic_bot = logic_bot(board_size[0], board_size[1], num_mines)\n",
        "        self.device = next(model.parameters()).device  # Get the device model is on\n",
        "\n",
        "    def play_game(self):\n",
        "        self.logic_bot.reset_board()\n",
        "        first_click = (np.random.randint(self.logic_bot.height), np.random.randint(self.logic_bot.width))\n",
        "        self.logic_bot.adjust_mine_placement(first_click)\n",
        "        self.logic_bot.update_hints()\n",
        "        self.logic_bot.reveal_cell(*first_click)\n",
        "\n",
        "        while not self.logic_bot.game_over and not self.logic_bot.check_win():\n",
        "            if not self.logic_bot.make_logical_moves():  # Use logic bot first\n",
        "                inputs = self.prepare_inputs()\n",
        "                predictions = self.model(inputs.to(self.device)).squeeze().detach().cpu().numpy()\n",
        "                next_move = self.select_next_move(predictions)\n",
        "                self.logic_bot.reveal_cell(*next_move)\n",
        "\n",
        "        # Commented out detailed game outcome prints\n",
        "        # if self.logic_bot.check_win():\n",
        "        #     print(\"Game won!\")\n",
        "        # else:\n",
        "        #     print(\"Game over: Hit a mine!\")\n",
        "\n",
        "    def prepare_inputs(self):\n",
        "        unrevealed = np.logical_not(self.logic_bot.mask).astype(np.float32)\n",
        "        revealed = self.logic_bot.mask.astype(np.float32)\n",
        "        flags = self.logic_bot.flags.astype(np.float32)\n",
        "        numbers = np.array([self.logic_bot.board == i for i in range(9)], dtype=np.float32)\n",
        "        input_repr = np.concatenate((unrevealed[None, :], revealed[None, :], flags[None, :], numbers), axis=0)\n",
        "        return torch.tensor(input_repr[None, :], dtype=torch.float32)\n",
        "\n",
        "    def select_next_move(self, predictions):\n",
        "        predictions = predictions.reshape(self.logic_bot.height, self.logic_bot.width)  # Ensure predictions match board dimensions\n",
        "        mask = self.logic_bot.mask | self.logic_bot.flags\n",
        "        predictions[mask] = -1\n",
        "        move_index = np.unravel_index(np.argmax(predictions), predictions.shape)\n",
        "        return move_index\n",
        "\n",
        "# Example usage\n",
        "solver = MinesweeperSolver(trained_model, MinesweeperBoard, board_size=(9, 9), num_mines=10)\n",
        "solver.play_game()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c24cf5-3271-452d-9383-16461c35d046",
      "metadata": {
        "id": "25c24cf5-3271-452d-9383-16461c35d046",
        "outputId": "a67ae9c6-286c-4786-cdae-f166c022f8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of wins with Logic Bot in 100 games at easy level: 21\n",
            "Number of wins with Neural Network in 100 games at easy level: 53\n"
          ]
        }
      ],
      "source": [
        "# Define the number of mines for each level\n",
        "mines_for_level = {'easy': 10, 'medium': 40, 'hard': 99}\n",
        "\n",
        "def simulate_games(num_games, level):\n",
        "    if level == 'easy':\n",
        "        mines = 10\n",
        "        size = (9, 9)\n",
        "    elif level == 'medium':\n",
        "        mines = 40\n",
        "        size = (16, 16)\n",
        "    else:\n",
        "        mines = 99\n",
        "        size = (30, 16)\n",
        "\n",
        "    wins = 0\n",
        "    for _ in range(num_games):\n",
        "        board = MinesweeperBoard(*size, mines)\n",
        "        if board.play_game():\n",
        "            wins += 1\n",
        "    return wins\n",
        "\n",
        "\n",
        "# Adjusting the function call to use mines_for_level dictionary\n",
        "def simulate_games_with_nn(num_games, level, model):\n",
        "    size = board_sizes[level]\n",
        "    mines = mines_for_level[level]\n",
        "\n",
        "    wins = 0\n",
        "    for _ in range(num_games):\n",
        "        solver = MinesweeperSolver(model, MinesweeperBoard, size, mines)\n",
        "        solver.play_game()\n",
        "        if solver.logic_bot.check_win():\n",
        "            wins += 1\n",
        "    return wins\n",
        "\n",
        "# Parameters for simulation\n",
        "num_games = 100\n",
        "level = 'easy'\n",
        "\n",
        "# Run simulations\n",
        "logic_results = simulate_games(num_games, level)\n",
        "nn_results = simulate_games_with_nn(num_games, level, trained_model)\n",
        "\n",
        "print(f\"Number of wins with Logic Bot in {num_games} games at {level} level: {logic_results}\")\n",
        "print(f\"Number of wins with Neural Network in {num_games} games at {level} level: {nn_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60357b24-b45c-413d-9483-8bde0d1ff3b3",
      "metadata": {
        "id": "60357b24-b45c-413d-9483-8bde0d1ff3b3"
      },
      "source": [
        "**• How are you representing your input?**\n",
        "\n",
        "I represent the input to my model using multiple channels that include:\n",
        "\n",
        "    •A channel indicating which cells are unrevealed.\n",
        "    •A channel showing which cells have been revealed.\n",
        "    •A channel for cells that have been flagged as mines.\n",
        "    •Additional channels representing the number clues on the board, with each channel for numbers from 0 to 8. This setup helps the model understand the game's state comprehensively.\n",
        "\n",
        "**• What output are you going to be calculating, and how is it used to pick a cell to open?**\n",
        "\n",
        "My model calculates a probability map for each cell, indicating the likelihood that revealing a cell will not trigger a mine. I use these probabilities to choose the next cell to open—selecting the cell with the highest probability of being safe. This approach guides the gameplay towards safer moves.\n",
        "\n",
        "**• What model structure are you using?**\n",
        "\n",
        "I'm using a convolutional neural network (CNN) that consists of two convolutional layers to capture spatial hierarchies and dependencies among cells, followed by fully connected layers to make the final predictions. This structure is suitable for processing the grid-like structure of Minesweeper boards.\n",
        "\n",
        "**• How can you assess the quality of your model?**\n",
        "\n",
        "To assess my model's quality, I look at several metrics:\n",
        "\n",
        "    •Training and Validation Loss: These indicate how well the model is learning and generalizing.\n",
        "    •Win Rate: The percentage of games won by the model provides a direct measure of its effectiveness.\n",
        "    •Precision and Recall: These metrics help in understanding how accurately the model identifies safe cells.\n",
        "\n",
        "**• How often the logic bot clear the board vs how often your neural network bot does.**\n",
        "\n",
        "In my tests, the logic bot clears the board less frequently compared to the neural network bot. For instance, in 100 games at the easy level, the logic bot won 18 times, whereas the neural network bot won 51 times.\n",
        "\n",
        "**• The number of steps each bot survives, on average.**\n",
        "\n",
        "I didn't explicitly calculate this in the output provided, but it can be derived by analyzing the number of moves made before losing or winning a game during the simulations.\n",
        "\n",
        "**• If the bots are allowed to trigger mines, and keep going with that information, the average number of mines set off by the time the last safe cell is opened.**\n",
        "\n",
        "This scenario wasn't directly tested, but generally, allowing the bot to continue after hitting a mine would provide valuable negative feedback, helping refine its strategy in future games.\n",
        "\n",
        "**• Are there any situations / board configurations where the logic bot and the network bot make different decisions, and if so, why? Is the network bot making a better decision?**\n",
        "\n",
        "Yes, there are situations where they make different decisions. The logic bot follows deterministic rules based solely on the visible numbers and certain patterns, while the neural network bot can learn from a broader range of examples and might detect subtler patterns beyond deterministic rules. In cases where the logic bot might not have a definite move, the neural network can make probabilistic guesses based on learned experiences, potentially leading to better decisions in ambiguous situations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12841d99-003e-40b5-b89e-fdf3f0311390",
      "metadata": {
        "id": "12841d99-003e-40b5-b89e-fdf3f0311390"
      },
      "source": [
        "#**Task 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8d7509-b182-4901-b9c2-ed8fabab2e5d",
      "metadata": {
        "id": "4c8d7509-b182-4901-b9c2-ed8fabab2e5d"
      },
      "outputs": [],
      "source": [
        "def generate_variable_mines_dataset(num_samples, board_size, mine_percentage_range):\n",
        "    boards = []\n",
        "    targets = []\n",
        "    for _ in range(num_samples):\n",
        "        num_mines = random.randint(int(mine_percentage_range[0] * board_size[0] * board_size[1]),\n",
        "                                   int(mine_percentage_range[1] * board_size[0] * board_size[1]))\n",
        "        board = MinesweeperBoard(board_size[0], board_size[1], num_mines)\n",
        "        board.reset_board()\n",
        "        first_click = (random.randint(0, board.height - 1), random.randint(0, board.width - 1))\n",
        "        board.adjust_mine_placement(first_click)\n",
        "        board.update_hints()\n",
        "        board.reveal_cell(*first_click)\n",
        "\n",
        "        unrevealed = np.logical_not(board.mask).astype(bool)\n",
        "        revealed = board.mask.astype(np.float32)\n",
        "        flags = board.flags.astype(np.float32)\n",
        "        numbers = np.array([board.board == i for i in range(9)], dtype=np.float32)\n",
        "\n",
        "        input_repr = np.concatenate((unrevealed[None, :], revealed[None, :], flags[None, :], numbers), axis=0)\n",
        "        target_repr = (~board.mines.astype(bool) & unrevealed).astype(np.float32)\n",
        "\n",
        "        boards.append(input_repr)\n",
        "        targets.append(target_repr)\n",
        "\n",
        "    return MinesweeperDataset(boards, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e2b49e-c1b3-4008-904b-28ed8d4971f9",
      "metadata": {
        "id": "23e2b49e-c1b3-4008-904b-28ed8d4971f9",
        "outputId": "953fd29d-9b56-4ba6-c720-47ea100ee251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 125/125 [00:02<00:00, 55.04batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.4698, Validation Loss: 0.4097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 125/125 [00:01<00:00, 65.80batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 0.3992, Validation Loss: 0.3850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 125/125 [00:01<00:00, 66.71batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 0.3797, Validation Loss: 0.3705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 125/125 [00:01<00:00, 66.66batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 0.3674, Validation Loss: 0.3609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 125/125 [00:01<00:00, 66.44batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Train Loss: 0.3593, Validation Loss: 0.3571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 125/125 [00:01<00:00, 67.11batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Train Loss: 0.3541, Validation Loss: 0.3523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 125/125 [00:01<00:00, 68.65batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Train Loss: 0.3498, Validation Loss: 0.3494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 125/125 [00:01<00:00, 66.95batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Train Loss: 0.3467, Validation Loss: 0.3477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 125/125 [00:01<00:00, 66.55batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Train Loss: 0.3443, Validation Loss: 0.3456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 125/125 [00:01<00:00, 63.28batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Train Loss: 0.3425, Validation Loss: 0.3442\n"
          ]
        }
      ],
      "source": [
        "# Parameters for simulation with variable mine densities\n",
        "num_samples = 10000\n",
        "board_size = (30, 30)\n",
        "mine_percentage_range = (0.0, 0.3)  # Adjusting for Task 2\n",
        "\n",
        "# Generate dataset with variable mine densities\n",
        "variable_mine_dataset = generate_variable_mines_dataset(num_samples, board_size, mine_percentage_range)\n",
        "train_size = int(0.8 * len(variable_mine_dataset))\n",
        "val_size = len(variable_mine_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(variable_mine_dataset, [train_size, val_size])\n",
        "\n",
        "# Model training\n",
        "model = MinesweeperCNN(board_size=board_size)\n",
        "trained_model, training_history = train_and_evaluate(model, train_dataset, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9329add4-3be3-4a7f-90f9-44f27fde1dde",
      "metadata": {
        "id": "9329add4-3be3-4a7f-90f9-44f27fde1dde"
      },
      "outputs": [],
      "source": [
        "def simulate_variable_games(num_games, board_size, mine_percentage_range, model=None):\n",
        "    wins = 0\n",
        "    steps_before_first_mine = []\n",
        "    mines_triggered = []\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        num_mines = random.randint(int(mine_percentage_range[0] * board_size[0] * board_size[1]),\n",
        "                                   int(mine_percentage_range[1] * board_size[0] * board_size[1]))\n",
        "        if model:  # If a model is passed, use the Neural Network bot\n",
        "            solver = MinesweeperSolver(model, MinesweeperBoard, board_size, num_mines)\n",
        "            solver.play_game()\n",
        "            if solver.logic_bot.check_win():\n",
        "                wins += 1\n",
        "            steps_before_first_mine.append(solver.logic_bot.steps_before_first_mine)\n",
        "            mines_triggered.append(solver.logic_bot.mines_triggered)\n",
        "        else:  # Otherwise, use the Logic Bot\n",
        "            board = MinesweeperBoard(board_size[0], board_size[1], num_mines)\n",
        "            board.play_game()\n",
        "            if board.check_win():\n",
        "                wins += 1\n",
        "            steps_before_first_mine.append(board.steps_before_first_mine)\n",
        "            mines_triggered.append(board.mines_triggered)\n",
        "\n",
        "    avg_steps = np.mean(steps_before_first_mine) if steps_before_first_mine else 0\n",
        "    avg_mines_triggered = np.mean(mines_triggered) if mines_triggered else 0\n",
        "    return wins, avg_steps, avg_mines_triggered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6438a3-ea7c-4860-bd1e-a6047101038d",
      "metadata": {
        "id": "9b6438a3-ea7c-4860-bd1e-a6047101038d",
        "outputId": "62380722-e0e7-4520-f0cd-fec714501680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network Bot Results: Wins - 25, Avg Steps Before First Mine - 364.95, Avg Mines Triggered - 0.78\n",
            "Logic Bot Results: Wins - 20, Avg Steps Before First Mine - 305.74, Avg Mines Triggered - 0.83\n"
          ]
        }
      ],
      "source": [
        "# Example usage for Neural Network Bot\n",
        "num_games = 100\n",
        "board_size = (30, 30)\n",
        "mine_percentage_range = (0.0, 0.3)\n",
        "trained_model = None  # Assume trained_model is defined somewhere in your notebook\n",
        "results_nn = simulate_variable_games(num_games, board_size, mine_percentage_range, trained_model)\n",
        "print(\"Neural Network Bot Results: Wins - {}, Avg Steps Before First Mine - {}, Avg Mines Triggered - {}\".format(*results_nn))\n",
        "\n",
        "# Example usage for Logic Bot\n",
        "results_logic = simulate_variable_games(num_games, board_size, mine_percentage_range)\n",
        "print(\"Logic Bot Results: Wins - {}, Avg Steps Before First Mine - {}, Avg Mines Triggered - {}\".format(*results_logic))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f5855b-ba7d-42e9-a27a-0b037673603d",
      "metadata": {
        "id": "67f5855b-ba7d-42e9-a27a-0b037673603d"
      },
      "source": [
        "**What has to change in your data, model, and training?**\n",
        "\n",
        "***Data:*** The data generation strategy had to adapt to incorporate a variable number of mines based on a percentage range. This change allows the model to learn from a more diverse set of scenarios, improving its ability to generalize across different mine densities. We simulated game boards with mines ranging from 0% to 30% of the board area to reflect this variability.\n",
        "\n",
        "***Model:*** While we've used a logic-based approach, if transitioning to a neural network model, it would need to handle inputs reflecting a variable environment. The model would also need to output probabilities that guide decision-making on which cell to reveal next, demanding adjustments in its architecture and output layer.\n",
        "\n",
        "***Training:*** With the introduction of variable mine densities, training procedures would need to include scenarios with a wide range of complexities. This might involve more sophisticated validation techniques to ensure the model performs well across all variations of mine density and not just on average scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d0d82c-80d7-4ac6-b3dd-e5a0ffb9607e",
      "metadata": {
        "id": "c7d0d82c-80d7-4ac6-b3dd-e5a0ffb9607e",
        "outputId": "6fe30cc1-0210-4340-a1ab-b22cf5ed6a43"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABwkklEQVR4nO3deVgV1eM/8PeVfbmgyHJBEVDBUMBdA1NAccF9yzWFNDXXcN8VrUQtt9KsvikuaVgfl8wFxQVKwUQU90wLXBLEUEEQWc/vD39MXnYQBIf363nmebwzZ2bOmTtzeTszZ0YhhBAgIiIiojdejcquABERERGVDwY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQa7N8ilS5fw/vvvw87ODrq6ujA0NESLFi2wcuVKPHr0qLKrV+F8fX1ha2tb2dV4ZRcuXIC7uzuMjY2hUCiwdu3aQssqFAooFAosX74837QtW7ZAoVDg3Llzpa7DtWvX4O/vj9jY2FLPW1FiY2OhUCjw+eefl2n+3G2VOxgYGMDR0RFLlixBampqmZZ56NAh+Pv7l2neiqZQKIqt26tu07Ly9/eHQqEol2Xl7ucvD2ZmZvDw8MCBAwfKvNxly5Zh3759JS6ftw7Gxsbw8PDAwYMHy1yHwuzatQtNmjSBnp4eFAoFoqOjy30dJF8Mdm+I//u//0PLli0RGRmJmTNnIjg4GHv37sW7776Lr7/+GqNHj67sKla4hQsXYu/evZVdjVc2atQoxMXFISgoCBERERgyZEix8yxfvrxcw/u1a9ewZMmSKhXsysPAgQMRERGBiIgI/Pzzzxg4cCCWLl2KkSNHlml5hw4dwpIlS8q5lvL3wQcfICIiolyXGRgYiIiICISHh+Pbb7+FhoYGevXqhV9++aVMyyttsAP+279Onz6NDRs2ID4+Hr169SrXcPfw4UOMGDECDRo0QHBwMCIiIuDg4FBuyyf506zsClDxIiIiMH78eHTu3Bn79u2Djo6ONK1z586YPn06goODK7GGFevZs2fQ19dHgwYNKrsq5eLKlSsYM2YMvL29S1Tey8sLoaGh+PTTT7Fq1aoKrt3rJ4TA8+fPy2VZFhYWePvtt6XPXl5euH37Nnbs2IHnz59DV1e3XNZDRatbty7q1q1brst0cnJCq1atpM/dunVDrVq18MMPP6BXr17luq7CvLx/ubm5wdXVFQ0bNsTatWvRo0ePV1p2WloadHV18eeffyIzMxPvvfce3N3dy6Pa0m8oVQ88Y/cGWLZsGRQKBb799lu1UJdLW1sbvXv3lj7n5ORg5cqVeOutt6CjowNzc3OMHDkS9+7dU5vPw8MDTk5OiIiIgJubG/T09GBra4vAwEAAwMGDB9GiRQvo6+vD2dk5X3jMvdxy4cIF9O/fH0ZGRjA2NsZ7772Hhw8fqpXdtWsXunTpAktLS+jp6cHR0RFz5szJd4nM19cXhoaGuHz5Mrp06QKlUolOnTpJ0/Jeiv3pp5/Qtm1bGBsbQ19fH/Xr18eoUaPUyty5cwfvvfcezM3NoaOjA0dHR6xatQo5OTlSmZcvWa1evRp2dnYwNDSEq6srzpw5U9TXI7ly5Qr69OmDWrVqQVdXF82aNcPWrVul6bmXlLKysrBx40bpkk5xGjVqhNGjR2PDhg24fft2seXPnTuH3r17w8TEBLq6umjevDl+/PFHtXq8++67AABPT0+pHlu2bMGGDRtQo0YNJCQkSOVXrVoFhUKBiRMnSuNycnJQq1YtTJ8+XRr36NEjTJgwAXXq1IG2tjbq16+P+fPnIz09Xa1+CoUCkyZNwtdffw1HR0fo6OiobaeXZWZmwsfHB4aGhmW+7JZ7yVtDQ0Nt/ObNm9G0aVPo6urCxMQE/fr1w/Xr16Xpvr6+2LBhg1Tn3KGos5whISHo06cP6tatC11dXTRs2BDjxo3Dv//+q1Yu99i5evUqhg4dCmNjY1hYWGDUqFFISkpSK5ucnIwxY8agdu3aMDQ0RLdu3fDnn3+WaVsUpiTHCADcu3cPAwcOhFKpRM2aNTF8+HBERkZK+0/e9uW1c+dOuLq6wtDQEIaGhmjWrBk2bdpUpjrr6upCW1sbWlpaauNLsh8qFAqkpqZi69at0vfq4eFR6jo0aNAAZmZmasdlcccf8N9vwdGjRzFq1CiYmZlBX18fQ4cOxTvvvAMAGDx4cL567d+/H66urtDX14dSqUTnzp3znRnN3fbnz5/HwIEDUatWLek/xba2tujZsycOHDiA5s2bS7/FucfWli1b4OjoCAMDA7Rp0ybfbR7nzp3DkCFDYGtrK/29GDp0aL7fpdz2nTx5EuPHj4epqSlq166N/v374/79+/m2Y0n2i2PHjqFTp04wMjKCvr4+2rVrh+PHj5fka6p+BFVpWVlZQl9fX7Rt27bE84wdO1YAEJMmTRLBwcHi66+/FmZmZsLa2lo8fPhQKufu7i5q164tGjVqJDZt2iSOHDkievbsKQCIJUuWCGdnZ/HDDz+IQ4cOibffflvo6OiIf/75R5p/8eLFAoCwsbERM2fOFEeOHBGrV68WBgYGonnz5iIjI0Mq+/HHH4s1a9aIgwcPitDQUPH1118LOzs74enpqVZ3Hx8foaWlJWxtbUVAQIA4fvy4OHLkiDTNxsZGKhseHi4UCoUYMmSIOHTokDhx4oQIDAwUI0aMkMokJCSIOnXqCDMzM/H111+L4OBgMWnSJAFAjB8/XioXExMjAAhbW1vRrVs3sW/fPrFv3z7h7OwsatWqJZ48eVLkNv/jjz+EUqkUDRo0ENu2bRMHDx4UQ4cOFQDEihUrpLpEREQIAGLgwIEiIiJCREREFLlcAGLixIkiLi5O6Ovrq7UtMDBQABCRkZHSuBMnTghtbW3Rvn17sWvXLhEcHCx8fX0FABEYGCjVY9myZQKA2LBhg1SPhIQE8ccffwgAYufOndIyu3XrJvT09IS9vb007vfffxcAxKFDh4QQQqSlpQkXFxdhYGAgPv/8c3H06FGxcOFCoampKbp3756vTXXq1BEuLi5i586d4sSJE+LKlSvSd/DZZ58JIYR4/Pix8PT0FCqVSpw7d67I7ZS73AkTJojMzEyRmZkpHj9+LPbt2yeUSqUYPny4Wtnc9g8dOlQcPHhQbNu2TdSvX18YGxuLP//8UwghxK1bt8TAgQMFAGkbRUREiOfPnxdah40bN4qAgACxf/9+ERYWJrZu3SqaNm0qGjVqpHY85B47jRo1EosWLRIhISFi9erVQkdHR7z//vtSuZycHOHp6Sl0dHTEp59+Ko4ePSoWL14s6tevLwCIxYsXF7lN8m7TgpT0GElJSRENGzYUJiYmYsOGDeLIkSNi6tSpws7OTm3/erl9L1u4cKEAIPr37y9++ukncfToUbF69WqxcOHCItuQu5+fOXNGZGZmioyMDHH37l0xZcoUUaNGDREcHCyVLel+GBERIfT09ET37t2l7/Xq1atF1iP3WHzZo0ePRI0aNYSbm5sQomTH38ttqlOnjhg7dqw4fPiw+N///idu3bolNmzYIACIZcuWqdVrx44dAoDo0qWL2Ldvn9i1a5do2bKl0NbWFr/99lu+bW9jYyNmz54tQkJCxL59+4QQQtjY2Ii6desKJycn6be9bdu2QktLSyxatEi0a9dO7NmzR+zdu1c4ODgICwsL8ezZM2nZP/30k1i0aJHYu3evCAsLE0FBQcLd3V2YmZmp/W3JbV/9+vXF5MmTxZEjR8R3330natWqle83vyT7xfbt24VCoRB9+/YVe/bsEb/88ovo2bOn0NDQEMeOHSvye6uOGOyquPj4eAFADBkypETlr1+/Lv2Be1nuH+J58+ZJ49zd3QUAtT+aiYmJQkNDQ+jp6amFuOjoaAFAfPHFF9K43B+QqVOnqq0r9wfo+++/L7COOTk5IjMzU4SFhQkA4uLFi9I0Hx8fAUBs3rw533x5g93nn38uABQZuubMmSMAiN9//11t/Pjx44VCoRA3btwQQvz3B9DZ2VlkZWVJ5c6ePSsAiB9++KHQdQghxJAhQ4SOjo64c+eO2nhvb2+hr6+vVseC/kAU5uWy8+fPFzVq1JC2V0HB7q233hLNmzcXmZmZasvp2bOnsLS0FNnZ2UKIFz/QAMTJkyfzrbNu3bpi1KhRQggh0tPThYGBgZg9e7YAIG7fvi2EEOLTTz8VWlpaIiUlRQghxNdffy0AiB9//FFtWStWrBAAxNGjR9XaZGxsLB49eqRW9uUQEhMTIxo3biwaN24sYmNjS7ytChq8vb2legrxIjDm/lF/2Z07d4SOjo4YNmyYNG7ixIn5AkpJ5e7nt2/fFgDEzz//LE3LPXZWrlypNs+ECROErq6uyMnJEUIIcfjwYQFArFu3Tq3cp59+Wm7BrqTHSG7gOHz4sFq5cePGFRvs/v77b6GhoZEvYJdE7n6ed9DR0RFfffWVWtnS7IcGBgbCx8enxPV4+T8OGRkZ4vr168Lb21v6D5IQJT/+cts0cuTIfOs5efKkACB++uknaVx2drawsrISzs7O0jKEEOLp06fC3NxcCpZC/LftFy1alG/ZNjY2Qk9PT9y7d08al/vbbmlpKVJTU6Xx+/btEwDE/v37C90mWVlZIiUlRRgYGKjto7nty/t3aOXKlQKAiIuLE0KUbL9ITU0VJiYmolevXmrjs7OzRdOmTUWbNm0Knbe64qVYmTl58iSAF5eRXtamTRs4OjrmO3VtaWmJli1bSp9NTExgbm6OZs2awcrKShrv6OgIAAVeChw+fLja50GDBkFTU1OqCwD8/fffGDZsGFQqFTQ0NKClpSXdP/Ly5a9cAwYMKLatrVu3ltb3448/4p9//slX5sSJE2jcuDHatGmjNt7X1xdCCJw4cUJtfI8ePdQu2bm4uAAouN1519OpUydYW1vnW8+zZ8/K5UbyWbNmwcTEBLNnzy5w+q1bt/DHH39I30dWVpY0dO/eHXFxcbhx40ax6+nUqROOHTsGAAgPD8ezZ88wbdo0mJqaIiQkBMCLyyKurq4wMDAA8KL9BgYGGDhwoNqycvfDvPtdx44dUatWrQLXf/78ebz99tuwsLDA6dOnYWNjU2ydcw0aNAiRkZGIjIzEr7/+ii+++ALnzp1Dt27dpEtxERERSEtLy3eMWFtbo2PHjq90eSchIQEffvghrK2toampCS0tLan+Be3nL99CAbzY354/fy5dCs89hvIeY8OGDStzHfMq6TESFhYGpVKJbt26qZUbOnRosesICQlBdna22uX80tq2bZv03R4+fBg+Pj6YOHEi1q9fr9aW0uyHpfXVV19BS0sL2tracHR0RHh4OJYuXYoJEyaU6fgrye8cANy4cQP379/HiBEjUKPGf3+2DQ0NMWDAAJw5cwbPnj0r0bKbNWuGOnXqSJ9zf9s9PDzU7sMr6Dc/JSUFs2fPRsOGDaGpqQlNTU0YGhoiNTW1xPv3y8ssyX4RHh6OR48ewcfHR22b5uTkoFu3boiMjCxzr3e5YueJKs7U1BT6+vqIiYkpUfnExEQALwJbXlZWVvkCiomJSb5y2tra+cZra2sDQIE3uatUKrXPmpqaqF27tlSXlJQUtG/fHrq6uvjkk0/g4OAAfX193L17F/3790daWpra/Pr6+jAyMiquqejQoQP27duHL774AiNHjkR6ejqaNGmC+fPnS39sEhMTC3xESm5oza1jrtq1a6t9zr2nMW8d80pMTCx0mxe0nrIwMjLCggUL4Ofnpxaacz148AAAMGPGDMyYMaPAZeS916sgXl5e2Lp1K27evIljx46hefPmMDc3R8eOHXHs2DEMGzYM4eHhmD9/vjRPYmIiVCpVvvuqzM3Noampma/9BW2rXCEhIfj333+xevVq1KxZs9j6vszMzEztBvv27dvDzMwMQ4cOxZYtWzBu3Lhij5Hc8FpaOTk56NKlC+7fv4+FCxfC2dkZBgYGyMnJwdtvv13gPlTc/paYmCgdTy/Le8y9ipIeI4mJibCwsMhXrqBxeeXec/sqHSocHR3zdZ64ffs2Zs2ahffeew81a9Ys9X5YWoMGDcLMmTOhUCigVCrRoEED6T+CZTn+ijoOXlbcPpuTk4PHjx+rBbPCll3Yb3tJfvOHDRuG48ePY+HChWjdujWMjIygUCjQvXv3Mu3fJdkvcrdr3rD+skePHkn/ySQGuypPQ0MDnTp1wuHDh3Hv3r1ifxhzD6S4uLh8Ze/fvw9TU9Nyr2N8fLza/wCzsrKQmJgo1eXEiRO4f/8+QkND1Xp5PXnypMDlleb5V3369EGfPn2Qnp6OM2fOICAgAMOGDYOtrS1cXV1Ru3ZtxMXF5Zsv9wbe8toer2s948ePx7p16zB79myMHz9ebVruOubOnYv+/fsXOH+jRo2KXUduZ5Vjx44hJCQEnTt3lsYvWLAAv/76K9LT0+Hl5SXNU7t2bfz+++8QQqh9fwkJCcjKysrX/qK+45kzZ+Kvv/7CyJEjkZWVVeZHleTKPUtw8eJFqa4ACv2+yvpdXblyBRcvXsSWLVvg4+Mjjb9161aZlge8qGve4wl4ccyVl5Luu7Vr18bZs2fzlStJXczMzAC86HyR96z2q3BxccGRI0fw559/ok2bNqXeD0sr738cXlaW46+kv3XF7bM1atTIdwa8vJ4jmCspKQkHDhzA4sWLMWfOHGl8enp6mR/FVJL9Ine7fvnll2o93l9Wkv9cVCe8FPsGmDt3LoQQGDNmDDIyMvJNz8zMlJ7l1LFjRwDA999/r1YmMjIS169fl/5ol6cdO3aoff7xxx+RlZUl9ebK/YHJ26P3m2++Kbc66OjowN3dHStWrADw4iHAwIswcu3aNZw/f16t/LZt26BQKODp6Vku6+/UqZMUYPOuR19fv9AfpNLS1tbGJ598gsjISPz0009q0xo1agR7e3tcvHgRrVq1KnBQKpUAij4TaWlpicaNG2P37t2IioqSgl3nzp3x8OFDrF69GkZGRtKl8Nz2p6Sk5Hsu2LZt26TpJVWjRg188803+Oijj+Dr64uNGzeWeN6C5D7c1dzcHADg6uoKPT29fMfIvXv3pEvquUp6xhaomP08d//Me4zt3LmzzMvMq6THiLu7O54+fYrDhw+rlQsKCip2HV26dIGGhsYrf5d55X63uQGhNPuhjo5Oib7XkirN8VeWZdepUwc7d+6EEEIan5qait27d0s9ZSuSQqGAECLf/v3dd98hOzu7TMssyX7Rrl071KxZE9euXSt0u+aeXaQXeMbuDeDq6oqNGzdiwoQJaNmyJcaPH48mTZogMzMTFy5cwLfffgsnJyf06tULjRo1wtixY/Hll1+iRo0a8Pb2RmxsLBYuXAhra2tMnTq13Ou3Z88eaGpqonPnzrh69SoWLlyIpk2bYtCgQQBePO+pVq1a+PDDD7F48WJoaWlhx44d0hmUslq0aBHu3buHTp06oW7dunjy5AnWrVundv/e1KlTsW3bNvTo0QNLly6FjY0NDh48iK+++grjx48vtwd/Ll68GAcOHICnpycWLVoEExMT7NixAwcPHsTKlSthbGxcLusBXtzT9Pnnn+f7Awu8CBHe3t7o2rUrfH19UadOHTx69AjXr1/H+fPnpTDo5OQEAPj222+hVCqhq6sLOzs76cxAp06d8OWXX0JPTw/t2rUDANjZ2cHOzg5Hjx5F7969oan538/HyJEjsWHDBvj4+CA2NhbOzs44deoUli1bhu7du6ud3SupVatWQalUYsKECUhJScHMmTOLnefBgwfS42meP3+O6OhofPLJJ6hZsybef/99AEDNmjWxcOFCzJs3DyNHjsTQoUORmJiIJUuWQFdXF4sXL5aW5+zsDABYsWIFvL29oaGhARcXlwL/kLz11lto0KAB5syZAyEETExM8Msvv5T50i7w4g9fhw4dMGvWLKSmpqJVq1Y4ffo0tm/fXqrlXL58Gf/73//yjW/dunWJjxEfHx+sWbMG7733Hj755BM0bNgQhw8fxpEjRwBA7d6vvGxtbTFv3jx8/PHHSEtLkx7xcu3aNfz7778legj0lStXkJWVBeDFpck9e/YgJCQE/fr1g52dHYDS7YfOzs4IDQ3FL7/8AktLSyiVyhKd0S5KSY+/0qpRowZWrlyJ4cOHo2fPnhg3bhzS09Px2Wef4cmTJwW+maa8GRkZoUOHDvjss89gamoKW1tbhIWFYdOmTaW+ZSJXSfYLQ0NDfPnll/Dx8cGjR48wcOBAmJub4+HDh7h48SIePnxY7v9heONVXr8NKq3o6Gjh4+Mj6tWrJ7S1taXHiixatEgkJCRI5bKzs8WKFSuEg4OD0NLSEqampuK9994Td+/eVVueu7u7aNKkSb712NjYiB49euQbjzy9OXN7X0VFRYlevXoJQ0NDoVQqxdChQ8WDBw/U5g0PDxeurq5CX19fmJmZiQ8++ECcP38+X286Hx8fYWBgUGD78/aKPXDggPD29hZ16tQR2trawtzcXHTv3l2t678QQty+fVsMGzZM1K5dW2hpaYlGjRqJzz77TK13WVG9B1GC3odCCHH58mXRq1cvYWxsLLS1tUXTpk3V2vby8srSK/ZlR48elXoHvtwrVgghLl68KAYNGiTMzc2FlpaWUKlUomPHjuLrr79WK7d27VphZ2cnNDQ08n0PP//8swAgOnfurDbPmDFj8vWOzpWYmCg+/PBDYWlpKTQ1NYWNjY2YO3duvseDFNamwr6Dzz77rNBefnmX+/KgpaUl6tevL95//31x69atfOW/++474eLiIrS1tYWxsbHo06dPvkdepKeniw8++ECYmZkJhUIhAIiYmJhC63Dt2jXRuXNnoVQqRa1atcS7774r7ty5k28fyj12Xn5EhBD/9SZ8eR1PnjwRo0aNEjVr1hT6+vqic+fO0mNpStortrAh9zsvyTEixIuew/3795eO9QEDBohDhw4V2us3r23btonWrVsLXV1dYWhoKJo3b17gMVLQNnl5MDY2Fs2aNROrV6/Ot3+VdD+Mjo4W7dq1E/r6+gKAcHd3L7IeJT1uS3L8FdSjPVdBvWJz7du3T7Rt21bo6uoKAwMD0alTJ3H69Gm1MoXtW0KU/LddiIKPx3v37okBAwaIWrVqCaVSKbp16yauXLkibGxs1HoYF9a+3Lbl7Y1fkv0iLCxM9OjRQ5iYmAgtLS1Rp04d0aNHjwK3U3WnEOKl87pEpeDv748lS5bg4cOHFXLvHhFVfcuWLcOCBQtw586dcn/bBBGVHi/FEhFRieQ+WuStt95CZmYmTpw4gS+++ALvvfceQx1RFcFgR0REJaKvr481a9YgNjYW6enpqFevHmbPno0FCxZUdtWI6P/jpVgiIiIimeDjToiIiIhkgsGOiIiISCYY7IiIiIhk4o3sPJGTk4P79+9DqVSW+2tTiIiIiKoSIQSePn0KKyurIh8GDryhwe7+/fvl+r5BIiIioqru7t27xT5a6I0Mdrnv27t79y6MjIwquTZEREREFSc5ORnW1tYlet/wGxnsci+/GhkZMdgRVSEbN27Exo0bERsbCwBo0qQJFi1aBG9vb6nM9evXMXv2bISFhSEnJwdNmjTBjz/+iHr16qktSwiB7t27Izg4GHv37kXfvn0LXe/Tp0+xcOFC7N27FwkJCWjevDnWrVuH1q1bS2V8fX2xdetWtfnatm0rvVuWiKiqK8ntZ29ksCOiqqlu3bpYvnw5GjZsCADYunUr+vTpgwsXLqBJkyb466+/8M4772D06NFYsmQJjI2Ncf36dejq6uZb1tq1a0t8D+0HH3yAK1euYPv27bCyssL3338PLy8vXLt2DXXq1JHKdevWDYGBgdJnbW3tV2wxEVHV8kY+oDg5ORnGxsZISkriGTuiKs7ExASfffYZRo8ejSFDhkBLSwvbt28vcp6LFy+iZ8+eiIyMhKWlZZFn7NLS0qBUKvHzzz+jR48e0vhmzZqhZ8+e+OSTTwC8OGP35MkT7Nu3r7yaRkT0WpQm9/BxJ0RUIbKzsxEUFITU1FS4uroiJycHBw8ehIODA7p27Qpzc3O0bds2X9B69uwZhg4divXr10OlUhW7nqysLGRnZ+c766enp4dTp06pjQsNDYW5uTkcHBwwZswYJCQkvHI7iYiqEl6KJaJydfnyZbi6uuL58+cwNDTE3r170bhxY8THxyMlJQXLly/HJ598ghUrViA4OBj9+/fHyZMn4e7uDgCYOnUq3Nzc0KdPnxKtT6lUwtXVFR9//DEcHR1hYWGBH374Ab///jvs7e2lct7e3nj33XdhY2ODmJgYLFy4EB07dkRUVBR0dHQqZFsQFSY7OxuZmZmVXQ2qIrS0tKChoVEuy2KwI6Jy1ahRI0RHR+PJkyfYvXs3fHx8EBYWhpo1awIA+vTpg6lTpwJ4cbk0PDwcX3/9Ndzd3bF//36cOHECFy5cKNU6t2/fjlGjRqFOnTrQ0NBAixYtMGzYMJw/f14qM3jwYOnfTk5OaNWqFWxsbHDw4EH079//1RtOVAJCCMTHx+PJkyeVXRWqYmrWrAmVSvXKz+dlsCOicqWtrS11nmjVqhUiIyOxbt06fPnll9DU1ETjxo3Vyjs6OkqXTE+cOIG//vpLCoG5BgwYgPbt2yM0NLTAdTZo0ABhYWFITU1FcnIyLC0tMXjwYNjZ2RVaT0tLS9jY2ODmzZtlbyxRKeWGOnNzc+jr6/Mh+wQhBJ49eybdGmJpaflKy2OwI6IKJYRAeno6tLW10bp1a9y4cUNt+p9//gkbGxsAwJw5c/DBBx+oTXd2dsaaNWvQq1evYtdlYGAAAwMDPH78GEeOHMHKlSsLLZuYmIi7d+++8o8oUUllZ2dLoa527dqVXR2qQvT09AAACQkJMDc3f6XLsgx2RFRu5s2bB29vb1hbW+Pp06cICgpCaGgogoODAQAzZ87E4MGD0aFDB3h6eiI4OBi//PKLdCZOpVIV2GGiXr16amffOnXqhH79+mHSpEkAgCNHjkAIgUaNGuHWrVuYOXMmGjVqhPfffx8AkJKSAn9/fwwYMACWlpaIjY3FvHnzYGpqin79+lXwViF6IfeeOn19/UquCVVFuftFZmYmgx0RVQ0PHjzAiBEjEBcXB2NjY7i4uCA4OBidO3cGAPTr1w9ff/01AgICMGXKFDRq1Ai7d+/GO++8U6r1/PXXX/j333+lz0lJSZg7dy7u3bsHExMTDBgwAJ9++im0tLQAABoaGrh8+TK2bduGJ0+ewNLSEp6enti1a1eJnuROVJ54+ZUKUl77BZ9jR0RE9Bo8f/4cMTExsLOzK/Ch3FS9FbV/8Dl2RERERC/x8PCAn59fZVejwvFSLBERUSWynXPwta4vdnmP4gu9JPc9ywEBAZgzZ440ft++fejXrx9Kc+HP1tYWfn5+b0TAio2NVbu3V0tLC/Xq1YOvry/mz59f4kunucu5cOECmjVrVkG1/Q/P2BEREVGRdHV1sWLFCjx+/Liyq1Jqr/og6GPHjiEuLg43b97EkiVL8Omnn2Lz5s3lVLvyx2BHRERERfLy8oJKpUJAQECR5cLDw9GhQwfo6enB2toaU6ZMQWpqKoAXl0Jv376NqVOnQqFQQKFQQAgBMzMz7N69W1pGs2bNYG5uLn2OiIiAlpYWUlJSAAB37txBnz59YGhoCCMjIwwaNAgPHjyQyvv7+6NZs2bYvHkz6tevDx0dnQLPKgYHB8PY2Bjbtm0rsk21a9eGSqWCjY0Nhg8fDjc3N7WHn+fk5GDp0qWoW7cudHR00KxZM+lJAACks37NmzeHQqGAh4dHket7VQx2REREVCQNDQ0sW7YMX375Je7du1dgmcuXL6Nr167o378/Ll26hF27duHUqVPSY4n27NmDunXrYunSpYiLi0NcXBwUCgU6dOggPfLo8ePHuHbtGjIzM3Ht2jUAL97x3LJlSxgaGkIIgb59++LRo0cICwtDSEgI/vrrL7U3ywDArVu38OOPP2L37t2Ijo7OV9egoCAMGjQI27Ztw8iRI0u8Hc6dO4fz58+jbdu20rh169Zh1apV+Pzzz3Hp0iV07doVvXv3lh5+fvbsWQD/nfnbs2dPiddXFrzHjoiIiIrVr18/NGvWDIsXL8amTZvyTf/ss88wbNgw6f45e3t7fPHFF3B3d8fGjRthYmICDQ0NKJVKtedVenh44NtvvwUA/Prrr2jatCnq1auH0NBQNG7cGKGhodJZrmPHjuHSpUuIiYmBtbU1gBevFGzSpAkiIyPRunVrAEBGRga2b98OMzOzfPX86quvMG/ePPz888/w9PQstt1ubm6oUaMGMjIykJmZibFjx6qFwc8//xyzZ8/GkCFDAAArVqzAyZMnsXbtWmzYsEGqQ+6Zv4rGYEdExXrdN3dXZaW98ZxITlasWIGOHTti+vTp+aZFRUXh1q1b2LFjhzROCIGcnBzExMTA0dGxwGV6eHjgo48+wr///ouwsDB4eHigXr16CAsLw9ixYxEeHi6FxevXr8Pa2loKdQDQuHFj1KxZE9evX5eCnY2NTYGhbvfu3Xjw4AFOnTqFNm3alKjNu3btgqOjIzIzM3H58mVMmTIFtWrVwvLly5GcnIz79++jXbt2avO0a9cOFy9eLNHyyxsvxRIREVGJdOjQAV27dsW8efPyTcvJycG4ceMQHR0tDRcvXsTNmzfRoEGDQpfp5OSE2rVrIywsTAp27u7uCAsLQ2RkJNLS0qSHmAshCuyNmne8gYFBgetq1qwZzMzMEBgYWOLevNbW1mjYsCEcHR0xaNAg+Pn5YdWqVXj+/LlUJm+dCqvn68AzdkRERFRiy5cvR7NmzeDg4KA2vkWLFrh69SoaNmxY6Lza2trIzs5WG5d7n93PP/+MK1euoH379lAqlcjMzMTXX3+NFi1aSG+Iady4Me7cuYO7d+9KZ+2uXbuGpKSkQs8IvqxBgwZYtWoVPDw8oKGhgfXr15e2+dDQ0EBWVhYyMjJgZGQEKysrnDp1Ch06dJDKhIeHS2cEtbW1ASBfuysKz9gRERFRiTk7O2P48OH48ssv1cbPnj0bERERmDhxIqKjo3Hz5k3s378fkydPlsrY2tri119/xT///KP2WkAPDw/s3LkTLi4uMDIyksLejh071HqRenl5wcXFBcOHD8f58+dx9uxZjBw5Eu7u7mjVqlWJ6u/g4ICTJ09i9+7dJXqeXmJiIuLj43Hv3j0cPnwY69atg6enp/QGiJkzZ2LFihXYtWsXbty4gTlz5iA6OhofffQRAMDc3Bx6enoIDg7GgwcPkJSUVKJ6lhWDHREREZXKxx9/nO9SpouLC8LCwnDz5k20b98ezZs3x8KFC2FpaSmVWbp0KWJjY9GgQQO1e+A8PT2RnZ2tFuLc3d2RnZ0Nd3d3aZxCocC+fftQq1YtdOjQAV5eXqhfvz527dpVqvo3atQIJ06cwA8//FDg/YIv8/LygqWlJWxtbTF27Fh0795dbX1TpkzB9OnTMX36dDg7OyM4OBj79++Hvb09AEBTUxNffPEFvvnmG1hZWaFPnz6lqmtp8V2xRFQsdp74DztPUFnxXbFUFL4rloiIiIjUMNgRERERyQSDHREREZFMMNgRERERyUSpgt3GjRulrshGRkZwdXXF4cOHpem+vr7Si31zh7ffflttGenp6Zg8eTJMTU1hYGCA3r17F/reOSIiIiIquVIFu7p162L58uU4d+4czp07h44dO6JPnz64evWqVKZbt27Sy33j4uJw6NAhtWX4+flh7969CAoKwqlTp5CSkoKePXu+tgf3EREREclVqd480atXL7XPn376KTZu3IgzZ86gSZMmAAAdHZ1CX3KblJSETZs2Yfv27fDy8gIAfP/997C2tsaxY8fQtWvXsrSBiIiIiPAK99hlZ2cjKCgIqampcHV1lcaHhobC3NwcDg4OGDNmDBISEqRpUVFRyMzMRJcuXaRxVlZWcHJyQnh4eFmrQkREREQow7tiL1++DFdXVzx//hyGhobYu3cvGjduDADw9vbGu+++CxsbG8TExGDhwoXo2LEjoqKioKOjg/j4eGhra6NWrVpqy7SwsEB8fHyh60xPT0d6err0OTk5ubTVJiIiIpK9Up+xa9SoEaKjo3HmzBmMHz8ePj4+uHbtGgBg8ODB6NGjB5ycnNCrVy8cPnwYf/75Jw4eLPqp9UIIKBSKQqcHBATA2NhYGnJf/EtERERvPltbW6xdu7ayqyELpT5jp62tjYYNGwIAWrVqhcjISKxbtw7ffPNNvrKWlpawsbHBzZs3AQAqlQoZGRl4/Pix2lm7hIQEuLm5FbrOuXPnYtq0adLn5ORkhjsiIpIHf+PXvL7SvYTe19cXT548wb59+yqmPgAiIyNhYGBQ5vltbW1x+/ZtAECNGjVgYWEBb29vfP755/muEha3HD8/P/j5+ZW5LpXtlZ9jJ4RQu0z6ssTERNy9e1d6AXDLli2hpaWFkJAQqUxcXByuXLlSZLDT0dGRHrGSOxAREZE8mJmZQV9f/5WWsXTpUsTFxeHOnTvYsWMHfv31V0yZMqWcavjmKFWwmzdvHn777TfExsbi8uXLmD9/PkJDQzF8+HCkpKRgxowZiIiIQGxsLEJDQ9GrVy+YmpqiX79+AABjY2OMHj0a06dPx/Hjx3HhwgW89957cHZ2lnrJEhER0ZsjLCwMbdq0gY6ODiwtLTFnzhxkZWVJ058+fYrhw4fDwMAAlpaWWLNmDTw8PNTOiuW9FPvkyROMHTsWFhYW0NXVhZOTEw4cOFBkPZRKJVQqFerUqQNPT0+MHDkS58+fVyuze/duNGnSBDo6OrC1tcWqVaukaR4eHrh9+zamTp0qPYv3TVSqS7EPHjzAiBEjEBcXB2NjY7i4uCA4OBidO3dGWloaLl++jG3btuHJkyewtLSEp6cndu3aBaVSKS1jzZo10NTUxKBBg5CWloZOnTphy5Yt0NDQKPfGERERUcX5559/0L17d/j6+mLbtm34448/MGbMGOjq6sLf3x8AMG3aNJw+fRr79++HhYUFFi1ahPPnz6NZs2YFLjMnJwfe3t54+vQpvv/+ezRo0ADXrl0rVU74559/cODAAbRt21YaFxUVhUGDBsHf3x+DBw9GeHg4JkyYgNq1a8PX1xd79uxB06ZNMXbsWIwZM+ZVNkulKlWw27RpU6HT9PT0cOTIkWKXoauriy+//BJffvllaVZNREREVcxXX30Fa2trrF+/HgqFAm+99Rbu37+P2bNnY9GiRUhNTcXWrVuxc+dOdOrUCQAQGBgIKyurQpd57NgxnD17FtevX4eDgwMAoH79+sXWZfbs2ViwYAGys7Px/PlztG3bFqtXr5amr169Gp06dcLChQsBAA4ODrh27Ro+++wz+Pr6wsTEBBoaGtKZvzcV3xVLREREZXL9+nW4urqqXbZs164dUlJScO/ePfz999/IzMxEmzZtpOnGxsZo1KhRocuMjo5G3bp1pVBXUjNnzkR0dDQuXbqE48ePAwB69Oghvdnq+vXraNeundo87dq1w82bN2X19isGOyIiIiqTgh5XJoQAACgUCrV/F1SmIHp6emWqi6mpKRo2bAh7e3t07NgRa9euRXh4OE6ePFlsXeWEwY6IiIjKpHHjxggPD1cLSOHh4VAqlahTpw4aNGgALS0tnD17VpqenJwsPQatIC4uLrh37x7+/PPPV6pb7j15aWlpUl1PnTqlViY8PBwODg5SWW1t7Tf+7F2pn2NHRERE1UtSUhKio6PVxpmYmGDChAlYu3YtJk+ejEmTJuHGjRtYvHgxpk2bhho1akCpVMLHxwczZ86EiYkJzM3NsXjxYtSoUaPQXqfu7u7o0KEDBgwYgNWrV6Nhw4b4448/oFAo0K1bt0Lr+PTpU8THx0MIgbt372LWrFkwNTWVHqc2ffp0tG7dGh9//DEGDx6MiIgIrF+/Hl999ZW0DFtbW/z6668YMmQIdHR0YGpq+uob7zXjGTsiIiIqUmhoKJo3b642LFq0CHXq1MGhQ4dw9uxZNG3aFB9++CFGjx6NBQsWSPOuXr0arq6u6NmzJ7y8vNCuXTs4OjpCV1e30PXt3r0brVu3xtChQ9G4cWPMmjWr2DNpixYtgqWlJaysrNCzZ08YGBggJCQEtWvXBgC0aNECP/74I4KCguDk5IRFixZh6dKl8PX1lZaxdOlSxMbGokGDBjAzM3u1jVZJFOINvMCcnJwMY2NjJCUl8WHFRK+B7ZyiXwtYncQu71HZVaA31PPnzxETEwM7O7siQ43cpaamok6dOli1ahVGjx5d2dWpMoraP0qTe3gploiIiCrMhQsX8Mcff6BNmzZISkrC0qVLAQB9+vSp5JrJE4MdERERVajPP/8cN27cgLa2Nlq2bInffvvtjbx/7U3AYEdEREQVpnnz5oiKiqrsalQb7DxBREREJBMMdkREREQywWBHRET0GuXk5FR2FagKKq/9gvfYERERvQba2tqoUaMG7t+/DzMzM2hraxf6kF6qPoQQyMjIwMOHD1GjRg1oa2u/0vIY7IiIiF6DGjVqwM7ODnFxcbh//35lV4eqGH19fdSrVw81arzaxVQGOyIiotdEW1sb9erVQ1ZW1hv/TlIqPxoaGtDU1CyXM7gMdkRERK+RQqGAlpYWtLS0KrsqJEPsPEFEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2REQVZOPGjXBxcYGRkRGMjIzg6uqKw4cPS9OFEPD394eVlRX09PTg4eGBq1evqi3Dw8MDCoVCbRgyZEiR67W1tc03j0KhwMSJEwssP27cOCgUCqxdu/aV20xElYvBjoiogtStWxfLly/HuXPncO7cOXTs2BF9+vSRwtvKlSuxevVqrF+/HpGRkVCpVOjcuTOePn2qtpwxY8YgLi5OGr755psi1xsZGalWPiQkBADw7rvv5iu7b98+/P7777CysiqnVhNRZWKwIyKqIL169UL37t3h4OAABwcHfPrppzA0NMSZM2cghMDatWsxf/589O/fH05OTti6dSuePXuGnTt3qi1HX18fKpVKGoyNjYtcr5mZmVr5AwcOoEGDBnB3d1cr988//2DSpEnYsWMHn6lGJBMMdkREr0F2djaCgoKQmpoKV1dXxMTEID4+Hl26dJHK6OjowN3dHeHh4Wrz7tixA6ampmjSpAlmzJiR74xeUTIyMvD9999j1KhRak+1z8nJwYgRIzBz5kw0adLk1RtIRFUC3zxBRFSBLl++DFdXVzx//hyGhobYu3cvGjduLIU3CwsLtfIWFha4ffu29Hn48OGws7ODSqXClStXMHfuXFy8eFG6vFqcffv24cmTJ/D19VUbv2LFCmhqamLKlCmv1kAiqlIY7IiIKlCjRo0QHR2NJ0+eYPfu3fDx8UFYWJg0Pe+7IYUQauPGjBkj/dvJyQn29vZo1aoVzp8/jxYtWhS7/k2bNsHb21vtHrqoqCisW7cO58+fL5d3UxJR1cFLsUREFUhbWxsNGzZEq1atEBAQgKZNm2LdunVQqVQAgPj4eLXyCQkJ+c7ivaxFixbQ0tLCzZs3i1337du3cezYMXzwwQdq43/77TckJCSgXr160NTUhKamJm7fvo3p06fD1ta29I0koiqDwY6I6DUSQiA9PV26vPryJdWMjAyEhYXBzc2t0PmvXr2KzMxMWFpaFruuwMBAmJubo0ePHmrjR4wYgUuXLiE6OloarKysMHPmTBw5cqTsjSOiSsdLsUREFWTevHnw9vaGtbU1nj59iqCgIISGhiI4OBgKhQJ+fn5YtmwZ7O3tYW9vj2XLlkFfXx/Dhg0DAPz111/YsWMHunfvDlNTU1y7dg3Tp09H8+bN0a5dO2k9nTp1Qr9+/TBp0iRpXE5ODgIDA+Hj4wNNTfWf+tq1a6N27dpq47S0tKBSqdCoUaMK3CJEVNEY7IiIKsiDBw8wYsQIxMXFwdjYGC4uLggODkbnzp0BALNmzUJaWhomTJiAx48fo23btjh69CiUSiWAF5dxjx8/jnXr1iElJQXW1tbo0aMHFi9eDA0NDWk9f/31F/7991+1dR87dgx37tzBqFGjXl+DiajSKYQQorIrUVrJyckwNjZGUlISjIyMKrs6RLJnO+dgZVehyohd3qP4QkRE5ag0uYf32BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUzwOXZERKXhb1zZNaha/JMquwZE9BKesSMiIiKSiVIFu40bN8LFxQVGRkYwMjKCq6srDh8+LE0XQsDf3x9WVlbQ09ODh4cHrl69qraM9PR0TJ48GaampjAwMEDv3r1x79698mkNERERUTVWqmBXt25dLF++HOfOncO5c+fQsWNH9OnTRwpvK1euxOrVq7F+/XpERkZCpVKhc+fOePr0qbQMPz8/7N27F0FBQTh16hRSUlLQs2dPZGdnl2/LiIiIiKqZV36lmImJCT777DOMGjUKVlZW8PPzw+zZswG8ODtnYWGBFStWYNy4cUhKSoKZmRm2b9+OwYMHAwDu378Pa2trHDp0CF27di3ROvlKMaLXi68U+0+s7rDKrkLVwnvsiCrca3mlWHZ2NoKCgpCamgpXV1fExMQgPj4eXbp0kcro6OjA3d0d4eHhAICoqChkZmaqlbGysoKTk5NUpiDp6elITk5WG4iIiIhIXamD3eXLl2FoaAgdHR18+OGH2Lt3Lxo3boz4+HgAgIWFhVp5CwsLaVp8fDy0tbVRq1atQssUJCAgAMbGxtJgbW1d2moTERERyV6pg12jRo0QHR2NM2fOYPz48fDx8cG1a9ek6QqFQq28ECLfuLyKKzN37lwkJSVJw927d0tbbSIiIiLZK3Ww09bWRsOGDdGqVSsEBASgadOmWLduHVQqFQDkO/OWkJAgncVTqVTIyMjA48ePCy1TEB0dHaknbu5AREREROpe+Tl2Qgikp6fDzs4OKpUKISEh0rSMjAyEhYXBzc0NANCyZUtoaWmplYmLi8OVK1ekMkRERERUNqV688S8efPg7e0Na2trPH36FEFBQQgNDUVwcDAUCgX8/PywbNky2Nvbw97eHsuWLYO+vj6GDXvRi8zY2BijR4/G9OnTUbt2bZiYmGDGjBlwdnaGl5dXhTSQiIiIqLooVbB78OABRowYgbi4OBgbG8PFxQXBwcHo3LkzAGDWrFlIS0vDhAkT8PjxY7Rt2xZHjx6FUqmUlrFmzRpoampi0KBBSEtLQ6dOnbBlyxZoaGiUb8uIiIiIqplXfo5dZeBz7IheLz7H7j98jl0efI4dUYV7Lc+xIyIiIqKqhcGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkgsGOiIiISCYY7IiIiIhkolTBLiAgAK1bt4ZSqYS5uTn69u2LGzduqJXx9fWFQqFQG95++221Munp6Zg8eTJMTU1hYGCA3r174969e6/eGiIiIqJqrFTBLiwsDBMnTsSZM2cQEhKCrKwsdOnSBampqWrlunXrhri4OGk4dOiQ2nQ/Pz/s3bsXQUFBOHXqFFJSUtCzZ09kZ2e/eouIiIiIqinN0hQODg5W+xwYGAhzc3NERUWhQ4cO0ngdHR2oVKoCl5GUlIRNmzZh+/bt8PLyAgB8//33sLa2xrFjx9C1a9fStoGIiIiI8Ir32CUlJQEATExM1MaHhobC3NwcDg4OGDNmDBISEqRpUVFRyMzMRJcuXaRxVlZWcHJyQnh4+KtUh4iIiKhaK9UZu5cJITBt2jS88847cHJyksZ7e3vj3XffhY2NDWJiYrBw4UJ07NgRUVFR0NHRQXx8PLS1tVGrVi215VlYWCA+Pr7AdaWnpyM9PV36nJycXNZqExEREclWmYPdpEmTcOnSJZw6dUpt/ODBg6V/Ozk5oVWrVrCxscHBgwfRv3//QpcnhIBCoShwWkBAAJYsWVLWqhIRERFVC2W6FDt58mTs378fJ0+eRN26dYssa2lpCRsbG9y8eRMAoFKpkJGRgcePH6uVS0hIgIWFRYHLmDt3LpKSkqTh7t27Zak2ERERkayVKtgJITBp0iTs2bMHJ06cgJ2dXbHzJCYm4u7du7C0tAQAtGzZElpaWggJCZHKxMXF4cqVK3BzcytwGTo6OjAyMlIbiIiIiEhdqS7FTpw4ETt37sTPP/8MpVIp3RNnbGwMPT09pKSkwN/fHwMGDIClpSViY2Mxb948mJqaol+/flLZ0aNHY/r06ahduzZMTEwwY8YMODs7S71kiYiIiKj0ShXsNm7cCADw8PBQGx8YGAhfX19oaGjg8uXL2LZtG548eQJLS0t4enpi165dUCqVUvk1a9ZAU1MTgwYNQlpaGjp16oQtW7ZAQ0Pj1VtEREREVE0phBCisitRWsnJyTA2NkZSUhIvyxK9BrZzDlZ2FaqMWN1hlV2FqsU/qbJrQCR7pck9fFcsERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJRKmCXUBAAFq3bg2lUglzc3P07dsXN27cUCsjhIC/vz+srKygp6cHDw8PXL16Va1Meno6Jk+eDFNTUxgYGKB37964d+/eq7eGiIiIqBorVbALCwvDxIkTcebMGYSEhCArKwtdunRBamqqVGblypVYvXo11q9fj8jISKhUKnTu3BlPnz6Vyvj5+WHv3r0ICgrCqVOnkJKSgp49eyI7O7v8WkZERERUzSiEEKKsMz98+BDm5uYICwtDhw4dIISAlZUV/Pz8MHv2bAAvzs5ZWFhgxYoVGDduHJKSkmBmZobt27dj8ODBAID79+/D2toahw4dQteuXYtdb3JyMoyNjZGUlAQjI6OyVp+ISsh2zsHKrkKVEas7rLKrULX4J1V2DYhkrzS555XusUtKenFAm5iYAABiYmIQHx+PLl26SGV0dHTg7u6O8PBwAEBUVBQyMzPVylhZWcHJyUkqk1d6ejqSk5PVBiIiIiJSV+ZgJ4TAtGnT8M4778DJyQkAEB8fDwCwsLBQK2thYSFNi4+Ph7a2NmrVqlVombwCAgJgbGwsDdbW1mWtNhEREZFslTnYTZo0CZcuXcIPP/yQb5pCoVD7LITINy6vosrMnTsXSUlJ0nD37t2yVpuIiIhItsoU7CZPnoz9+/fj5MmTqFu3rjRepVIBQL4zbwkJCdJZPJVKhYyMDDx+/LjQMnnp6OjAyMhIbSAiIiIidaUKdkIITJo0CXv27MGJEydgZ2enNt3Ozg4qlQohISHSuIyMDISFhcHNzQ0A0LJlS2hpaamViYuLw5UrV6QyRERERFR6mqUpPHHiROzcuRM///wzlEqldGbO2NgYenp6UCgU8PPzw7Jly2Bvbw97e3ssW7YM+vr6GDZsmFR29OjRmD59OmrXrg0TExPMmDEDzs7O8PLyKv8WEhEREVUTpQp2GzduBAB4eHiojQ8MDISvry8AYNasWUhLS8OECRPw+PFjtG3bFkePHoVSqZTKr1mzBpqamhg0aBDS0tLQqVMnbNmyBRoaGq/WGiIiIqJq7JWeY1dZ+Bw7oteLz7H7D59jlwefY0dU4V7bc+yIiIiIqOpgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIpkodbD79ddf0atXL1hZWUGhUGDfvn1q0319faFQKNSGt99+W61Meno6Jk+eDFNTUxgYGKB37964d+/eKzWEiIiIqLordbBLTU1F06ZNsX79+kLLdOvWDXFxcdJw6NAhtel+fn7Yu3cvgoKCcOrUKaSkpKBnz57Izs4ufQuIiIiICACgWdoZvL294e3tXWQZHR0dqFSqAqclJSVh06ZN2L59O7y8vAAA33//PaytrXHs2DF07dq1tFUiIiIiIlTQPXahoaEwNzeHg4MDxowZg4SEBGlaVFQUMjMz0aVLF2mclZUVnJycEB4eXuDy0tPTkZycrDYQERERkbpyD3be3t7YsWMHTpw4gVWrViEyMhIdO3ZEeno6ACA+Ph7a2tqoVauW2nwWFhaIj48vcJkBAQEwNjaWBmtr6/KuNhEREdEbr9SXYoszePBg6d9OTk5o1aoVbGxscPDgQfTv37/Q+YQQUCgUBU6bO3cupk2bJn1OTk5muCMiIiLKo8Ifd2JpaQkbGxvcvHkTAKBSqZCRkYHHjx+rlUtISICFhUWBy9DR0YGRkZHaQERERETqKjzYJSYm4u7du7C0tAQAtGzZElpaWggJCZHKxMXF4cqVK3Bzc6vo6hARERHJVqkvxaakpODWrVvS55iYGERHR8PExAQmJibw9/fHgAEDYGlpidjYWMybNw+mpqbo168fAMDY2BijR4/G9OnTUbt2bZiYmGDGjBlwdnaWeskSERERUemVOtidO3cOnp6e0ufce998fHywceNGXL58Gdu2bcOTJ09gaWkJT09P7Nq1C0qlUppnzZo10NTUxKBBg5CWloZOnTphy5Yt0NDQKIcmEREREVVPCiGEqOxKlFZycjKMjY2RlJTE++2IXgPbOQcruwpVRqzusMquQtXin1TZNSCSvdLkHr4rloiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmSh3sfv31V/Tq1QtWVlZQKBTYt2+f2nQhBPz9/WFlZQU9PT14eHjg6tWramXS09MxefJkmJqawsDAAL1798a9e/deqSFERERE1V2pg11qaiqaNm2K9evXFzh95cqVWL16NdavX4/IyEioVCp07twZT58+lcr4+flh7969CAoKwqlTp5CSkoKePXsiOzu77C0hIiIiquY0SzuDt7c3vL29C5wmhMDatWsxf/589O/fHwCwdetWWFhYYOfOnRg3bhySkpKwadMmbN++HV5eXgCA77//HtbW1jh27Bi6du36Cs0hIiIiqr7K9R67mJgYxMfHo0uXLtI4HR0duLu7Izw8HAAQFRWFzMxMtTJWVlZwcnKSyhARERFR6ZX6jF1R4uPjAQAWFhZq4y0sLHD79m2pjLa2NmrVqpWvTO78eaWnpyM9PV36nJycXJ7VJiIiIpKFCukVq1Ao1D4LIfKNy6uoMgEBATA2NpYGa2vrcqsrERERkVyUa7BTqVQAkO/MW0JCgnQWT6VSISMjA48fPy60TF5z585FUlKSNNy9e7c8q01EREQkC+Ua7Ozs7KBSqRASEiKNy8jIQFhYGNzc3AAALVu2hJaWllqZuLg4XLlyRSqTl46ODoyMjNQGIiIiIlJX6nvsUlJScOvWLelzTEwMoqOjYWJignr16sHPzw/Lli2Dvb097O3tsWzZMujr62PYsGEAAGNjY4wePRrTp09H7dq1YWJighkzZsDZ2VnqJUtEREREpVfqYHfu3Dl4enpKn6dNmwYA8PHxwZYtWzBr1iykpaVhwoQJePz4Mdq2bYujR49CqVRK86xZswaampoYNGgQ0tLS0KlTJ2zZsgUaGhrl0CQiIiKi6kkhhBCVXYnSSk5OhrGxMZKSknhZlug1sJ1zsLKrUGXE6g6r7CpULf5JlV0DItkrTe7hu2KJiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmyj3Y+fv7Q6FQqA0qlUqaLoSAv78/rKysoKenBw8PD1y9erW8q0FERERU7VTIGbsmTZogLi5OGi5fvixNW7lyJVavXo3169cjMjISKpUKnTt3xtOnTyuiKkRERETVRoUEO01NTahUKmkwMzMD8OJs3dq1azF//nz0798fTk5O2Lp1K549e4adO3dWRFWIiIiIqo0KCXY3b96ElZUV7OzsMGTIEPz9998AgJiYGMTHx6NLly5SWR0dHbi7uyM8PLzQ5aWnpyM5OVltICIiIiJ15R7s2rZti23btuHIkSP4v//7P8THx8PNzQ2JiYmIj48HAFhYWKjNY2FhIU0rSEBAAIyNjaXB2tq6vKtNRERE9MYr92Dn7e2NAQMGwNnZGV5eXjh48CAAYOvWrVIZhUKhNo8QIt+4l82dOxdJSUnScPfu3fKuNhEREdEbr8Ifd2JgYABnZ2fcvHlT6h2b9+xcQkJCvrN4L9PR0YGRkZHaQERERETqKjzYpaen4/r167C0tISdnR1UKhVCQkKk6RkZGQgLC4Obm1tFV4WIiIhI1jTLe4EzZsxAr169UK9ePSQkJOCTTz5BcnIyfHx8oFAo4Ofnh2XLlsHe3h729vZYtmwZ9PX1MWzYsPKuChEREVG1Uu7B7t69exg6dCj+/fdfmJmZ4e2338aZM2dgY2MDAJg1axbS0tIwYcIEPH78GG3btsXRo0ehVCrLuypERERE1YpCCCEquxKllZycDGNjYyQlJfF+O6LXwHbOwcquQpURq8urC2r8kyq7BkSyV5rcw3fFEhEREckEg91rFhAQgNatW0OpVMLc3Bx9+/bFjRs31Mr4+vrme9/u22+/Xeyyd+/ejcaNG0NHRweNGzfG3r17K6oZREREVAUx2L1mYWFhmDhxIs6cOYOQkBBkZWWhS5cuSE1NVSvXrVs3tfftHjp0qMjlRkREYPDgwRgxYgQuXryIESNGYNCgQfj9998rsjlERERUhfAeu0r28OFDmJubIywsDB06dADw4ozdkydPsG/fvhIvZ/DgwUhOTsbhw4elcd26dUOtWrXwww8/lHe1qZrhPXb/4T12efAeO6IKx3vs3iBJSS9+FE1MTNTGh4aGwtzcHA4ODhgzZgwSEhKKXE5ERITaO3gBoGvXrkW+g5eIiIjkhcGuEgkhMG3aNLzzzjtwcnKSxnt7e2PHjh04ceIEVq1ahcjISHTs2BHp6emFLis+Pr7U7+AlIiIieWGwq0STJk3CpUuX8l0qHTx4MHr06AEnJyf06tULhw8fxp9//im9d7cwpX0Hb2UrSUcSIQT8/f1hZWUFPT09eHh44OrVq8Uumx1JiIioOmKwqySTJ0/G/v37cfLkSdStW7fIspaWlrCxscHNmzcLLaNSqUr9Dt7KVpKOJCtXrsTq1auxfv16REZGQqVSoXPnznj69Gmhy2VHEiIiqq7YeeI1E0Jg8uTJ2Lt3L0JDQ2Fvb1/sPImJiahTpw6+/fZbjBw5ssAygwcPxtOnT9V6z3p7e6NmzZpvTOeJvB1JhBCwsrKCn58fZs+eDeDFu4ctLCywYsUKjBs3rsDlsCNJ+WPnif+w80Qe7DxBVOHYeaIKmzhxIr7//nvs3LkTSqUS8fHxiI+PR1paGgAgJSUFM2bMQEREBGJjYxEaGopevXrB1NQU/fr1k5YzcuRIzJ07V/r80Ucf4ejRo1ixYgX++OMPrFixAseOHYOfn9/rbmKZ5e1IEhMTg/j4eLVOITo6OnB3dy+yUwg7khARUXXFYPeabdy4EUlJSfDw8IClpaU07Nq1CwCgoaGBy5cvo0+fPnBwcICPjw8cHBwQERGh9j7dO3fuIC4uTvrs5uaGoKAgBAYGwsXFBVu2bMGuXbvQtm3b197GsiioI0nupeXSdgphRxIiIqquNCu7AtVNcVe+9fT0cOTIkWKXExoamm/cwIEDMXDgwLJWrVLldiQ5depUvmll6RTypnUkISIiKg88Y0eVrrCOJCqVCgBK3SnkTexIQkREVB4Y7KjSCCEwadIk7NmzBydOnICdnZ3adDs7O6hUKoSEhEjjMjIyEBYWBjc3t0KX6+rqqjYPABw9erTIeYiIiOSAl2Kp0kycOBE7d+7Ezz//LHUkAQBjY2Po6elBoVDAz88Py5Ytg729Pezt7bFs2TLo6+tj2LD/eiaOHDkSderUQUBAAIAXHUk6dOiAFStWoE+fPvj5559x7NixAi/zEhERyQmDHVWajRs3AgA8PDzUxgcGBsLX1xcAMGvWLKSlpWHChAl4/Pgx2rZti6NHj+brSFKjxn8nn3M7kixYsAALFy5EgwYN3qiOJERERGXF59gRUbH4HLv/8Dl2efA5dkQVjs+xIyIiIqqGGOyIiIiIZIL32BWDl6DUxS7vUdlVICIiokLwjB0RERGRTDDYEREREckEgx0RERGRTDDYEREREckEO09Q6fgbV3YNqhY+w4uIiKoQnrEjIiIikgkGOyIiokrw1Vdfwc7ODrq6umjZsiV+++23Qsv6+vpCoVDkG5o0aaJWbu3atWjUqBH09PRgbW2NqVOn4vnz5xXdlFdWmm0BADt27EDTpk2hr68PS0tLvP/++0hMTJSm79mzB61atULNmjVhYGCAZs2aYfv27RXdjCqBwY6IiOg127VrF/z8/DB//nxcuHAB7du3h7e3N+7cuVNg+XXr1iEuLk4a7t69CxMTE7z77rtSmR07dmDOnDlYvHgxrl+/jk2bNmHXrl2YO3fu62pWmZR2W5w6dQojR47E6NGjcfXqVfz000+IjIzEBx98IJUxMTHB/PnzERERgUuXLuH999/H+++/jyNHjryuZlUaBjsiIqLXbPXq1Rg9ejQ++OADODo6Yu3atbC2tsbGjRsLLG9sbAyVSiUN586dw+PHj/H+++9LZSIiItCuXTsMGzYMtra26NKlC4YOHYpz5869rmaVSWm3xZkzZ2Bra4spU6bAzs4O77zzDsaNG6fWTg8PD/Tr1w+Ojo5o0KABPvroI7i4uODUqVOvq1mVhsGOiIjoNcrIyEBUVBS6dOmiNr5Lly4IDw8v0TI2bdoELy8v2NjYSOPeeecdREVF4ezZswCAv//+G4cOHUKPHlX3jUFl2RZubm64d+8eDh06BCEEHjx4gP/973+FtlMIgePHj+PGjRvo0KFDubehqmGvWCIiotfo33//RXZ2NiwsLNTGW1hYID4+vtj54+LicPjwYezcuVNt/JAhQ/Dw4UO88847EEIgKysL48ePx5w5c8q1/uWpLNvCzc0NO3bswODBg/H8+XNkZWWhd+/e+PLLL9XKJSUloU6dOkhPT4eGhga++uordO7cucLaUlXwjB0REVElUCgUap+FEPnGFWTLli2oWbMm+vbtqzY+NDQUn376Kb766iucP38ee/bswYEDB/Dxxx+XZ7UrRGm2xbVr1zBlyhQsWrQIUVFRCA4ORkxMDD788EO1ckqlEtHR0YiMjMSnn36KadOmITQ0tKKaUGXwjB0REdFrZGpqCg0NjXxnpBISEvKducpLCIHNmzdjxIgR0NbWVpu2cOFCjBgxQupE4OzsjNTUVIwdOxbz589HjRpV71xOWbZFQEAA2rVrh5kzZwIAXFxcYGBggPbt2+OTTz6BpaUlAKBGjRpo2LAhAKBZs2a4fv06AgIC4OHhUXENqgKq3rdMREQkY9ra2mjZsiVCQkLUxoeEhMDNza3IecPCwnDr1i2MHj0637Rnz57lC28aGhoQQkAI8eoVrwBl2RaFtRNAke0UQiA9Pf0Va1z18YwdERHRazZt2jSMGDECrVq1gqurK7799lvcuXNHupw4d+5c/PPPP9i2bZvafJs2bULbtm3h5OSUb5m9evXC6tWr0bx5c7Rt2xa3bt3CwoUL0bt3byn4VEWl3Ra9evXCmDFjsHHjRnTt2hVxcXHw8/NDmzZtYGVlBeDFWb1WrVqhQYMGyMjIwKFDh7Bt27ZCe9rKCYMdERHRazZ48GAkJiZi6dKliIuLg5OTEw4dOiT1co2Li8v3HLekpCTs3r0b69atK3CZCxYsgEKhwIIFC/DPP//AzMwMvXr1wqefflrh7XkVpd0Wvr6+ePr0KdavX4/p06ejZs2a6NixI1asWCGVSU1NxYQJE3Dv3j3o6enhrbfewvfff4/Bgwe/9va9bgpRVc/PFiE5ORnGxsZISkqCkZFRha7Lds7BCl3+myZWd1hlV6FqqSbviuVx8B8eA3lUk2OAqDKVJvfwHjsiIiIimWCwIyIiIpKJSg12pX3pLxEREREVrtKCXWlf+ktERERERau0YFfal/4SERERUdEqJdiVxwuQiYiIiEhdpTzHrrQv/U1PT1d7WnRS0ovu9cnJyRVbUQA56c8qfB1vkmTFG/d0nIr1GvbBqoDHwX94DORRTY4Bp8VHKrsKVcYV3fxvvajW5t6r8FXk5p2SPKGuUh9QXNKX/gYEBGDJkiX5xltbW1dY3ahgxpVdgapmObdIdcNvPA8eA9UOv/E8XuMx8PTpUxgbF72+Sgl2pX3p79y5czFt2jTpc05ODh49eoTatWsXGASpYiQnJ8Pa2hp3796t8AdDE1VFPAaouuMxUDmEEHj69Kn0yrSiVEqwe/mlv/369ZPGh4SEoE+fPvnK6+joQEdHR21czZo1K7qaVAgjIyMe0FSt8Rig6o7HwOtX3Jm6XJV2Kba4l/4SERERUelUWrAr7qW/RERERFQ6ldp5YsKECZgwYUJlVoFKQUdHB4sXL853WZyouuAxQNUdj4GqTyFK0neWiIiIiKq8Sn1XLBERERGVHwY7IiIiIplgsKNihYaGQqFQ4MmTJ5VdFaJKs2/fPjRs2BAaGhrw8/Or7OpI/P390axZs8quRoFsbW2xdu3ayq5GteDr64u+fftWdjVK5U2sc15V8fhjsKtmvv76ayiVSmRlZUnjUlJSoKWlhfbt26uV/e2336BQKGBlZYW4uLgSP0OHXq/w8HBoaGigW7dur2V9qampmD17NurXrw9dXV2YmZnBw8MDBw4ckMpUlT/oW7ZsgUKhkAZDQ0O0bNkSe/bsKfWyxo0bh4EDB+Lu3bv4+OOPK6C2hcvbjtzhu+++w4wZM3D8+PFXWr6Hh0eJwqqHhwcUCgWWL1+eb1r37t2hUCjg7+8vjYuMjMTYsWNfqW5Vxes+zvz9/aFQKApc38qVK6FQKODh4SGNW7duHbZs2fJa6pZX7n/+ixoKqltl1lnOKrVXLL1+np6eSElJwblz5/D2228DeBHgVCoVIiMj8ezZM+jr6wN4cbBaWVnBwcGhMqtMxdi8eTMmT56M7777Dnfu3EG9evUqdH0ffvghzp49i/Xr16Nx48ZITExEeHg4EhMTK3S9ZWVkZIQbN24AePE6nsDAQAwaNAhXr15Fo0aNSrSMlJQUJCQkoGvXriV68nthMjIyoK2tXaZ5X25HLmNjY+jp6cHQ0LBC1lkQa2trBAYGYs6cOdK4+/fv48SJE7C0tFQra2ZmVm7rrWyv+zgDAEtLS5w8eRL37t1D3bp1pfGBgYH51l+Z//F2c3NDXFyc9Pmjjz5CcnIyAgMDpXEv1y87OxsKhaLKnCzIzMyElpZWZVej3PCMXTXTqFEjWFlZITQ0VBoXGhqKPn36oEGDBggPD1cb7+npme9S7JYtW1CzZk0cOXIEjo6OMDQ0RLdu3dQO7NDQULRp0wYGBgaoWbMm2rVrh9u3b7+uZlYbqamp+PHHHzF+/Hj07NlT7X+/rq6uan98AeDhw4fQ0tLCyZMnAQBxcXHo0aMH9PT0YGdnh507dxZ7tu2XX37BvHnz0L17d9ja2qJly5aYPHkyfHx8ALw4q3P79m1MnTpV+t96rvDwcHTo0AF6enqwtrbGlClTkJqaKk23tbXFxx9/jGHDhsHQ0BBWVlb48ssv1dbv7++PevXqQUdHB1ZWVpgyZUqR20ihUEClUkGlUsHe3h6ffPIJatSogUuXLkllMjIyMGvWLNSpUwcGBgZo27atdIyEhoZCqVQCADp27AiFQiFN2717N5o0aQIdHR3Y2tpi1apVauu2tbXFJ598Al9fXxgbG2PMmDEl2g7FtSN30NPTy3cpKPfyVkBAgNp/zL766ivY29tDV1cXFhYWGDhwoFQ+LCwM69atk76v2NjYQuvRs2dPJCYm4vTp09K4LVu2oEuXLjA3N8/X/pf3pdyzjP369YO+vj7s7e2xf/9+tXmuXbuG7t27w9DQEBYWFhgxYgT+/fffIrdNRauM4wwAzM3N0aVLF2zdulUaFx4ejn///Rc9evRQK5v3sqaHhwemTJmCWbNmwcTEBCqVSu1sKgAkJSVh7NixMDc3h5GRETp27IiLFy9K0y9evAhPT08olUoYGRmhZcuWOHfuXL56amtr59svdXR0pM/BwcGwtLTEgQMH0LhxY+jo6OD27dv56vz06VMMHz4cBgYGsLS0xJo1a/KdTS7JtiyuXbnHzObNm1G/fn3o6OhACFHsfACwfPlyWFhYQKlUYvTo0Xj+/HlRX2GlYLCrhjw8PKQfHAA4efIkPDw84O7uLo3PyMhAREQEPD09C1zGs2fP8Pnnn2P79u349ddfcefOHcyYMQMAkJWVhb59+8Ld3R2XLl1CREQExo4dy/f6VoBdu3ahUaNGaNSoEd577z0EBgYi9wlGw4cPxw8//ICXn2i0a9cuWFhYwN3dHQAwcuRI3L9/H6Ghodi9eze+/fZbJCQkFLlOlUqFQ4cO4enTpwVO37NnD+rWrSs9fDw38F++fBldu3ZF//79cenSJezatQunTp3CpEmT1Ob/7LPP4OLigvPnz2Pu3LmYOnUqQkJCAAD/+9//sGbNGnzzzTe4efMm9u3bB2dn5xJvr+zsbOmPZIsWLaTx77//Pk6fPo2goCBcunQJ7777Lrp164abN2/Czc1NOlO2e/duxMXFwc3NDVFRURg0aBCGDBmCy5cvw9/fHwsXLsx3aemzzz6Dk5MToqKisHDhwhJvh1dx/PhxXL9+HSEhIThw4ADOnTuHKVOmYOnSpbhx4waCg4PRoUMHAC8uh7m6umLMmDHS92VtbV3osrW1tTF8+HC1szFbtmzBqFGjSlS3JUuWYNCgQbh06RK6d++O4cOH49GjRwBe/NF2d3dHs2bNcO7cOQQHB+PBgwcYNGjQK2yNV1cZx1muUaNGqe1TmzdvxvDhw0t0Fnbr1q0wMDDA77//jpUrV2Lp0qXSsSSEQI8ePRAfH49Dhw4hKioKLVq0QKdOnaTvY/jw4ahbty4iIyMRFRWFOXPmlPnM1rNnzxAQEIDvvvsOV69ezfefAODFG6lOnz6N/fv3IyQkBL/99hvOnz+vVqa4bVmSdgHArVu38OOPP2L37t2Ijo4GgGLn+/HHH7F48WJ8+umnOHfuHCwtLfHVV1+VaXtUKEHVzrfffisMDAxEZmamSE5OFpqamuLBgwciKChIuLm5CSGECAsLEwDEX3/9JU6ePCkAiMePHwshhAgMDBQAxK1bt6RlbtiwQVhYWAghhEhMTBQARGho6GtvW3Xj5uYm1q5dK4QQIjMzU5iamoqQkBAhhBAJCQlCU1NT/Prrr1J5V1dXMXPmTCGEENevXxcARGRkpDT95s2bAoBYs2ZNoesMCwsTdevWFVpaWqJVq1bCz89PnDp1Sq2MjY1NvmWMGDFCjB07Vm3cb7/9JmrUqCHS0tKk+bp166ZWZvDgwcLb21sIIcSqVauEg4ODyMjIKG7TCCH+21cNDAyEgYGBqFGjhtDR0RGBgYFSmVu3bgmFQiH++ecftXk7deok5s6dK4QQ4vHjxwKAOHnypDR92LBhonPnzmrzzJw5UzRu3FhtO/Tt27fU26G4dhgYGEjH2+LFi0XTpk2lsj4+PsLCwkKkp6dL43bv3i2MjIxEcnJygct3d3cXH330UYHTCip38eJFoVQqRUpKiggLCxPm5uYiIyNDNG3aVCxevFit/S/vBwDEggULpM8pKSlCoVCIw4cPCyGEWLhwoejSpYvaOu/evSsAiBs3bhRbv4pSGcdZ7veakZEhzM3NRVhYmEhJSRFKpVJcvHhRfPTRR8Ld3V0q7+PjI/r06SN9dnd3F++8847aMlu3bi1mz54thBDi+PHjwsjISDx//lytTIMGDcQ333wjhBBCqVSKLVu2lHAr/SdvXXL33+jo6ELLJScnCy0tLfHTTz9J0588eSL09fWlfbMk27Ik7Vq8eLHQ0tISCQkJ0vSSzOfq6io+/PBDtelt27ZVO/6qAp6xq4Y8PT2RmpqKyMhI/Pbbb3BwcIC5uTnc3d0RGRmJ1NRUhIaGol69eqhfv36By9DX10eDBg2kz5aWltL/mkxMTODr64uuXbuiV69eWLdundplWiofN27cwNmzZzFkyBAAgKamJgYPHozNmzcDeHF/U+fOnbFjxw4AQExMDCIiIjB8+HBpfk1NTbUzVw0bNkStWrWKXG+HDh3w999/4/jx4xgwYACuXr2K9u3bF9uhICoqClu2bIGhoaE0dO3aFTk5OYiJiZHKubq6qs3n6uqK69evAwDeffddpKWloX79+hgzZgz27t2r1hGoIEqlEtHR0YiOjsaFCxewbNkyjBs3Dr/88gsA4Pz58xBCwMHBQa1uYWFh+Ouvvwpd7vXr19GuXTu1ce3atcPNmzeRnZ0tjWvVqlWZtkNR7YiOjla7bSIvZ2dntTM6nTt3ho2NDerXr48RI0Zgx44dePbsWaHzF8fFxQX29vb43//+h82bN2PEiBElPpPj4uIi/dvAwABKpVL67YiKisLJkyfVts1bb70FAEV+FxWpso6zXFpaWtJZwp9++gkODg5q27Aoecu9/DsdFRWFlJQU1K5dW217x8TESNt62rRp+OCDD+Dl5YXly5e/0negra1dZL3//vtvZGZmok2bNtI4Y2NjtftgS7ItS9IuALCxsVG7B7Qk812/fr3A36eqhp0nqqGGDRuibt26OHnyJB4/fixdLlCpVLCzs8Pp06dx8uRJdOzYsdBl5P0RVygUapciAgMDMWXKFAQHB2PXrl1YsGABQkJCpA4b9Oo2bdqErKws1KlTRxonhICWlhYeP36MWrVqYfjw4fjoo4/w5ZdfYufOnWjSpAmaNm0qlS1IYeNfltuLun379pgzZw4++eQTLF26FLNnzy70ElFOTg7GjRtX4D1xxd2InnsZ39raGjdu3EBISAiOHTuGCRMm4LPPPkNYWFihwaJGjRpo2LCh9NnFxQVHjx7FihUr0KtXL+Tk5EBDQwNRUVHQ0NBQm7eoTglCiHy3FxS07QwMDNQ+l3U75G1HUfKuU6lU4vz58wgNDcXRo0exaNEi+Pv7IzIyEjVr1izRMvMaNWoUNmzYgGvXruHs2bMlnq+g346cnBwAL7ZNr169sGLFinzz5e2Y8bpU5nGWa9SoUWjbti2uXLlS4kveQPHb2tLSUu1+61y5+4S/vz+GDRuGgwcP4vDhw1i8eDGCgoLQr1+/Etchl56eXpG34+Ruj6KOqZJsy5K0Cyj4uCzJfG8CnrGrpnI7RYSGhqp1mXd3d8eRI0dw5syZQu+vK6nmzZtj7ty5CA8Ph5OTE3bu3PmKtaZcWVlZ2LZtG1atWqV2FufixYuwsbGRzh707dsXz58/R3BwMHbu3In33ntPWsZbb72FrKwsXLhwQRp369atMj2vsHHjxsjKypJuJNbW1lY7awW8uKft6tWraNiwYb7h5TB45swZtfnOnDkjnbUBXvyB6N27N7744guEhoYiIiICly9fLlV9NTQ0kJaWBuDFfpqdnY2EhIR89VKpVEW2+dSpU2rjwsPD4eDgkC8glmU7lDdNTU14eXlh5cqVuHTpEmJjY3HixAkABX9fxRk2bBguX74MJycnNG7cuFzqmLttbG1t822bvH+IX4eqcpw1adIETZo0wZUrVzBs2LByaVuLFi0QHx8PTU3NfNva1NRUKufg4ICpU6fi6NGj6N+/v9q9leWpQYMG0NLSUvtPQnJyMm7evCl9Lsm2LGm78irJfI6OjgX+PlU1DHbVlKenJ06dOoXo6GjpjB3wItj93//9H54/f17mYBcTE4O5c+ciIiICt2/fxtGjR/Hnn3/C0dGxvKpf7R04cACPHz/G6NGj4eTkpDYMHDgQmzZtAvDif6V9+vTBwoULcf36dbU/Cm+99Ra8vLwwduxYnD17FhcuXMDYsWOL/Z+1h4cHvvnmG0RFRSE2NhaHDh3CvHnz4OnpCSMjIwAvekP++uuv+Oeff6QejbNnz0ZERAQmTpyI6Oho3Lx5E/v378fkyZPVln/69GmsXLkSf/75JzZs2ICffvoJH330EYAXN+lv2rQJV65cwd9//43t27dDT08PNjY2hdZXCIH4+HjEx8cjJiYG3377LY4cOYI+ffoAePGHa/jw4Rg5ciT27NmDmJgYREZGYsWKFTh06FChy50+fTqOHz+Ojz/+GH/++Se2bt2K9evXS52IClPS7VCeDhw4gC+++ALR0dG4ffs2tm3bhpycHOkyl62tLX7//XfExsbi33//lc7qFKVWrVqIi4t75WfovWzixIl49OgRhg4dirNnz+Lvv//G0aNHMWrUqFIHz/JQmcdZXidOnEBcXFy5nT3y8vKCq6sr+vbtiyNHjiA2Nhbh4eFYsGABzp07h7S0NEyaNAmhoaG4ffs2Tp8+jcjIyAr7HVcqlfDx8cHMmTNx8uRJXL16FaNGjUKNGjWk7VSSbVlcu8q6PYAXj3HZvHkzNm/ejD///BOLFy/G1atXK2R7vAoGu2rK09MTaWlpaNiwISwsLKTx7u7uePr0KRo0aFBkz7ii6Ovr448//sCAAQPg4OCAsWPHYtKkSRg3blx5Vb/a27RpE7y8vAp8DtSAAQMQHR0t9SYbPnw4Ll68iPbt2+e71Ldt2zZYWFigQ4cO6NevH8aMGQOlUgldXd1C1921a1ds3boVXbp0gaOjIyZPnoyuXbvixx9/lMosXboUsbGxaNCggXQfi4uLC8LCwnDz5k20b98ezZs3x8KFC/NdYps+fTqioqLQvHlzfPzxx1i1ahW6du0K4MUlkf/7v/9Du3bt4OLiguPHj+OXX35B7dq1C61vcnIyLC0tYWlpCUdHR6xatQpLly7F/PnzpTKBgYEYOXIkpk+fjkaNGqF37974/fffizwGWrRogR9//BFBQUFwcnLCokWLsHTpUvj6+hY6T2m2Q3mqWbMm9uzZg44dO8LR0RFff/01fvjhBzRp0gQAMGPGDGhoaKBx48YwMzPDnTt3Srzc8jyTZmVlhdOnTyM7Oxtdu3aFk5MTPvroIxgbG6NGjdf/56oyj7O8ch8dVV4UCgUOHTqEDh06YNSoUXBwcMCQIUMQGxsLCwsLaGhoIDExESNHjoSDgwMGDRoEb29vLFmypNzqkNfq1avh6uqKnj17wsvLC+3atYOjo6PadipuWxbXrrJuDwAYPHgwFi1ahNmzZ6Nly5a4ffs2xo8fX2Hbo6wUojQX+olI1u7duwdra2scO3YMnTp1eu3rt7W1hZ+fX5V6ZRdReavs4+xNkZqaijp16mDVqlUYPXp0gWW4LfNj5wmiauzEiRNISUmBs7Mz4uLiMGvWLNja2krPOCOiV8fjrGQuXLiAP/74A23atEFSUhKWLl0KANJtEwC3ZUkw2BFVY5mZmZg3bx7+/vtvKJVKuLm5YceOHbJ6vQ5RZeNxVnKff/45bty4AW1tbbRs2RK//fabWqcHbsvi8VIsERERkUyw8wQRERGRTDDYEREREckEgx0RERGRTDDYEREREckEgx0RERGRTDDYERGVE4VCgX379lV2NYioGmOwIyJZ8fX1hUKhwIcffphv2oQJE6BQKIp97Veu0NBQKBSKEr+wPS4uDt7e3qWoLRFR+WKwIyLZsba2RlBQENLS0qRxz58/xw8//JDvPZ7lISMjAwCgUqmgo6NT7ssnIiopBjsikp0WLVqgXr162LNnjzRuz549sLa2RvPmzaVxQgisXLkS9evXh56eHpo2bYr//e9/AIDY2Fh4enoCAGrVqqV2ps/DwwOTJk3CtGnTYGpqis6dOwPIfyn23r17GDJkCExMTGBgYIBWrVrh999/BwBcvHgRnp6eUCqVMDIyQsuWLXHu3LmK3CxEVA3wlWJEJEvvv/8+AgMDMXz4cADA5s2bMWrUKISGhkplFixYgD179mDjxo2wt7fHr7/+ivfeew9mZmZ45513sHv3bgwYMAA3btyAkZER9PT0pHm3bt2K8ePH4/Tp0yjoBT4pKSlwd3dHnTp1sH//fqhUKpw/fx45OTkAgOHDh6N58+bYuHEjNDQ0EB0dzdciEdErY7AjIlkaMWIE5s6di9jYWCgUCpw+fRpBQUFSsEtNTcXq1atx4sQJuLq6AgDq16+PU6dO4ZtvvoG7uztMTEwAAObm5qhZs6ba8hs2bIiVK1cWuv6dO3fi4cOHiIyMlJbTsGFDafqdO3cwc+ZMvPXWWwAAe3v78mo6EVVjDHZEJEumpqbo0aMHtm7dCiEEevToofYy8WvXruH58+fSZdRcGRkZapdrC9OqVasip0dHR6N58+ZSqMtr2rRp+OCDD7B9+3Z4eXnh3XffRYMGDUrQMiKiwjHYEZFsjRo1CpMmTQIAbNiwQW1a7iXRgwcPok6dOmrTStIBwsDAoMjpL1+2LYi/vz+GDRuGgwcP4vDhw1i8eDGCgoLQr1+/YtdNRFQYdp4gItnq1q0bMjIykJGRga5du6pNa9y4MXR0dHDnzh00bNhQbbC2tgYAaGtrAwCys7NLvW4XFxdER0fj0aNHhZZxcHDA1KlTcfToUfTv3x+BgYGlXg8R0csY7IhItjQ0NHD9+nVcv34dGhoaatOUSiVmzJiBqVOnYuvWrfjrr79w4cIFbNiwAVu3bgUA2NjYQKFQ4MCBA3j48CFSUlJKvO6hQ4dCpVKhb9++OH36NP7++2/s3r0bERERSEtLw6RJkxAaGorbt2/j9OnTiIyMhKOjY7m2n4iqHwY7IpI1IyMjGBkZFTjt448/xqJFixAQEABHR0d07doVv/zyC+zs7AAAderUwZIlSzBnzhxYWFhIl3VLQltbG0ePHoW5uTm6d+8OZ2dnLF++HBoaGtDQ0EBiYiJGjhwJBwcHDBo0CN7e3liyZEm5tJmIqi+FKKifPhERERG9cXjGjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZOL/AaZHsRfEkhEwAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data preparation\n",
        "categories = ['Wins', 'Avg Steps Before First Mine', 'Avg Mines Triggered']\n",
        "nn_results = [25, 364.95, 0.78]\n",
        "logic_results = [20, 305.74, 0.83]\n",
        "\n",
        "x = np.arange(len(categories))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, nn_results, width, label='Network Bot')\n",
        "rects2 = ax.bar(x + width/2, logic_results, width, label='Logic Bot')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_title('Comparison of Network Bot and Logic Bot Performance')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories)\n",
        "ax.legend()\n",
        "\n",
        "# Function to attach a label above each bar in *rects*, displaying its height.\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b1cd2d-f7db-487a-ae7a-5d3675b54301",
      "metadata": {
        "id": "45b1cd2d-f7db-487a-ae7a-5d3675b54301"
      },
      "source": [
        "**Wins:** The Neural Network Bot achieved 25 wins, showing a noticeable improvement over the 20 wins by the Logic Bot. This indicates that the machine learning model is more effective at navigating through the minefields, potentially recognizing patterns and risky areas better than the rule-based logic of the traditional bot.\n",
        "\n",
        "**Average Steps Before First Mine:** The plot also shows a significant difference in the average number of steps taken before the first mine is triggered. The Neural Network Bot averages around 365 steps, considerably higher than the 306 steps by the Logic Bot. This suggests that the Neural Network Bot not only plays a safer game but also manages to explore more of the board without hitting a mine, which is crucial for achieving higher win rates in Minesweeper.\n",
        "\n",
        "**Average Mines Triggered:** Both bots are relatively close in the average number of mines triggered, with the Neural Network Bot at 0.78 and the Logic Bot at 0.83. While the difference is minimal, it still points to a slight edge for the Neural Network Bot in terms of cautious gameplay."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845f913f-b9a5-49ad-a271-92cf4e0c809a",
      "metadata": {
        "id": "845f913f-b9a5-49ad-a271-92cf4e0c809a"
      },
      "source": [
        "#**Task 3:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3634f42-2de8-4361-99c0-300748f261ae",
      "metadata": {
        "id": "f3634f42-2de8-4361-99c0-300748f261ae"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class FullyConvMinesweeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConvMinesweeperCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)  # Corrected to single input channel\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return torch.sigmoid(self.conv3(x)).squeeze(1)\n",
        "\n",
        "# Define the dataset\n",
        "class MinesweeperDataset(Dataset):\n",
        "    def __init__(self, boards, labels):\n",
        "        self.boards = boards\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.boards)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.boards[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "# Dataset creation function\n",
        "def create_dataset(num_samples, min_size, max_size, mine_probability):\n",
        "    boards = []\n",
        "    labels = []\n",
        "    for _ in range(num_samples):\n",
        "        size = np.random.randint(min_size, max_size + 1)\n",
        "        board = np.zeros((1, size, size))  # Single-channel input for mines\n",
        "        label = (np.random.rand(size, size) < mine_probability).astype(float)\n",
        "        boards.append(board)\n",
        "        labels.append(label)\n",
        "    return MinesweeperDataset(boards, labels)\n",
        "\n",
        "# Collate function to handle batches\n",
        "def custom_collate(batch):\n",
        "    max_height = max(board.shape[1] for board, _ in batch)\n",
        "    max_width = max(board.shape[2] for board, _ in batch)\n",
        "\n",
        "    padded_boards = []\n",
        "    padded_labels = []\n",
        "    for board, label in batch:\n",
        "        # Determine padding sizes for board\n",
        "        padding_height = max_height - board.shape[1]\n",
        "        padding_width = max_width - board.shape[2]\n",
        "        padded_board = F.pad(board, (0, padding_width, 0, padding_height))\n",
        "\n",
        "        # Ensure label is at least 3D (1 channel) for consistent padding\n",
        "        if label.dim() == 2:\n",
        "            label = label.unsqueeze(0)  # Add channel dimension if missing\n",
        "\n",
        "        # Determine padding sizes for label\n",
        "        padding_height_label = max_height - label.shape[1]\n",
        "        padding_width_label = max_width - label.shape[2]\n",
        "        padded_label = F.pad(label, (0, padding_width_label, 0, padding_height_label))\n",
        "\n",
        "        padded_boards.append(padded_board)\n",
        "        padded_labels.append(padded_label.squeeze(0))  # Remove channel dimension from labels if added\n",
        "\n",
        "    return torch.stack(padded_boards), torch.stack(padded_labels)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for boards, labels in train_loader:\n",
        "            boards, labels = boards.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(boards)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for boards, labels in val_loader:\n",
        "                boards, labels = boards.to(device), labels.to(device)\n",
        "                outputs = model(boards)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Training Loss: {total_train_loss / len(train_loader):.4f}, Validation Loss: {total_val_loss / len(val_loader):.4f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e95180d-f8c5-469d-b403-8e612fa89ca7",
      "metadata": {
        "id": "7e95180d-f8c5-469d-b403-8e612fa89ca7"
      },
      "outputs": [],
      "source": [
        "def simulate_game(model, board_size, mine_probability):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Generate a board\n",
        "    board = np.random.rand(board_size, board_size) < mine_probability\n",
        "    label = board.copy()  # Copy board to labels for simulation\n",
        "\n",
        "    # Convert board to tensor and add necessary dimensions\n",
        "    board_tensor = torch.tensor(board, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get predictions from the model\n",
        "    with torch.no_grad():\n",
        "        output = model(board_tensor).cpu()\n",
        "    prediction = (output > 0.5).squeeze()  # Remove unnecessary dimensions and apply threshold\n",
        "\n",
        "    # Calculate metrics for the game\n",
        "    steps_before_first_mine = 0\n",
        "    mines_triggered = 0\n",
        "    for i in range(board_size):\n",
        "        for j in range(board_size):\n",
        "            if prediction[i, j] and board[i, j]:  # Adjusted indexing for prediction\n",
        "                if mines_triggered == 0:\n",
        "                    steps_before_first_mine = i * board_size + j + 1\n",
        "                mines_triggered += 1\n",
        "\n",
        "    game_over = mines_triggered > 0\n",
        "    correct_predictions = (prediction == torch.tensor(board, dtype=torch.bool))\n",
        "    win = not game_over and correct_predictions.all().item()\n",
        "\n",
        "    return win, steps_before_first_mine, mines_triggered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3c09d0-e118-4622-8b8e-7ee6d94188bc",
      "metadata": {
        "id": "6b3c09d0-e118-4622-8b8e-7ee6d94188bc",
        "outputId": "d8bd583a-7693-4096-fbc8-0f3113f09e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss: 0.3371, Validation Loss: 0.3148\n",
            "Epoch 2, Training Loss: 0.2969, Validation Loss: 0.3128\n",
            "Epoch 3, Training Loss: 0.2933, Validation Loss: 0.3123\n",
            "Epoch 4, Training Loss: 0.2934, Validation Loss: 0.3121\n",
            "Epoch 5, Training Loss: 0.2945, Validation Loss: 0.3166\n",
            "Epoch 6, Training Loss: 0.2968, Validation Loss: 0.3112\n",
            "Epoch 7, Training Loss: 0.2940, Validation Loss: 0.3101\n",
            "Epoch 8, Training Loss: 0.2946, Validation Loss: 0.3100\n",
            "Epoch 9, Training Loss: 0.2912, Validation Loss: 0.3100\n",
            "Epoch 10, Training Loss: 0.2886, Validation Loss: 0.3097\n",
            "Epoch 11, Training Loss: 0.2898, Validation Loss: 0.3086\n",
            "Epoch 12, Training Loss: 0.2911, Validation Loss: 0.3086\n",
            "Epoch 13, Training Loss: 0.2933, Validation Loss: 0.3093\n",
            "Epoch 14, Training Loss: 0.2921, Validation Loss: 0.3091\n",
            "Epoch 15, Training Loss: 0.2882, Validation Loss: 0.3130\n",
            "Epoch 16, Training Loss: 0.2943, Validation Loss: 0.3091\n",
            "Epoch 17, Training Loss: 0.2890, Validation Loss: 0.3081\n",
            "Epoch 18, Training Loss: 0.2876, Validation Loss: 0.3086\n",
            "Epoch 19, Training Loss: 0.2913, Validation Loss: 0.3082\n",
            "Epoch 20, Training Loss: 0.2907, Validation Loss: 0.3080\n",
            "Epoch 21, Training Loss: 0.2907, Validation Loss: 0.3096\n",
            "Epoch 22, Training Loss: 0.2891, Validation Loss: 0.3082\n",
            "Epoch 23, Training Loss: 0.2891, Validation Loss: 0.3083\n",
            "Epoch 24, Training Loss: 0.2931, Validation Loss: 0.3105\n",
            "Epoch 25, Training Loss: 0.2911, Validation Loss: 0.3081\n",
            "Epoch 26, Training Loss: 0.2888, Validation Loss: 0.3082\n",
            "Epoch 27, Training Loss: 0.2929, Validation Loss: 0.3079\n",
            "Epoch 28, Training Loss: 0.2903, Validation Loss: 0.3082\n",
            "Epoch 29, Training Loss: 0.2872, Validation Loss: 0.3083\n",
            "Epoch 30, Training Loss: 0.2885, Validation Loss: 0.3080\n",
            "Epoch 31, Training Loss: 0.2936, Validation Loss: 0.3095\n",
            "Epoch 32, Training Loss: 0.2885, Validation Loss: 0.3082\n",
            "Epoch 33, Training Loss: 0.2897, Validation Loss: 0.3085\n",
            "Epoch 34, Training Loss: 0.2938, Validation Loss: 0.3080\n",
            "Epoch 35, Training Loss: 0.2873, Validation Loss: 0.3088\n",
            "Epoch 36, Training Loss: 0.2897, Validation Loss: 0.3080\n",
            "Epoch 37, Training Loss: 0.2891, Validation Loss: 0.3111\n",
            "Epoch 38, Training Loss: 0.2939, Validation Loss: 0.3082\n",
            "Epoch 39, Training Loss: 0.2885, Validation Loss: 0.3118\n",
            "Epoch 40, Training Loss: 0.2891, Validation Loss: 0.3137\n",
            "Epoch 41, Training Loss: 0.2922, Validation Loss: 0.3084\n",
            "Epoch 42, Training Loss: 0.2910, Validation Loss: 0.3079\n",
            "Epoch 43, Training Loss: 0.2927, Validation Loss: 0.3084\n",
            "Epoch 44, Training Loss: 0.2940, Validation Loss: 0.3080\n",
            "Epoch 45, Training Loss: 0.2869, Validation Loss: 0.3104\n",
            "Epoch 46, Training Loss: 0.2886, Validation Loss: 0.3084\n",
            "Epoch 47, Training Loss: 0.2935, Validation Loss: 0.3117\n",
            "Epoch 48, Training Loss: 0.2918, Validation Loss: 0.3095\n",
            "Epoch 49, Training Loss: 0.2913, Validation Loss: 0.3093\n",
            "Epoch 50, Training Loss: 0.2874, Validation Loss: 0.3148\n",
            "Epoch 51, Training Loss: 0.2894, Validation Loss: 0.3081\n",
            "Epoch 52, Training Loss: 0.2908, Validation Loss: 0.3091\n",
            "Epoch 53, Training Loss: 0.2916, Validation Loss: 0.3081\n",
            "Epoch 54, Training Loss: 0.2912, Validation Loss: 0.3087\n",
            "Epoch 55, Training Loss: 0.2903, Validation Loss: 0.3088\n",
            "Epoch 56, Training Loss: 0.2902, Validation Loss: 0.3090\n",
            "Epoch 57, Training Loss: 0.2912, Validation Loss: 0.3081\n",
            "Epoch 58, Training Loss: 0.2939, Validation Loss: 0.3079\n",
            "Epoch 59, Training Loss: 0.2877, Validation Loss: 0.3084\n",
            "Epoch 60, Training Loss: 0.2881, Validation Loss: 0.3114\n",
            "Epoch 61, Training Loss: 0.2903, Validation Loss: 0.3086\n",
            "Epoch 62, Training Loss: 0.2923, Validation Loss: 0.3080\n",
            "Epoch 63, Training Loss: 0.2925, Validation Loss: 0.3090\n",
            "Epoch 64, Training Loss: 0.2918, Validation Loss: 0.3150\n",
            "Epoch 65, Training Loss: 0.2898, Validation Loss: 0.3080\n",
            "Epoch 66, Training Loss: 0.2896, Validation Loss: 0.3083\n",
            "Epoch 67, Training Loss: 0.2886, Validation Loss: 0.3079\n",
            "Epoch 68, Training Loss: 0.2886, Validation Loss: 0.3099\n",
            "Epoch 69, Training Loss: 0.2891, Validation Loss: 0.3087\n",
            "Epoch 70, Training Loss: 0.2877, Validation Loss: 0.3090\n",
            "Epoch 71, Training Loss: 0.2917, Validation Loss: 0.3089\n",
            "Epoch 72, Training Loss: 0.2891, Validation Loss: 0.3089\n",
            "Epoch 73, Training Loss: 0.2937, Validation Loss: 0.3082\n",
            "Epoch 74, Training Loss: 0.2871, Validation Loss: 0.3082\n",
            "Epoch 75, Training Loss: 0.2893, Validation Loss: 0.3081\n",
            "Epoch 76, Training Loss: 0.2905, Validation Loss: 0.3101\n",
            "Epoch 77, Training Loss: 0.2870, Validation Loss: 0.3093\n",
            "Epoch 78, Training Loss: 0.2899, Validation Loss: 0.3103\n",
            "Epoch 79, Training Loss: 0.2892, Validation Loss: 0.3083\n",
            "Epoch 80, Training Loss: 0.2905, Validation Loss: 0.3114\n",
            "Epoch 81, Training Loss: 0.2915, Validation Loss: 0.3079\n",
            "Epoch 82, Training Loss: 0.2915, Validation Loss: 0.3082\n",
            "Epoch 83, Training Loss: 0.2886, Validation Loss: 0.3083\n",
            "Epoch 84, Training Loss: 0.2932, Validation Loss: 0.3091\n",
            "Epoch 85, Training Loss: 0.2908, Validation Loss: 0.3083\n",
            "Epoch 86, Training Loss: 0.2880, Validation Loss: 0.3085\n",
            "Epoch 87, Training Loss: 0.2887, Validation Loss: 0.3089\n",
            "Epoch 88, Training Loss: 0.2855, Validation Loss: 0.3099\n",
            "Epoch 89, Training Loss: 0.2866, Validation Loss: 0.3102\n",
            "Epoch 90, Training Loss: 0.2902, Validation Loss: 0.3094\n",
            "Epoch 91, Training Loss: 0.2913, Validation Loss: 0.3089\n",
            "Epoch 92, Training Loss: 0.2917, Validation Loss: 0.3080\n",
            "Epoch 93, Training Loss: 0.2884, Validation Loss: 0.3080\n",
            "Epoch 94, Training Loss: 0.2910, Validation Loss: 0.3079\n",
            "Epoch 95, Training Loss: 0.2886, Validation Loss: 0.3087\n",
            "Epoch 96, Training Loss: 0.2869, Validation Loss: 0.3081\n",
            "Epoch 97, Training Loss: 0.2865, Validation Loss: 0.3079\n",
            "Epoch 98, Training Loss: 0.2928, Validation Loss: 0.3083\n",
            "Epoch 99, Training Loss: 0.2864, Validation Loss: 0.3102\n",
            "Epoch 100, Training Loss: 0.2931, Validation Loss: 0.3178\n",
            "Model loaded successfully.\n",
            "Board size from (5x5 to 50x50): Win Probability: 0.0000, Avg Steps Before First Mine: 0.00, Avg Mines Triggered: 0.00\n"
          ]
        }
      ],
      "source": [
        "# Save and load model\n",
        "model_path = 'trained_minesweeper_model.pth'\n",
        "train_dataset = create_dataset(1000, 5, 50, 0.20)\n",
        "val_dataset = create_dataset(300, 5, 50, 0.20)\n",
        "\n",
        "# Prepare the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, collate_fn=custom_collate, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10, collate_fn=custom_collate, shuffle=False)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = FullyConvMinesweeperCNN()\n",
        "trained_model = train_model(model, train_loader, val_loader)\n",
        "\n",
        "# Save the model\n",
        "torch.save(trained_model.state_dict(), model_path)\n",
        "\n",
        "# Check if the model file exists and load it for simulation\n",
        "if os.path.exists(model_path):\n",
        "    trained_model.load_state_dict(torch.load(model_path))\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file not found.\")\n",
        "\n",
        "# Example usage\n",
        "model = FullyConvMinesweeperCNN().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load('trained_minesweeper_model.pth'))\n",
        "\n",
        "sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]  # Board sizes to simulate\n",
        "mine_probability = 0.2  # Probability of a cell containing a mine\n",
        "num_games_per_size = 100  # Number of games to simulate per board size\n",
        "\n",
        "total_wins = 0\n",
        "total_steps_before_first_mine = 0\n",
        "total_mines_triggered = 0\n",
        "total_games = 0\n",
        "\n",
        "for size in sizes:\n",
        "    win_count = 0\n",
        "    steps_before_first_mine = 0\n",
        "    mines_triggered = 0\n",
        "\n",
        "    for _ in range(num_games_per_size):\n",
        "        win, steps, mines = simulate_game(model, size, mine_probability)\n",
        "        if win:\n",
        "            win_count += 1\n",
        "        steps_before_first_mine += steps\n",
        "        mines_triggered += mines\n",
        "\n",
        "    total_wins += win_count\n",
        "    total_steps_before_first_mine += steps_before_first_mine\n",
        "    total_mines_triggered += mines_triggered\n",
        "    total_games += num_games_per_size\n",
        "\n",
        "# Calculate averages\n",
        "avg_win_probability = total_wins / total_games\n",
        "avg_steps_before_first_mine = total_steps_before_first_mine / total_games if total_steps_before_first_mine > 0 else 0\n",
        "avg_mines_triggered = total_mines_triggered / total_games if total_mines_triggered > 0 else 0\n",
        "\n",
        "# Print consolidated results\n",
        "print(f\"Board size from (5x5 to 50x50): Win Probability: {avg_win_probability:.4f}, Avg Steps Before First Mine: {avg_steps_before_first_mine:.2f}, Avg Mines Triggered: {avg_mines_triggered:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b699c101-418b-49a9-b55e-f16161c61c90",
      "metadata": {
        "id": "b699c101-418b-49a9-b55e-f16161c61c90"
      },
      "source": [
        "**Question: I would like your network to be able to handle boards of any size 𝐾×𝐾 for 𝐾≥5. How does your model have to change in order to accommodate this? How does your data or training have to change?**\n",
        "\n",
        "To make my network handle any size of Minesweeper boards from 𝐾×𝐾 where 𝐾 is 5 or larger, I'd have to make a few adjustments. First, I'd switch to using a fully convolutional network. This means the network will use layers that can work with inputs of any size, making it flexible to handle different board sizes without needing changes in the structure.\n",
        "\n",
        "For the data and training, I'd also need to tweak how I manage different sizes of boards. Right now, training with boards of various sizes requires padding them to match the largest one in each batch. Instead, I would train with batches of the same size boards or adjust the batching strategy so that the network learns from different sizes directly without padding. This way, the network learns to recognize patterns and predict mines on boards that could be larger or smaller than what it saw during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1443ecf5-464a-4ed4-8be5-55246e86a065",
      "metadata": {
        "id": "1443ecf5-464a-4ed4-8be5-55246e86a065"
      },
      "source": [
        "#**Bonus Task:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc99538a-17df-4426-a006-007824172c3f",
      "metadata": {
        "id": "cc99538a-17df-4426-a006-007824172c3f",
        "outputId": "ce9fd617-c80d-40d6-b53e-25900e7e5b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | D Loss: 1.3508594036102295 | G Loss: 0.6828537583351135\n",
            "Epoch 1 | D Loss: 1.310128092765808 | G Loss: 0.6803613901138306\n",
            "Epoch 1 | D Loss: 1.2675795555114746 | G Loss: 0.6773263216018677\n",
            "Epoch 1 | D Loss: 1.227689504623413 | G Loss: 0.6738971471786499\n",
            "Epoch 1 | D Loss: 1.196332573890686 | G Loss: 0.6701850891113281\n",
            "Epoch 1 | D Loss: 1.1648558378219604 | G Loss: 0.6650014519691467\n",
            "Epoch 1 | D Loss: 1.1341605186462402 | G Loss: 0.6575888395309448\n",
            "Epoch 1 | D Loss: 1.1090079545974731 | G Loss: 0.6504510641098022\n",
            "Epoch 1 | D Loss: 1.0916945934295654 | G Loss: 0.6390897631645203\n",
            "Epoch 1 | D Loss: 1.0748624801635742 | G Loss: 0.6255398392677307\n",
            "Epoch 1 | D Loss: 1.0716842412948608 | G Loss: 0.6117859482765198\n",
            "Epoch 1 | D Loss: 1.077449083328247 | G Loss: 0.5926346778869629\n",
            "Epoch 1 | D Loss: 1.0891140699386597 | G Loss: 0.5723646879196167\n",
            "Epoch 1 | D Loss: 1.109316110610962 | G Loss: 0.551175594329834\n",
            "Epoch 1 | D Loss: 1.1260015964508057 | G Loss: 0.535661518573761\n",
            "Epoch 2 | D Loss: 1.168172001838684 | G Loss: 0.5199569463729858\n",
            "Epoch 2 | D Loss: 1.2046191692352295 | G Loss: 0.5051520466804504\n",
            "Epoch 2 | D Loss: 1.2207385301589966 | G Loss: 0.5111474990844727\n",
            "Epoch 2 | D Loss: 1.2610071897506714 | G Loss: 0.5130336284637451\n",
            "Epoch 2 | D Loss: 1.2648513317108154 | G Loss: 0.5277870893478394\n",
            "Epoch 2 | D Loss: 1.271761178970337 | G Loss: 0.545281171798706\n",
            "Epoch 2 | D Loss: 1.2633641958236694 | G Loss: 0.5725347399711609\n",
            "Epoch 2 | D Loss: 1.2573970556259155 | G Loss: 0.6015322804450989\n",
            "Epoch 2 | D Loss: 1.2634530067443848 | G Loss: 0.6301358938217163\n",
            "Epoch 2 | D Loss: 1.2793262004852295 | G Loss: 0.6568844318389893\n",
            "Epoch 2 | D Loss: 1.258928656578064 | G Loss: 0.6812073588371277\n",
            "Epoch 2 | D Loss: 1.2752735614776611 | G Loss: 0.7033554315567017\n",
            "Epoch 2 | D Loss: 1.2751502990722656 | G Loss: 0.7234085202217102\n",
            "Epoch 2 | D Loss: 1.2685130834579468 | G Loss: 0.7387877106666565\n",
            "Epoch 2 | D Loss: 1.2893493175506592 | G Loss: 0.7451961040496826\n",
            "Epoch 3 | D Loss: 1.290102243423462 | G Loss: 0.7449058294296265\n",
            "Epoch 3 | D Loss: 1.2999509572982788 | G Loss: 0.743762195110321\n",
            "Epoch 3 | D Loss: 1.2900328636169434 | G Loss: 0.7409202456474304\n",
            "Epoch 3 | D Loss: 1.2907979488372803 | G Loss: 0.7394567728042603\n",
            "Epoch 3 | D Loss: 1.27608323097229 | G Loss: 0.7330785989761353\n",
            "Epoch 3 | D Loss: 1.2599058151245117 | G Loss: 0.7337982654571533\n",
            "Epoch 3 | D Loss: 1.2454384565353394 | G Loss: 0.7321949005126953\n",
            "Epoch 3 | D Loss: 1.2193337678909302 | G Loss: 0.7327675223350525\n",
            "Epoch 3 | D Loss: 1.1890994310379028 | G Loss: 0.7371734380722046\n",
            "Epoch 3 | D Loss: 1.1643967628479004 | G Loss: 0.7426031827926636\n",
            "Epoch 3 | D Loss: 1.1313689947128296 | G Loss: 0.750551700592041\n",
            "Epoch 3 | D Loss: 1.1045259237289429 | G Loss: 0.7640921473503113\n",
            "Epoch 3 | D Loss: 1.0731916427612305 | G Loss: 0.7765897512435913\n",
            "Epoch 3 | D Loss: 1.0426182746887207 | G Loss: 0.7877507209777832\n",
            "Epoch 3 | D Loss: 1.0309029817581177 | G Loss: 0.7974894642829895\n",
            "Epoch 4 | D Loss: 0.9939894676208496 | G Loss: 0.8065798282623291\n",
            "Epoch 4 | D Loss: 0.9620187282562256 | G Loss: 0.8217319846153259\n",
            "Epoch 4 | D Loss: 0.9649240970611572 | G Loss: 0.8353343605995178\n",
            "Epoch 4 | D Loss: 0.9476544857025146 | G Loss: 0.8411346077919006\n",
            "Epoch 4 | D Loss: 0.9347555041313171 | G Loss: 0.8479921817779541\n",
            "Epoch 4 | D Loss: 0.9522210955619812 | G Loss: 0.853667140007019\n",
            "Epoch 4 | D Loss: 0.9763591289520264 | G Loss: 0.8450807929039001\n",
            "Epoch 4 | D Loss: 1.0209770202636719 | G Loss: 0.8313859701156616\n",
            "Epoch 4 | D Loss: 1.046072006225586 | G Loss: 0.8054289817810059\n",
            "Epoch 4 | D Loss: 1.0878561735153198 | G Loss: 0.7832897901535034\n",
            "Epoch 4 | D Loss: 1.1511039733886719 | G Loss: 0.7604576349258423\n",
            "Epoch 4 | D Loss: 1.2273467779159546 | G Loss: 0.7431485652923584\n",
            "Epoch 4 | D Loss: 1.285976767539978 | G Loss: 0.7365128993988037\n",
            "Epoch 4 | D Loss: 1.370276927947998 | G Loss: 0.7138450145721436\n",
            "Epoch 4 | D Loss: 1.4121403694152832 | G Loss: 0.7113302946090698\n",
            "Epoch 5 | D Loss: 1.4595646858215332 | G Loss: 0.7102994918823242\n",
            "Epoch 5 | D Loss: 1.534895420074463 | G Loss: 0.7121658325195312\n",
            "Epoch 5 | D Loss: 1.4576383829116821 | G Loss: 0.7333991527557373\n",
            "Epoch 5 | D Loss: 1.511045217514038 | G Loss: 0.75418621301651\n",
            "Epoch 5 | D Loss: 1.5053317546844482 | G Loss: 0.7678418159484863\n",
            "Epoch 5 | D Loss: 1.504936695098877 | G Loss: 0.7832536697387695\n",
            "Epoch 5 | D Loss: 1.440629243850708 | G Loss: 0.7813974618911743\n",
            "Epoch 5 | D Loss: 1.49123215675354 | G Loss: 0.7839345335960388\n",
            "Epoch 5 | D Loss: 1.4760562181472778 | G Loss: 0.8033137917518616\n",
            "Epoch 5 | D Loss: 1.4211854934692383 | G Loss: 0.8146199584007263\n",
            "Epoch 5 | D Loss: 1.3814191818237305 | G Loss: 0.8400707244873047\n",
            "Epoch 5 | D Loss: 1.3731064796447754 | G Loss: 0.8612239956855774\n",
            "Epoch 5 | D Loss: 1.3109354972839355 | G Loss: 0.8675000667572021\n",
            "Epoch 5 | D Loss: 1.2931013107299805 | G Loss: 0.8992735743522644\n",
            "Epoch 5 | D Loss: 1.2267956733703613 | G Loss: 0.9304259419441223\n",
            "Epoch 6 | D Loss: 1.1370491981506348 | G Loss: 0.970522940158844\n",
            "Epoch 6 | D Loss: 1.0899462699890137 | G Loss: 1.0061225891113281\n",
            "Epoch 6 | D Loss: 1.0343263149261475 | G Loss: 1.0311675071716309\n",
            "Epoch 6 | D Loss: 0.9984561204910278 | G Loss: 1.0447289943695068\n",
            "Epoch 6 | D Loss: 1.0011019706726074 | G Loss: 1.0400222539901733\n",
            "Epoch 6 | D Loss: 1.0391737222671509 | G Loss: 1.0103391408920288\n",
            "Epoch 6 | D Loss: 1.0503653287887573 | G Loss: 0.9822508692741394\n",
            "Epoch 6 | D Loss: 1.0750102996826172 | G Loss: 0.9748505353927612\n",
            "Epoch 6 | D Loss: 1.0883442163467407 | G Loss: 0.9502872228622437\n",
            "Epoch 6 | D Loss: 1.1008493900299072 | G Loss: 0.940827488899231\n",
            "Epoch 6 | D Loss: 1.1122275590896606 | G Loss: 0.9211306571960449\n",
            "Epoch 6 | D Loss: 1.1011828184127808 | G Loss: 0.9142369031906128\n",
            "Epoch 6 | D Loss: 1.11539626121521 | G Loss: 0.9099818468093872\n",
            "Epoch 6 | D Loss: 1.1328153610229492 | G Loss: 0.9034581184387207\n",
            "Epoch 6 | D Loss: 1.1229770183563232 | G Loss: 0.9086946845054626\n",
            "Epoch 7 | D Loss: 1.0767595767974854 | G Loss: 0.9134349822998047\n",
            "Epoch 7 | D Loss: 1.1583187580108643 | G Loss: 0.9062409400939941\n",
            "Epoch 7 | D Loss: 1.173058271408081 | G Loss: 0.8832576274871826\n",
            "Epoch 7 | D Loss: 1.1445401906967163 | G Loss: 0.889560878276825\n",
            "Epoch 7 | D Loss: 1.1798399686813354 | G Loss: 0.8900320529937744\n",
            "Epoch 7 | D Loss: 1.2652788162231445 | G Loss: 0.857477605342865\n",
            "Epoch 7 | D Loss: 1.299836277961731 | G Loss: 0.8047597408294678\n",
            "Epoch 7 | D Loss: 1.345132827758789 | G Loss: 0.7820334434509277\n",
            "Epoch 7 | D Loss: 1.3431057929992676 | G Loss: 0.7661073207855225\n",
            "Epoch 7 | D Loss: 1.3889038562774658 | G Loss: 0.7424396276473999\n",
            "Epoch 7 | D Loss: 1.45680832862854 | G Loss: 0.726952314376831\n",
            "Epoch 7 | D Loss: 1.393585443496704 | G Loss: 0.7338365912437439\n",
            "Epoch 7 | D Loss: 1.4290605783462524 | G Loss: 0.7479479312896729\n",
            "Epoch 7 | D Loss: 1.396815299987793 | G Loss: 0.7711330056190491\n",
            "Epoch 7 | D Loss: 1.3774845600128174 | G Loss: 0.7844767570495605\n",
            "Epoch 8 | D Loss: 1.3426058292388916 | G Loss: 0.8013429641723633\n",
            "Epoch 8 | D Loss: 1.2749300003051758 | G Loss: 0.8210557699203491\n",
            "Epoch 8 | D Loss: 1.298241376876831 | G Loss: 0.8413259387016296\n",
            "Epoch 8 | D Loss: 1.2555081844329834 | G Loss: 0.8610144257545471\n",
            "Epoch 8 | D Loss: 1.2689623832702637 | G Loss: 0.8801596164703369\n",
            "Epoch 8 | D Loss: 1.2623202800750732 | G Loss: 0.8998136520385742\n",
            "Epoch 8 | D Loss: 1.271467924118042 | G Loss: 0.8996291160583496\n",
            "Epoch 8 | D Loss: 1.2490272521972656 | G Loss: 0.898063063621521\n",
            "Epoch 8 | D Loss: 1.1763997077941895 | G Loss: 0.8986269235610962\n",
            "Epoch 8 | D Loss: 1.2369027137756348 | G Loss: 0.8798649311065674\n",
            "Epoch 8 | D Loss: 1.2878261804580688 | G Loss: 0.8656953573226929\n",
            "Epoch 8 | D Loss: 1.3539884090423584 | G Loss: 0.865660548210144\n",
            "Epoch 8 | D Loss: 1.292968988418579 | G Loss: 0.8601744174957275\n",
            "Epoch 8 | D Loss: 1.30954909324646 | G Loss: 0.8596078157424927\n",
            "Epoch 8 | D Loss: 1.3596773147583008 | G Loss: 0.8434408903121948\n",
            "Epoch 9 | D Loss: 1.2791800498962402 | G Loss: 0.8498917818069458\n",
            "Epoch 9 | D Loss: 1.3780264854431152 | G Loss: 0.8449157476425171\n",
            "Epoch 9 | D Loss: 1.3218779563903809 | G Loss: 0.847662091255188\n",
            "Epoch 9 | D Loss: 1.3293153047561646 | G Loss: 0.8528849482536316\n",
            "Epoch 9 | D Loss: 1.2776784896850586 | G Loss: 0.8733368515968323\n",
            "Epoch 9 | D Loss: 1.2321559190750122 | G Loss: 0.9092905521392822\n",
            "Epoch 9 | D Loss: 1.2313882112503052 | G Loss: 0.9396091103553772\n",
            "Epoch 9 | D Loss: 1.1790446043014526 | G Loss: 0.9618609547615051\n",
            "Epoch 9 | D Loss: 1.1816434860229492 | G Loss: 0.966842532157898\n",
            "Epoch 9 | D Loss: 1.155471682548523 | G Loss: 0.9960948824882507\n",
            "Epoch 9 | D Loss: 1.120434284210205 | G Loss: 1.0199456214904785\n",
            "Epoch 9 | D Loss: 1.080481767654419 | G Loss: 1.046769380569458\n",
            "Epoch 9 | D Loss: 1.1059678792953491 | G Loss: 1.0609915256500244\n",
            "Epoch 9 | D Loss: 1.0798932313919067 | G Loss: 1.066463828086853\n",
            "Epoch 9 | D Loss: 1.0792244672775269 | G Loss: 1.0794224739074707\n",
            "Epoch 10 | D Loss: 1.0910615921020508 | G Loss: 1.0666697025299072\n",
            "Epoch 10 | D Loss: 1.126891016960144 | G Loss: 1.0458276271820068\n",
            "Epoch 10 | D Loss: 1.1470147371292114 | G Loss: 1.028879165649414\n",
            "Epoch 10 | D Loss: 1.201480507850647 | G Loss: 0.99153071641922\n",
            "Epoch 10 | D Loss: 1.1990652084350586 | G Loss: 0.9726767539978027\n",
            "Epoch 10 | D Loss: 1.2694644927978516 | G Loss: 0.9395029544830322\n",
            "Epoch 10 | D Loss: 1.3186259269714355 | G Loss: 0.9313470125198364\n",
            "Epoch 10 | D Loss: 1.373497486114502 | G Loss: 0.9212678670883179\n",
            "Epoch 10 | D Loss: 1.3562135696411133 | G Loss: 0.9023701548576355\n",
            "Epoch 10 | D Loss: 1.4346226453781128 | G Loss: 0.895260214805603\n",
            "Epoch 10 | D Loss: 1.458612084388733 | G Loss: 0.8820319175720215\n",
            "Epoch 10 | D Loss: 1.46612548828125 | G Loss: 0.8740949034690857\n",
            "Epoch 10 | D Loss: 1.516698956489563 | G Loss: 0.8575561046600342\n",
            "Epoch 10 | D Loss: 1.504396915435791 | G Loss: 0.8492708206176758\n",
            "Epoch 10 | D Loss: 1.5438593626022339 | G Loss: 0.833369255065918\n",
            "Epoch 11 | D Loss: 1.5545380115509033 | G Loss: 0.8242950439453125\n",
            "Epoch 11 | D Loss: 1.6296429634094238 | G Loss: 0.8392816185951233\n",
            "Epoch 11 | D Loss: 1.6105873584747314 | G Loss: 0.8554220199584961\n",
            "Epoch 11 | D Loss: 1.5943522453308105 | G Loss: 0.8765104413032532\n",
            "Epoch 11 | D Loss: 1.5341212749481201 | G Loss: 0.8993110060691833\n",
            "Epoch 11 | D Loss: 1.4960086345672607 | G Loss: 0.912540078163147\n",
            "Epoch 11 | D Loss: 1.5230441093444824 | G Loss: 0.9190006256103516\n",
            "Epoch 11 | D Loss: 1.4627667665481567 | G Loss: 0.936948835849762\n",
            "Epoch 11 | D Loss: 1.4353071451187134 | G Loss: 0.9637186527252197\n",
            "Epoch 11 | D Loss: 1.387514591217041 | G Loss: 0.9892681241035461\n",
            "Epoch 11 | D Loss: 1.361069679260254 | G Loss: 1.027733325958252\n",
            "Epoch 11 | D Loss: 1.3216197490692139 | G Loss: 1.0525095462799072\n",
            "Epoch 11 | D Loss: 1.2784481048583984 | G Loss: 1.0815186500549316\n",
            "Epoch 11 | D Loss: 1.2486443519592285 | G Loss: 1.0922635793685913\n",
            "Epoch 11 | D Loss: 1.2678778171539307 | G Loss: 1.1061053276062012\n",
            "Epoch 12 | D Loss: 1.244494915008545 | G Loss: 1.1149277687072754\n",
            "Epoch 12 | D Loss: 1.266137719154358 | G Loss: 1.113885521888733\n",
            "Epoch 12 | D Loss: 1.238935112953186 | G Loss: 1.1051933765411377\n",
            "Epoch 12 | D Loss: 1.2096023559570312 | G Loss: 1.0982372760772705\n",
            "Epoch 12 | D Loss: 1.2604142427444458 | G Loss: 1.0845032930374146\n",
            "Epoch 12 | D Loss: 1.2557744979858398 | G Loss: 1.0683974027633667\n",
            "Epoch 12 | D Loss: 1.2877962589263916 | G Loss: 1.0693728923797607\n",
            "Epoch 12 | D Loss: 1.291844129562378 | G Loss: 1.0661115646362305\n",
            "Epoch 12 | D Loss: 1.3407812118530273 | G Loss: 1.0582342147827148\n",
            "Epoch 12 | D Loss: 1.3357806205749512 | G Loss: 1.0521180629730225\n",
            "Epoch 12 | D Loss: 1.327908992767334 | G Loss: 1.0481719970703125\n",
            "Epoch 12 | D Loss: 1.347433090209961 | G Loss: 1.0380278825759888\n",
            "Epoch 12 | D Loss: 1.3936398029327393 | G Loss: 1.0436924695968628\n",
            "Epoch 12 | D Loss: 1.2584598064422607 | G Loss: 1.0733206272125244\n",
            "Epoch 12 | D Loss: 1.3483225107192993 | G Loss: 1.0820153951644897\n",
            "Epoch 13 | D Loss: 1.3231642246246338 | G Loss: 1.0874669551849365\n",
            "Epoch 13 | D Loss: 1.2579247951507568 | G Loss: 1.0829250812530518\n",
            "Epoch 13 | D Loss: 1.3003864288330078 | G Loss: 1.0672919750213623\n",
            "Epoch 13 | D Loss: 1.3924146890640259 | G Loss: 1.038301944732666\n",
            "Epoch 13 | D Loss: 1.3650710582733154 | G Loss: 1.0178616046905518\n",
            "Epoch 13 | D Loss: 1.3911550045013428 | G Loss: 1.0240013599395752\n",
            "Epoch 13 | D Loss: 1.3948121070861816 | G Loss: 1.0347764492034912\n",
            "Epoch 13 | D Loss: 1.435312271118164 | G Loss: 1.0506036281585693\n",
            "Epoch 13 | D Loss: 1.4081462621688843 | G Loss: 1.0534194707870483\n",
            "Epoch 13 | D Loss: 1.3879166841506958 | G Loss: 1.050034523010254\n",
            "Epoch 13 | D Loss: 1.3526558876037598 | G Loss: 1.0718799829483032\n",
            "Epoch 13 | D Loss: 1.3403329849243164 | G Loss: 1.1019688844680786\n",
            "Epoch 13 | D Loss: 1.2937287092208862 | G Loss: 1.1367876529693604\n",
            "Epoch 13 | D Loss: 1.232002854347229 | G Loss: 1.1749773025512695\n",
            "Epoch 13 | D Loss: 1.1325578689575195 | G Loss: 1.2196223735809326\n",
            "Epoch 14 | D Loss: 1.074029564857483 | G Loss: 1.2414426803588867\n",
            "Epoch 14 | D Loss: 1.0751856565475464 | G Loss: 1.2700815200805664\n",
            "Epoch 14 | D Loss: 1.0347304344177246 | G Loss: 1.258455514907837\n",
            "Epoch 14 | D Loss: 1.029266595840454 | G Loss: 1.2480238676071167\n",
            "Epoch 14 | D Loss: 1.0786421298980713 | G Loss: 1.2138649225234985\n",
            "Epoch 14 | D Loss: 1.0956560373306274 | G Loss: 1.1868610382080078\n",
            "Epoch 14 | D Loss: 1.1291791200637817 | G Loss: 1.1830313205718994\n",
            "Epoch 14 | D Loss: 1.1530680656433105 | G Loss: 1.1607494354248047\n",
            "Epoch 14 | D Loss: 1.1412007808685303 | G Loss: 1.1749694347381592\n",
            "Epoch 14 | D Loss: 1.1744046211242676 | G Loss: 1.1787986755371094\n",
            "Epoch 14 | D Loss: 1.120356559753418 | G Loss: 1.1851072311401367\n",
            "Epoch 14 | D Loss: 1.192509651184082 | G Loss: 1.159129023551941\n",
            "Epoch 14 | D Loss: 1.216896653175354 | G Loss: 1.1301255226135254\n",
            "Epoch 14 | D Loss: 1.287793517112732 | G Loss: 1.0857014656066895\n",
            "Epoch 14 | D Loss: 1.3295886516571045 | G Loss: 1.0521785020828247\n",
            "Epoch 15 | D Loss: 1.434586524963379 | G Loss: 0.998075544834137\n",
            "Epoch 15 | D Loss: 1.4186785221099854 | G Loss: 0.9419538974761963\n",
            "Epoch 15 | D Loss: 1.5747177600860596 | G Loss: 0.8923556804656982\n",
            "Epoch 15 | D Loss: 1.620259165763855 | G Loss: 0.8790006637573242\n",
            "Epoch 15 | D Loss: 1.698469877243042 | G Loss: 0.8740475177764893\n",
            "Epoch 15 | D Loss: 1.6829044818878174 | G Loss: 0.888918399810791\n",
            "Epoch 15 | D Loss: 1.6137559413909912 | G Loss: 0.918080747127533\n",
            "Epoch 15 | D Loss: 1.65634286403656 | G Loss: 0.9310928583145142\n",
            "Epoch 15 | D Loss: 1.5435234308242798 | G Loss: 0.9325239658355713\n",
            "Epoch 15 | D Loss: 1.4973926544189453 | G Loss: 0.9518662691116333\n",
            "Epoch 15 | D Loss: 1.508720874786377 | G Loss: 0.9758575558662415\n",
            "Epoch 15 | D Loss: 1.443108320236206 | G Loss: 0.9921967387199402\n",
            "Epoch 15 | D Loss: 1.3093194961547852 | G Loss: 1.0172615051269531\n",
            "Epoch 15 | D Loss: 1.2890315055847168 | G Loss: 1.0361660718917847\n",
            "Epoch 15 | D Loss: 1.2638754844665527 | G Loss: 1.034365177154541\n",
            "Epoch 16 | D Loss: 1.185797929763794 | G Loss: 1.0628842115402222\n",
            "Epoch 16 | D Loss: 1.189560890197754 | G Loss: 1.0536456108093262\n",
            "Epoch 16 | D Loss: 1.1653307676315308 | G Loss: 1.0559606552124023\n",
            "Epoch 16 | D Loss: 1.0767250061035156 | G Loss: 1.062329649925232\n",
            "Epoch 16 | D Loss: 1.1360747814178467 | G Loss: 1.0440340042114258\n",
            "Epoch 16 | D Loss: 1.1268996000289917 | G Loss: 1.0187625885009766\n",
            "Epoch 16 | D Loss: 1.1687016487121582 | G Loss: 1.0033342838287354\n",
            "Epoch 16 | D Loss: 1.1864938735961914 | G Loss: 0.9784718155860901\n",
            "Epoch 16 | D Loss: 1.160281777381897 | G Loss: 0.953027069568634\n",
            "Epoch 16 | D Loss: 1.2193248271942139 | G Loss: 0.9314477443695068\n",
            "Epoch 16 | D Loss: 1.226790428161621 | G Loss: 0.9278650283813477\n",
            "Epoch 16 | D Loss: 1.2308852672576904 | G Loss: 0.9258731603622437\n",
            "Epoch 16 | D Loss: 1.2386054992675781 | G Loss: 0.9053263068199158\n",
            "Epoch 16 | D Loss: 1.2515945434570312 | G Loss: 0.9081836938858032\n",
            "Epoch 16 | D Loss: 1.2717015743255615 | G Loss: 0.8918576240539551\n",
            "Epoch 17 | D Loss: 1.294447422027588 | G Loss: 0.8687335848808289\n",
            "Epoch 17 | D Loss: 1.3552844524383545 | G Loss: 0.8512130379676819\n",
            "Epoch 17 | D Loss: 1.3352993726730347 | G Loss: 0.8427984118461609\n",
            "Epoch 17 | D Loss: 1.3753834962844849 | G Loss: 0.8285037279129028\n",
            "Epoch 17 | D Loss: 1.3091750144958496 | G Loss: 0.8440757989883423\n",
            "Epoch 17 | D Loss: 1.2938318252563477 | G Loss: 0.850609540939331\n",
            "Epoch 17 | D Loss: 1.2689354419708252 | G Loss: 0.8679875135421753\n",
            "Epoch 17 | D Loss: 1.3039089441299438 | G Loss: 0.8798085451126099\n",
            "Epoch 17 | D Loss: 1.3006083965301514 | G Loss: 0.8980686068534851\n",
            "Epoch 17 | D Loss: 1.234745979309082 | G Loss: 0.9276664853096008\n",
            "Epoch 17 | D Loss: 1.1934465169906616 | G Loss: 0.9593325853347778\n",
            "Epoch 17 | D Loss: 1.1377220153808594 | G Loss: 0.9847424030303955\n",
            "Epoch 17 | D Loss: 1.123530387878418 | G Loss: 1.0144726037979126\n",
            "Epoch 17 | D Loss: 1.0183830261230469 | G Loss: 1.053565263748169\n",
            "Epoch 17 | D Loss: 1.0292330980300903 | G Loss: 1.0803207159042358\n",
            "Epoch 18 | D Loss: 1.0203279256820679 | G Loss: 1.0991270542144775\n",
            "Epoch 18 | D Loss: 0.9584036469459534 | G Loss: 1.1211318969726562\n",
            "Epoch 18 | D Loss: 0.9726384878158569 | G Loss: 1.1401727199554443\n",
            "Epoch 18 | D Loss: 0.9863026142120361 | G Loss: 1.1402342319488525\n",
            "Epoch 18 | D Loss: 0.9396520853042603 | G Loss: 1.1523066759109497\n",
            "Epoch 18 | D Loss: 0.9392316341400146 | G Loss: 1.1653090715408325\n",
            "Epoch 18 | D Loss: 0.9952545166015625 | G Loss: 1.1622192859649658\n",
            "Epoch 18 | D Loss: 0.9711324572563171 | G Loss: 1.159308910369873\n",
            "Epoch 18 | D Loss: 0.9950002431869507 | G Loss: 1.1533727645874023\n",
            "Epoch 18 | D Loss: 1.090474009513855 | G Loss: 1.1213277578353882\n",
            "Epoch 18 | D Loss: 1.0550494194030762 | G Loss: 1.0918537378311157\n",
            "Epoch 18 | D Loss: 1.0733083486557007 | G Loss: 1.0854179859161377\n",
            "Epoch 18 | D Loss: 1.1561760902404785 | G Loss: 1.0568172931671143\n",
            "Epoch 18 | D Loss: 1.2173765897750854 | G Loss: 1.0220303535461426\n",
            "Epoch 18 | D Loss: 1.2664942741394043 | G Loss: 0.9818586111068726\n",
            "Epoch 19 | D Loss: 1.329946517944336 | G Loss: 0.9620729088783264\n",
            "Epoch 19 | D Loss: 1.2980968952178955 | G Loss: 0.9430608749389648\n",
            "Epoch 19 | D Loss: 1.287895679473877 | G Loss: 0.9478076696395874\n",
            "Epoch 19 | D Loss: 1.294294834136963 | G Loss: 0.9469496011734009\n",
            "Epoch 19 | D Loss: 1.3034570217132568 | G Loss: 0.9514715671539307\n",
            "Epoch 19 | D Loss: 1.266910195350647 | G Loss: 0.9515367746353149\n",
            "Epoch 19 | D Loss: 1.2802695035934448 | G Loss: 0.9501308798789978\n",
            "Epoch 19 | D Loss: 1.3442132472991943 | G Loss: 0.9386717081069946\n",
            "Epoch 19 | D Loss: 1.272614598274231 | G Loss: 0.9226574897766113\n",
            "Epoch 19 | D Loss: 1.3063180446624756 | G Loss: 0.9180858135223389\n",
            "Epoch 19 | D Loss: 1.3562195301055908 | G Loss: 0.8905729055404663\n",
            "Epoch 19 | D Loss: 1.3496712446212769 | G Loss: 0.892263650894165\n",
            "Epoch 19 | D Loss: 1.330968976020813 | G Loss: 0.8908805847167969\n",
            "Epoch 19 | D Loss: 1.3471060991287231 | G Loss: 0.9020065069198608\n",
            "Epoch 19 | D Loss: 1.3130624294281006 | G Loss: 0.8865607976913452\n",
            "Epoch 20 | D Loss: 1.3049920797348022 | G Loss: 0.899638295173645\n",
            "Epoch 20 | D Loss: 1.2708420753479004 | G Loss: 0.8970763683319092\n",
            "Epoch 20 | D Loss: 1.2500300407409668 | G Loss: 0.906852662563324\n",
            "Epoch 20 | D Loss: 1.2028987407684326 | G Loss: 0.9186517000198364\n",
            "Epoch 20 | D Loss: 1.1686656475067139 | G Loss: 0.9372933506965637\n",
            "Epoch 20 | D Loss: 1.1184600591659546 | G Loss: 0.9571571350097656\n",
            "Epoch 20 | D Loss: 1.1289763450622559 | G Loss: 0.9884541630744934\n",
            "Epoch 20 | D Loss: 1.0701265335083008 | G Loss: 0.9940022230148315\n",
            "Epoch 20 | D Loss: 1.0533056259155273 | G Loss: 1.0027551651000977\n",
            "Epoch 20 | D Loss: 1.0697602033615112 | G Loss: 1.0036370754241943\n",
            "Epoch 20 | D Loss: 1.0521552562713623 | G Loss: 1.000784158706665\n",
            "Epoch 20 | D Loss: 1.0299365520477295 | G Loss: 0.993662416934967\n",
            "Epoch 20 | D Loss: 1.0211684703826904 | G Loss: 1.0192837715148926\n",
            "Epoch 20 | D Loss: 1.0246167182922363 | G Loss: 1.0199744701385498\n",
            "Epoch 20 | D Loss: 1.0058190822601318 | G Loss: 1.0275169610977173\n",
            "Epoch 21 | D Loss: 0.9316079020500183 | G Loss: 1.0473318099975586\n",
            "Epoch 21 | D Loss: 0.9058198928833008 | G Loss: 1.0438188314437866\n",
            "Epoch 21 | D Loss: 0.8993650674819946 | G Loss: 1.0465973615646362\n",
            "Epoch 21 | D Loss: 0.9286667704582214 | G Loss: 1.0506683588027954\n",
            "Epoch 21 | D Loss: 0.9206998348236084 | G Loss: 1.0510467290878296\n",
            "Epoch 21 | D Loss: 1.0047097206115723 | G Loss: 1.0042141675949097\n",
            "Epoch 21 | D Loss: 1.004835844039917 | G Loss: 0.9739351868629456\n",
            "Epoch 21 | D Loss: 1.020599603652954 | G Loss: 0.9507582187652588\n",
            "Epoch 21 | D Loss: 1.0811231136322021 | G Loss: 0.9178421497344971\n",
            "Epoch 21 | D Loss: 1.1534876823425293 | G Loss: 0.8971230983734131\n",
            "Epoch 21 | D Loss: 1.2296127080917358 | G Loss: 0.8804858326911926\n",
            "Epoch 21 | D Loss: 1.332910180091858 | G Loss: 0.8309558033943176\n",
            "Epoch 21 | D Loss: 1.3614081144332886 | G Loss: 0.8320872783660889\n",
            "Epoch 21 | D Loss: 1.4474732875823975 | G Loss: 0.8042757511138916\n",
            "Epoch 21 | D Loss: 1.4787713289260864 | G Loss: 0.8079713582992554\n",
            "Epoch 22 | D Loss: 1.6176100969314575 | G Loss: 0.7802320718765259\n",
            "Epoch 22 | D Loss: 1.6001060009002686 | G Loss: 0.7905185222625732\n",
            "Epoch 22 | D Loss: 1.555478572845459 | G Loss: 0.8248773813247681\n",
            "Epoch 22 | D Loss: 1.5837137699127197 | G Loss: 0.8582891821861267\n",
            "Epoch 22 | D Loss: 1.5448403358459473 | G Loss: 0.8524025082588196\n",
            "Epoch 22 | D Loss: 1.6119794845581055 | G Loss: 0.8193873167037964\n",
            "Epoch 22 | D Loss: 1.6453511714935303 | G Loss: 0.7905640006065369\n",
            "Epoch 22 | D Loss: 1.666680932044983 | G Loss: 0.7682113647460938\n",
            "Epoch 22 | D Loss: 1.5980615615844727 | G Loss: 0.7604564428329468\n",
            "Epoch 22 | D Loss: 1.747528076171875 | G Loss: 0.7347575426101685\n",
            "Epoch 22 | D Loss: 1.830392837524414 | G Loss: 0.7137044072151184\n",
            "Epoch 22 | D Loss: 1.7856340408325195 | G Loss: 0.717771053314209\n",
            "Epoch 22 | D Loss: 1.7984154224395752 | G Loss: 0.7258065342903137\n",
            "Epoch 22 | D Loss: 1.8521443605422974 | G Loss: 0.7404443621635437\n",
            "Epoch 22 | D Loss: 1.8202263116836548 | G Loss: 0.7469660043716431\n",
            "Epoch 23 | D Loss: 1.7501494884490967 | G Loss: 0.7766310572624207\n",
            "Epoch 23 | D Loss: 1.7769712209701538 | G Loss: 0.7937575578689575\n",
            "Epoch 23 | D Loss: 1.7165758609771729 | G Loss: 0.8053191900253296\n",
            "Epoch 23 | D Loss: 1.6175661087036133 | G Loss: 0.8318636417388916\n",
            "Epoch 23 | D Loss: 1.5854732990264893 | G Loss: 0.8616983890533447\n",
            "Epoch 23 | D Loss: 1.5204319953918457 | G Loss: 0.8901035785675049\n",
            "Epoch 23 | D Loss: 1.4170883893966675 | G Loss: 0.9259535074234009\n",
            "Epoch 23 | D Loss: 1.4095547199249268 | G Loss: 0.9512696266174316\n",
            "Epoch 23 | D Loss: 1.2979748249053955 | G Loss: 0.98660808801651\n",
            "Epoch 23 | D Loss: 1.2893444299697876 | G Loss: 0.9990667104721069\n",
            "Epoch 23 | D Loss: 1.2536659240722656 | G Loss: 1.0178431272506714\n",
            "Epoch 23 | D Loss: 1.195185899734497 | G Loss: 1.050363540649414\n",
            "Epoch 23 | D Loss: 1.1758733987808228 | G Loss: 1.0702195167541504\n",
            "Epoch 23 | D Loss: 1.14426851272583 | G Loss: 1.0836186408996582\n",
            "Epoch 23 | D Loss: 1.1199681758880615 | G Loss: 1.099313735961914\n",
            "Epoch 24 | D Loss: 1.102933406829834 | G Loss: 1.1105210781097412\n",
            "Epoch 24 | D Loss: 1.1224018335342407 | G Loss: 1.1183706521987915\n",
            "Epoch 24 | D Loss: 1.0822553634643555 | G Loss: 1.1280574798583984\n",
            "Epoch 24 | D Loss: 1.09737229347229 | G Loss: 1.1074137687683105\n",
            "Epoch 24 | D Loss: 1.0878448486328125 | G Loss: 1.1083855628967285\n",
            "Epoch 24 | D Loss: 1.1090426445007324 | G Loss: 1.0956177711486816\n",
            "Epoch 24 | D Loss: 1.1135780811309814 | G Loss: 1.1046593189239502\n",
            "Epoch 24 | D Loss: 1.1743032932281494 | G Loss: 1.1020426750183105\n",
            "Epoch 24 | D Loss: 1.193345069885254 | G Loss: 1.079604983329773\n",
            "Epoch 24 | D Loss: 1.199098825454712 | G Loss: 1.0758436918258667\n",
            "Epoch 24 | D Loss: 1.2118037939071655 | G Loss: 1.0681889057159424\n",
            "Epoch 24 | D Loss: 1.2530815601348877 | G Loss: 1.0674760341644287\n",
            "Epoch 24 | D Loss: 1.2912242412567139 | G Loss: 1.0513720512390137\n",
            "Epoch 24 | D Loss: 1.276949405670166 | G Loss: 1.057654619216919\n",
            "Epoch 24 | D Loss: 1.2871001958847046 | G Loss: 1.0679664611816406\n",
            "Epoch 25 | D Loss: 1.2784031629562378 | G Loss: 1.0745820999145508\n",
            "Epoch 25 | D Loss: 1.2749879360198975 | G Loss: 1.1019495725631714\n",
            "Epoch 25 | D Loss: 1.1879687309265137 | G Loss: 1.1573166847229004\n",
            "Epoch 25 | D Loss: 1.0914362668991089 | G Loss: 1.202599287033081\n",
            "Epoch 25 | D Loss: 1.0931758880615234 | G Loss: 1.2344987392425537\n",
            "Epoch 25 | D Loss: 1.1335091590881348 | G Loss: 1.20805025100708\n",
            "Epoch 25 | D Loss: 1.1033471822738647 | G Loss: 1.1851842403411865\n",
            "Epoch 25 | D Loss: 1.1786234378814697 | G Loss: 1.1259502172470093\n",
            "Epoch 25 | D Loss: 1.2321081161499023 | G Loss: 1.0777826309204102\n",
            "Epoch 25 | D Loss: 1.246877908706665 | G Loss: 1.0699894428253174\n",
            "Epoch 25 | D Loss: 1.2897933721542358 | G Loss: 1.0694983005523682\n",
            "Epoch 25 | D Loss: 1.335214376449585 | G Loss: 1.0611369609832764\n",
            "Epoch 25 | D Loss: 1.3517982959747314 | G Loss: 1.0584607124328613\n",
            "Epoch 25 | D Loss: 1.2615187168121338 | G Loss: 1.0668110847473145\n",
            "Epoch 25 | D Loss: 1.3095154762268066 | G Loss: 1.081268548965454\n",
            "Epoch 26 | D Loss: 1.3177862167358398 | G Loss: 1.0619828701019287\n",
            "Epoch 26 | D Loss: 1.317749261856079 | G Loss: 1.0436232089996338\n",
            "Epoch 26 | D Loss: 1.2802042961120605 | G Loss: 1.0499662160873413\n",
            "Epoch 26 | D Loss: 1.352940320968628 | G Loss: 1.0255417823791504\n",
            "Epoch 26 | D Loss: 1.364415168762207 | G Loss: 1.0212959051132202\n",
            "Epoch 26 | D Loss: 1.2792630195617676 | G Loss: 1.0418851375579834\n",
            "Epoch 26 | D Loss: 1.3913307189941406 | G Loss: 1.0538127422332764\n",
            "Epoch 26 | D Loss: 1.3093351125717163 | G Loss: 1.0588423013687134\n",
            "Epoch 26 | D Loss: 1.2909542322158813 | G Loss: 1.0722064971923828\n",
            "Epoch 26 | D Loss: 1.3504856824874878 | G Loss: 1.0679761171340942\n",
            "Epoch 26 | D Loss: 1.372701644897461 | G Loss: 1.0654940605163574\n",
            "Epoch 26 | D Loss: 1.3689117431640625 | G Loss: 1.0619882345199585\n",
            "Epoch 26 | D Loss: 1.3631850481033325 | G Loss: 1.0646916627883911\n",
            "Epoch 26 | D Loss: 1.3929433822631836 | G Loss: 1.0648338794708252\n",
            "Epoch 26 | D Loss: 1.3346014022827148 | G Loss: 1.0617215633392334\n",
            "Epoch 27 | D Loss: 1.3213725090026855 | G Loss: 1.0592801570892334\n",
            "Epoch 27 | D Loss: 1.2727112770080566 | G Loss: 1.0847954750061035\n",
            "Epoch 27 | D Loss: 1.2739444971084595 | G Loss: 1.080388069152832\n",
            "Epoch 27 | D Loss: 1.2597687244415283 | G Loss: 1.064845323562622\n",
            "Epoch 27 | D Loss: 1.201615810394287 | G Loss: 1.07210373878479\n",
            "Epoch 27 | D Loss: 1.243912696838379 | G Loss: 1.0611028671264648\n",
            "Epoch 27 | D Loss: 1.213320016860962 | G Loss: 1.0372295379638672\n",
            "Epoch 27 | D Loss: 1.2901298999786377 | G Loss: 1.023838996887207\n",
            "Epoch 27 | D Loss: 1.2345929145812988 | G Loss: 1.0282015800476074\n",
            "Epoch 27 | D Loss: 1.2479069232940674 | G Loss: 1.0270092487335205\n",
            "Epoch 27 | D Loss: 1.2307097911834717 | G Loss: 1.0266010761260986\n",
            "Epoch 27 | D Loss: 1.2256361246109009 | G Loss: 1.0268094539642334\n",
            "Epoch 27 | D Loss: 1.1956466436386108 | G Loss: 1.0369791984558105\n",
            "Epoch 27 | D Loss: 1.1242387294769287 | G Loss: 1.0507657527923584\n",
            "Epoch 27 | D Loss: 1.0704190731048584 | G Loss: 1.0739009380340576\n",
            "Epoch 28 | D Loss: 1.046721339225769 | G Loss: 1.0940521955490112\n",
            "Epoch 28 | D Loss: 1.0730220079421997 | G Loss: 1.09701406955719\n",
            "Epoch 28 | D Loss: 1.0485496520996094 | G Loss: 1.0990755558013916\n",
            "Epoch 28 | D Loss: 1.016845464706421 | G Loss: 1.0932822227478027\n",
            "Epoch 28 | D Loss: 1.0779234170913696 | G Loss: 1.1008412837982178\n",
            "Epoch 28 | D Loss: 1.0914196968078613 | G Loss: 1.1003780364990234\n",
            "Epoch 28 | D Loss: 1.0471184253692627 | G Loss: 1.1061732769012451\n",
            "Epoch 28 | D Loss: 0.959442675113678 | G Loss: 1.1458683013916016\n",
            "Epoch 28 | D Loss: 1.0528991222381592 | G Loss: 1.1371232271194458\n",
            "Epoch 28 | D Loss: 1.025114893913269 | G Loss: 1.127148985862732\n",
            "Epoch 28 | D Loss: 1.025397539138794 | G Loss: 1.1457102298736572\n",
            "Epoch 28 | D Loss: 0.9933980703353882 | G Loss: 1.1585063934326172\n",
            "Epoch 28 | D Loss: 1.0093399286270142 | G Loss: 1.1498243808746338\n",
            "Epoch 28 | D Loss: 1.0286734104156494 | G Loss: 1.1180646419525146\n",
            "Epoch 28 | D Loss: 1.1360785961151123 | G Loss: 1.0921077728271484\n",
            "Epoch 29 | D Loss: 1.1330900192260742 | G Loss: 1.0691800117492676\n",
            "Epoch 29 | D Loss: 1.1333199739456177 | G Loss: 1.0529210567474365\n",
            "Epoch 29 | D Loss: 1.2191941738128662 | G Loss: 1.0558956861495972\n",
            "Epoch 29 | D Loss: 1.1710216999053955 | G Loss: 1.0761308670043945\n",
            "Epoch 29 | D Loss: 1.1740385293960571 | G Loss: 1.0818383693695068\n",
            "Epoch 29 | D Loss: 1.1280946731567383 | G Loss: 1.084483027458191\n",
            "Epoch 29 | D Loss: 1.1747366189956665 | G Loss: 1.086120843887329\n",
            "Epoch 29 | D Loss: 1.1697916984558105 | G Loss: 1.0788114070892334\n",
            "Epoch 29 | D Loss: 1.2464442253112793 | G Loss: 1.0285234451293945\n",
            "Epoch 29 | D Loss: 1.340852975845337 | G Loss: 1.000216007232666\n",
            "Epoch 29 | D Loss: 1.3424322605133057 | G Loss: 0.9744042158126831\n",
            "Epoch 29 | D Loss: 1.4627689123153687 | G Loss: 0.914829671382904\n",
            "Epoch 29 | D Loss: 1.512933373451233 | G Loss: 0.8928041458129883\n",
            "Epoch 29 | D Loss: 1.5943999290466309 | G Loss: 0.854724645614624\n",
            "Epoch 29 | D Loss: 1.7427935600280762 | G Loss: 0.8240552544593811\n",
            "Epoch 30 | D Loss: 1.6665401458740234 | G Loss: 0.8141825795173645\n",
            "Epoch 30 | D Loss: 1.6856639385223389 | G Loss: 0.8456441164016724\n",
            "Epoch 30 | D Loss: 1.6007438898086548 | G Loss: 0.8849486708641052\n",
            "Epoch 30 | D Loss: 1.5791773796081543 | G Loss: 0.9092839956283569\n",
            "Epoch 30 | D Loss: 1.4683645963668823 | G Loss: 0.9687539339065552\n",
            "Epoch 30 | D Loss: 1.2598663568496704 | G Loss: 1.0494909286499023\n",
            "Epoch 30 | D Loss: 1.191715955734253 | G Loss: 1.1165244579315186\n",
            "Epoch 30 | D Loss: 1.0427449941635132 | G Loss: 1.2096540927886963\n",
            "Epoch 30 | D Loss: 1.024871826171875 | G Loss: 1.2819513082504272\n",
            "Epoch 30 | D Loss: 0.8418551683425903 | G Loss: 1.3543154001235962\n",
            "Epoch 30 | D Loss: 0.7548248767852783 | G Loss: 1.4024094343185425\n",
            "Epoch 30 | D Loss: 0.7191168665885925 | G Loss: 1.456690788269043\n",
            "Epoch 30 | D Loss: 0.6995028257369995 | G Loss: 1.4731700420379639\n",
            "Epoch 30 | D Loss: 0.7337104082107544 | G Loss: 1.4669897556304932\n",
            "Epoch 30 | D Loss: 0.7833312749862671 | G Loss: 1.4343122243881226\n",
            "Epoch 31 | D Loss: 0.8175913095474243 | G Loss: 1.3973705768585205\n",
            "Epoch 31 | D Loss: 0.8866380453109741 | G Loss: 1.3725674152374268\n",
            "Epoch 31 | D Loss: 0.8724270462989807 | G Loss: 1.3380565643310547\n",
            "Epoch 31 | D Loss: 0.9779659509658813 | G Loss: 1.3090728521347046\n",
            "Epoch 31 | D Loss: 1.0307637453079224 | G Loss: 1.265973448753357\n",
            "Epoch 31 | D Loss: 1.1593599319458008 | G Loss: 1.2094359397888184\n",
            "Epoch 31 | D Loss: 1.1955821514129639 | G Loss: 1.188405990600586\n",
            "Epoch 31 | D Loss: 1.2101805210113525 | G Loss: 1.2044305801391602\n",
            "Epoch 31 | D Loss: 1.3405070304870605 | G Loss: 1.1669273376464844\n",
            "Epoch 31 | D Loss: 1.3066009283065796 | G Loss: 1.1335190534591675\n",
            "Epoch 31 | D Loss: 1.4885051250457764 | G Loss: 1.084843635559082\n",
            "Epoch 31 | D Loss: 1.5472452640533447 | G Loss: 1.0088987350463867\n",
            "Epoch 31 | D Loss: 1.556100606918335 | G Loss: 1.0103543996810913\n",
            "Epoch 31 | D Loss: 1.6435788869857788 | G Loss: 1.0308536291122437\n",
            "Epoch 31 | D Loss: 1.7015491724014282 | G Loss: 1.019221544265747\n",
            "Epoch 32 | D Loss: 1.6842758655548096 | G Loss: 1.0069806575775146\n",
            "Epoch 32 | D Loss: 1.7734605073928833 | G Loss: 1.0005476474761963\n",
            "Epoch 32 | D Loss: 1.9071727991104126 | G Loss: 0.948073148727417\n",
            "Epoch 32 | D Loss: 1.8368113040924072 | G Loss: 0.9330143928527832\n",
            "Epoch 32 | D Loss: 1.7966188192367554 | G Loss: 0.9692182540893555\n",
            "Epoch 32 | D Loss: 1.6804232597351074 | G Loss: 1.0044941902160645\n",
            "Epoch 32 | D Loss: 1.8090277910232544 | G Loss: 1.0135200023651123\n",
            "Epoch 32 | D Loss: 1.59237802028656 | G Loss: 1.0492632389068604\n",
            "Epoch 32 | D Loss: 1.5729224681854248 | G Loss: 1.0629656314849854\n",
            "Epoch 32 | D Loss: 1.520231008529663 | G Loss: 1.0681527853012085\n",
            "Epoch 32 | D Loss: 1.4417500495910645 | G Loss: 1.1016550064086914\n",
            "Epoch 32 | D Loss: 1.4390182495117188 | G Loss: 1.1039626598358154\n",
            "Epoch 32 | D Loss: 1.4151976108551025 | G Loss: 1.125256896018982\n",
            "Epoch 32 | D Loss: 1.4476275444030762 | G Loss: 1.114874005317688\n",
            "Epoch 32 | D Loss: 1.439157247543335 | G Loss: 1.1080691814422607\n",
            "Epoch 33 | D Loss: 1.4116485118865967 | G Loss: 1.1279340982437134\n",
            "Epoch 33 | D Loss: 1.4401524066925049 | G Loss: 1.1342517137527466\n",
            "Epoch 33 | D Loss: 1.4590661525726318 | G Loss: 1.1263561248779297\n",
            "Epoch 33 | D Loss: 1.4507725238800049 | G Loss: 1.1105926036834717\n",
            "Epoch 33 | D Loss: 1.3612496852874756 | G Loss: 1.1165649890899658\n",
            "Epoch 33 | D Loss: 1.4216969013214111 | G Loss: 1.1071606874465942\n",
            "Epoch 33 | D Loss: 1.3233888149261475 | G Loss: 1.112199306488037\n",
            "Epoch 33 | D Loss: 1.3813936710357666 | G Loss: 1.1216366291046143\n",
            "Epoch 33 | D Loss: 1.3739025592803955 | G Loss: 1.1039841175079346\n",
            "Epoch 33 | D Loss: 1.298436164855957 | G Loss: 1.102602243423462\n",
            "Epoch 33 | D Loss: 1.3106886148452759 | G Loss: 1.0928539037704468\n",
            "Epoch 33 | D Loss: 1.3336117267608643 | G Loss: 1.0996246337890625\n",
            "Epoch 33 | D Loss: 1.3275017738342285 | G Loss: 1.1023807525634766\n",
            "Epoch 33 | D Loss: 1.3337132930755615 | G Loss: 1.116979718208313\n",
            "Epoch 33 | D Loss: 1.329017162322998 | G Loss: 1.1395715475082397\n",
            "Epoch 34 | D Loss: 1.318831205368042 | G Loss: 1.1567585468292236\n",
            "Epoch 34 | D Loss: 1.3107346296310425 | G Loss: 1.182781457901001\n",
            "Epoch 34 | D Loss: 1.293379306793213 | G Loss: 1.2173717021942139\n",
            "Epoch 34 | D Loss: 1.2275118827819824 | G Loss: 1.2215032577514648\n",
            "Epoch 34 | D Loss: 1.2421543598175049 | G Loss: 1.22019624710083\n",
            "Epoch 34 | D Loss: 1.2097420692443848 | G Loss: 1.192185640335083\n",
            "Epoch 34 | D Loss: 1.2877731323242188 | G Loss: 1.1752514839172363\n",
            "Epoch 34 | D Loss: 1.3688360452651978 | G Loss: 1.1342236995697021\n",
            "Epoch 34 | D Loss: 1.3909446001052856 | G Loss: 1.124584436416626\n",
            "Epoch 34 | D Loss: 1.396219253540039 | G Loss: 1.1447129249572754\n",
            "Epoch 34 | D Loss: 1.4041287899017334 | G Loss: 1.1752502918243408\n",
            "Epoch 34 | D Loss: 1.2734882831573486 | G Loss: 1.2363736629486084\n",
            "Epoch 34 | D Loss: 1.2795588970184326 | G Loss: 1.2728307247161865\n",
            "Epoch 34 | D Loss: 1.2268928289413452 | G Loss: 1.313598871231079\n",
            "Epoch 34 | D Loss: 1.138062596321106 | G Loss: 1.3362152576446533\n",
            "Epoch 35 | D Loss: 1.0379533767700195 | G Loss: 1.355705738067627\n",
            "Epoch 35 | D Loss: 0.9869505763053894 | G Loss: 1.3758313655853271\n",
            "Epoch 35 | D Loss: 0.982896089553833 | G Loss: 1.3841279745101929\n",
            "Epoch 35 | D Loss: 0.9668388366699219 | G Loss: 1.3668882846832275\n",
            "Epoch 35 | D Loss: 0.9662065505981445 | G Loss: 1.3863353729248047\n",
            "Epoch 35 | D Loss: 0.8915797472000122 | G Loss: 1.4106559753417969\n",
            "Epoch 35 | D Loss: 0.9053822159767151 | G Loss: 1.424675464630127\n",
            "Epoch 35 | D Loss: 0.8269940614700317 | G Loss: 1.45103919506073\n",
            "Epoch 35 | D Loss: 0.8389502763748169 | G Loss: 1.4297349452972412\n",
            "Epoch 35 | D Loss: 0.8352632522583008 | G Loss: 1.3966741561889648\n",
            "Epoch 35 | D Loss: 0.8070391416549683 | G Loss: 1.3993152379989624\n",
            "Epoch 35 | D Loss: 0.8685526847839355 | G Loss: 1.336408019065857\n",
            "Epoch 35 | D Loss: 0.868462324142456 | G Loss: 1.326999545097351\n",
            "Epoch 35 | D Loss: 0.941619873046875 | G Loss: 1.2519378662109375\n",
            "Epoch 35 | D Loss: 1.0340447425842285 | G Loss: 1.1467583179473877\n",
            "Epoch 36 | D Loss: 1.126272439956665 | G Loss: 1.0876271724700928\n",
            "Epoch 36 | D Loss: 1.1919467449188232 | G Loss: 1.0344058275222778\n",
            "Epoch 36 | D Loss: 1.2599254846572876 | G Loss: 1.0011107921600342\n",
            "Epoch 36 | D Loss: 1.187645673751831 | G Loss: 0.9753564596176147\n",
            "Epoch 36 | D Loss: 1.282536268234253 | G Loss: 0.9882678985595703\n",
            "Epoch 36 | D Loss: 1.293242335319519 | G Loss: 0.9757830500602722\n",
            "Epoch 36 | D Loss: 1.330758810043335 | G Loss: 0.9521270990371704\n",
            "Epoch 36 | D Loss: 1.4314000606536865 | G Loss: 0.9515913724899292\n",
            "Epoch 36 | D Loss: 1.3326411247253418 | G Loss: 0.9742478132247925\n",
            "Epoch 36 | D Loss: 1.433483362197876 | G Loss: 0.988105058670044\n",
            "Epoch 36 | D Loss: 1.3105688095092773 | G Loss: 1.0258116722106934\n",
            "Epoch 36 | D Loss: 1.3908611536026 | G Loss: 1.0405960083007812\n",
            "Epoch 36 | D Loss: 1.3603544235229492 | G Loss: 1.0528478622436523\n",
            "Epoch 36 | D Loss: 1.2904949188232422 | G Loss: 1.093428611755371\n",
            "Epoch 36 | D Loss: 1.1767866611480713 | G Loss: 1.1320725679397583\n",
            "Epoch 37 | D Loss: 1.188558578491211 | G Loss: 1.1616415977478027\n",
            "Epoch 37 | D Loss: 1.3047610521316528 | G Loss: 1.1497327089309692\n",
            "Epoch 37 | D Loss: 1.1830874681472778 | G Loss: 1.1583646535873413\n",
            "Epoch 37 | D Loss: 1.1522375345230103 | G Loss: 1.1557072401046753\n",
            "Epoch 37 | D Loss: 1.2570195198059082 | G Loss: 1.146526575088501\n",
            "Epoch 37 | D Loss: 1.324398159980774 | G Loss: 1.1194071769714355\n",
            "Epoch 37 | D Loss: 1.3570711612701416 | G Loss: 1.0382513999938965\n",
            "Epoch 37 | D Loss: 1.4366251230239868 | G Loss: 1.0034451484680176\n",
            "Epoch 37 | D Loss: 1.4731881618499756 | G Loss: 0.965613067150116\n",
            "Epoch 37 | D Loss: 1.5540659427642822 | G Loss: 0.9399884939193726\n",
            "Epoch 37 | D Loss: 1.598191499710083 | G Loss: 0.925349771976471\n",
            "Epoch 37 | D Loss: 1.6627178192138672 | G Loss: 0.9026724100112915\n",
            "Epoch 37 | D Loss: 1.691913366317749 | G Loss: 0.8777429461479187\n",
            "Epoch 37 | D Loss: 1.7193397283554077 | G Loss: 0.840646505355835\n",
            "Epoch 37 | D Loss: 1.8390079736709595 | G Loss: 0.8221989870071411\n",
            "Epoch 38 | D Loss: 1.7231849431991577 | G Loss: 0.8076684474945068\n",
            "Epoch 38 | D Loss: 1.7978699207305908 | G Loss: 0.8092209100723267\n",
            "Epoch 38 | D Loss: 1.7526353597640991 | G Loss: 0.7999296188354492\n",
            "Epoch 38 | D Loss: 1.712458610534668 | G Loss: 0.8100759983062744\n",
            "Epoch 38 | D Loss: 1.644632339477539 | G Loss: 0.833075225353241\n",
            "Epoch 38 | D Loss: 1.6564891338348389 | G Loss: 0.8397799134254456\n",
            "Epoch 38 | D Loss: 1.656128168106079 | G Loss: 0.8445931673049927\n",
            "Epoch 38 | D Loss: 1.6096208095550537 | G Loss: 0.8608610033988953\n",
            "Epoch 38 | D Loss: 1.4378713369369507 | G Loss: 0.8908756375312805\n",
            "Epoch 38 | D Loss: 1.5049810409545898 | G Loss: 0.9054237604141235\n",
            "Epoch 38 | D Loss: 1.4194161891937256 | G Loss: 0.9182982444763184\n",
            "Epoch 38 | D Loss: 1.33970046043396 | G Loss: 0.9444941282272339\n",
            "Epoch 38 | D Loss: 1.2860100269317627 | G Loss: 0.9769152402877808\n",
            "Epoch 38 | D Loss: 1.2696142196655273 | G Loss: 1.0000321865081787\n",
            "Epoch 38 | D Loss: 1.2429933547973633 | G Loss: 1.0225000381469727\n",
            "Epoch 39 | D Loss: 1.2313969135284424 | G Loss: 1.033797264099121\n",
            "Epoch 39 | D Loss: 1.1665968894958496 | G Loss: 1.0482022762298584\n",
            "Epoch 39 | D Loss: 1.2245638370513916 | G Loss: 1.0597484111785889\n",
            "Epoch 39 | D Loss: 1.2048224210739136 | G Loss: 1.055654764175415\n",
            "Epoch 39 | D Loss: 1.163569450378418 | G Loss: 1.0691018104553223\n",
            "Epoch 39 | D Loss: 1.159613013267517 | G Loss: 1.0808460712432861\n",
            "Epoch 39 | D Loss: 1.1363188028335571 | G Loss: 1.0882066488265991\n",
            "Epoch 39 | D Loss: 1.1657569408416748 | G Loss: 1.0869414806365967\n",
            "Epoch 39 | D Loss: 1.2489697933197021 | G Loss: 1.0722863674163818\n",
            "Epoch 39 | D Loss: 1.333237886428833 | G Loss: 1.05531644821167\n",
            "Epoch 39 | D Loss: 1.245739221572876 | G Loss: 1.0629355907440186\n",
            "Epoch 39 | D Loss: 1.283174991607666 | G Loss: 1.0991919040679932\n",
            "Epoch 39 | D Loss: 1.2549936771392822 | G Loss: 1.1250076293945312\n",
            "Epoch 39 | D Loss: 1.2610386610031128 | G Loss: 1.1516695022583008\n",
            "Epoch 39 | D Loss: 1.2396104335784912 | G Loss: 1.1448569297790527\n",
            "Epoch 40 | D Loss: 1.257354736328125 | G Loss: 1.1258275508880615\n",
            "Epoch 40 | D Loss: 1.1885687112808228 | G Loss: 1.1181888580322266\n",
            "Epoch 40 | D Loss: 1.2719306945800781 | G Loss: 1.1247469186782837\n",
            "Epoch 40 | D Loss: 1.2220203876495361 | G Loss: 1.1225166320800781\n",
            "Epoch 40 | D Loss: 1.2057719230651855 | G Loss: 1.1251626014709473\n",
            "Epoch 40 | D Loss: 1.3293250799179077 | G Loss: 1.127000331878662\n",
            "Epoch 40 | D Loss: 1.294780969619751 | G Loss: 1.1099302768707275\n",
            "Epoch 40 | D Loss: 1.3092294931411743 | G Loss: 1.1133469343185425\n",
            "Epoch 40 | D Loss: 1.2495841979980469 | G Loss: 1.1380250453948975\n",
            "Epoch 40 | D Loss: 1.3030879497528076 | G Loss: 1.1691516637802124\n",
            "Epoch 40 | D Loss: 1.2759408950805664 | G Loss: 1.1903818845748901\n",
            "Epoch 40 | D Loss: 1.2817178964614868 | G Loss: 1.1963956356048584\n",
            "Epoch 40 | D Loss: 1.200146198272705 | G Loss: 1.2048678398132324\n",
            "Epoch 40 | D Loss: 1.1502292156219482 | G Loss: 1.2303645610809326\n",
            "Epoch 40 | D Loss: 1.2365137338638306 | G Loss: 1.2346537113189697\n",
            "Epoch 41 | D Loss: 1.1297986507415771 | G Loss: 1.2377856969833374\n",
            "Epoch 41 | D Loss: 1.0952305793762207 | G Loss: 1.2739391326904297\n",
            "Epoch 41 | D Loss: 1.0809414386749268 | G Loss: 1.2765591144561768\n",
            "Epoch 41 | D Loss: 1.088307499885559 | G Loss: 1.2713196277618408\n",
            "Epoch 41 | D Loss: 1.1098970174789429 | G Loss: 1.2454280853271484\n",
            "Epoch 41 | D Loss: 1.097707986831665 | G Loss: 1.2392802238464355\n",
            "Epoch 41 | D Loss: 1.0794541835784912 | G Loss: 1.215374231338501\n",
            "Epoch 41 | D Loss: 1.0501434803009033 | G Loss: 1.2194669246673584\n",
            "Epoch 41 | D Loss: 1.1584811210632324 | G Loss: 1.2265427112579346\n",
            "Epoch 41 | D Loss: 1.1420789957046509 | G Loss: 1.2013092041015625\n",
            "Epoch 41 | D Loss: 1.1248161792755127 | G Loss: 1.2079436779022217\n",
            "Epoch 41 | D Loss: 1.2052921056747437 | G Loss: 1.1914520263671875\n",
            "Epoch 41 | D Loss: 1.2059566974639893 | G Loss: 1.1617106199264526\n",
            "Epoch 41 | D Loss: 1.1411091089248657 | G Loss: 1.1718297004699707\n",
            "Epoch 41 | D Loss: 1.1705951690673828 | G Loss: 1.1815264225006104\n",
            "Epoch 42 | D Loss: 1.2204341888427734 | G Loss: 1.1773366928100586\n",
            "Epoch 42 | D Loss: 1.2046880722045898 | G Loss: 1.1929118633270264\n",
            "Epoch 42 | D Loss: 1.2168341875076294 | G Loss: 1.1841142177581787\n",
            "Epoch 42 | D Loss: 1.163731575012207 | G Loss: 1.210845947265625\n",
            "Epoch 42 | D Loss: 1.1414237022399902 | G Loss: 1.2557742595672607\n",
            "Epoch 42 | D Loss: 1.2323970794677734 | G Loss: 1.2632367610931396\n",
            "Epoch 42 | D Loss: 1.042700171470642 | G Loss: 1.3094220161437988\n",
            "Epoch 42 | D Loss: 1.0134880542755127 | G Loss: 1.313853144645691\n",
            "Epoch 42 | D Loss: 1.0186433792114258 | G Loss: 1.3192198276519775\n",
            "Epoch 42 | D Loss: 1.0534772872924805 | G Loss: 1.2954628467559814\n",
            "Epoch 42 | D Loss: 1.0713798999786377 | G Loss: 1.283674955368042\n",
            "Epoch 42 | D Loss: 1.0849777460098267 | G Loss: 1.2935659885406494\n",
            "Epoch 42 | D Loss: 1.0135749578475952 | G Loss: 1.314578652381897\n",
            "Epoch 42 | D Loss: 1.0386906862258911 | G Loss: 1.2965326309204102\n",
            "Epoch 42 | D Loss: 1.1203691959381104 | G Loss: 1.2753169536590576\n",
            "Epoch 43 | D Loss: 1.0818264484405518 | G Loss: 1.269402027130127\n",
            "Epoch 43 | D Loss: 1.1905500888824463 | G Loss: 1.2373827695846558\n",
            "Epoch 43 | D Loss: 1.0980876684188843 | G Loss: 1.2380027770996094\n",
            "Epoch 43 | D Loss: 1.1743450164794922 | G Loss: 1.2044720649719238\n",
            "Epoch 43 | D Loss: 1.140279769897461 | G Loss: 1.1875039339065552\n",
            "Epoch 43 | D Loss: 1.156860589981079 | G Loss: 1.150576114654541\n",
            "Epoch 43 | D Loss: 1.2518432140350342 | G Loss: 1.1185201406478882\n",
            "Epoch 43 | D Loss: 1.2077908515930176 | G Loss: 1.1018784046173096\n",
            "Epoch 43 | D Loss: 1.18910813331604 | G Loss: 1.113316535949707\n",
            "Epoch 43 | D Loss: 1.223740577697754 | G Loss: 1.1152334213256836\n",
            "Epoch 43 | D Loss: 1.3178126811981201 | G Loss: 1.0881373882293701\n",
            "Epoch 43 | D Loss: 1.226059913635254 | G Loss: 1.0801304578781128\n",
            "Epoch 43 | D Loss: 1.1725422143936157 | G Loss: 1.0930304527282715\n",
            "Epoch 43 | D Loss: 1.2484161853790283 | G Loss: 1.0779979228973389\n",
            "Epoch 43 | D Loss: 1.2465269565582275 | G Loss: 1.0853222608566284\n",
            "Epoch 44 | D Loss: 1.0845427513122559 | G Loss: 1.1175813674926758\n",
            "Epoch 44 | D Loss: 1.0881433486938477 | G Loss: 1.1414616107940674\n",
            "Epoch 44 | D Loss: 1.031569004058838 | G Loss: 1.1784586906433105\n",
            "Epoch 44 | D Loss: 1.0274055004119873 | G Loss: 1.2122817039489746\n",
            "Epoch 44 | D Loss: 0.8808881044387817 | G Loss: 1.2452330589294434\n",
            "Epoch 44 | D Loss: 0.8727717399597168 | G Loss: 1.2608392238616943\n",
            "Epoch 44 | D Loss: 0.8876023292541504 | G Loss: 1.284376621246338\n",
            "Epoch 44 | D Loss: 0.8976591229438782 | G Loss: 1.265444040298462\n",
            "Epoch 44 | D Loss: 0.8794707655906677 | G Loss: 1.2708659172058105\n",
            "Epoch 44 | D Loss: 0.8837385773658752 | G Loss: 1.289964199066162\n",
            "Epoch 44 | D Loss: 0.8407406210899353 | G Loss: 1.2368831634521484\n",
            "Epoch 44 | D Loss: 0.8131289482116699 | G Loss: 1.2598369121551514\n",
            "Epoch 44 | D Loss: 0.9083318114280701 | G Loss: 1.2662689685821533\n",
            "Epoch 44 | D Loss: 0.8964946269989014 | G Loss: 1.2635037899017334\n",
            "Epoch 44 | D Loss: 0.9122067093849182 | G Loss: 1.2621722221374512\n",
            "Epoch 45 | D Loss: 0.9457796812057495 | G Loss: 1.2331666946411133\n",
            "Epoch 45 | D Loss: 0.9734541177749634 | G Loss: 1.2096595764160156\n",
            "Epoch 45 | D Loss: 1.0296123027801514 | G Loss: 1.1884124279022217\n",
            "Epoch 45 | D Loss: 0.9935104846954346 | G Loss: 1.2075796127319336\n",
            "Epoch 45 | D Loss: 1.0452111959457397 | G Loss: 1.2232911586761475\n",
            "Epoch 45 | D Loss: 1.121808409690857 | G Loss: 1.1915847063064575\n",
            "Epoch 45 | D Loss: 1.1517261266708374 | G Loss: 1.131820797920227\n",
            "Epoch 45 | D Loss: 1.2327229976654053 | G Loss: 1.0504405498504639\n",
            "Epoch 45 | D Loss: 1.3155624866485596 | G Loss: 1.0269843339920044\n",
            "Epoch 45 | D Loss: 1.3796777725219727 | G Loss: 0.9997273683547974\n",
            "Epoch 45 | D Loss: 1.6699509620666504 | G Loss: 0.9480905532836914\n",
            "Epoch 45 | D Loss: 1.4749255180358887 | G Loss: 0.949910044670105\n",
            "Epoch 45 | D Loss: 1.618424654006958 | G Loss: 0.9422937631607056\n",
            "Epoch 45 | D Loss: 1.5052330493927002 | G Loss: 0.9929057359695435\n",
            "Epoch 45 | D Loss: 1.6286728382110596 | G Loss: 0.9875756502151489\n",
            "Epoch 46 | D Loss: 1.5920342206954956 | G Loss: 0.9861880540847778\n",
            "Epoch 46 | D Loss: 1.6793344020843506 | G Loss: 0.9532037973403931\n",
            "Epoch 46 | D Loss: 1.5822079181671143 | G Loss: 0.9766069650650024\n",
            "Epoch 46 | D Loss: 1.6212139129638672 | G Loss: 0.9839146137237549\n",
            "Epoch 46 | D Loss: 1.52553391456604 | G Loss: 1.0162063837051392\n",
            "Epoch 46 | D Loss: 1.5269819498062134 | G Loss: 1.0648537874221802\n",
            "Epoch 46 | D Loss: 1.6514314413070679 | G Loss: 1.0576552152633667\n",
            "Epoch 46 | D Loss: 1.6049213409423828 | G Loss: 1.0542011260986328\n",
            "Epoch 46 | D Loss: 1.4913532733917236 | G Loss: 1.0696992874145508\n",
            "Epoch 46 | D Loss: 1.5411763191223145 | G Loss: 1.0900542736053467\n",
            "Epoch 46 | D Loss: 1.5623610019683838 | G Loss: 1.0754550695419312\n",
            "Epoch 46 | D Loss: 1.4823352098464966 | G Loss: 1.072404146194458\n",
            "Epoch 46 | D Loss: 1.540271520614624 | G Loss: 1.075878381729126\n",
            "Epoch 46 | D Loss: 1.5586416721343994 | G Loss: 1.0871539115905762\n",
            "Epoch 46 | D Loss: 1.5752426385879517 | G Loss: 1.0593345165252686\n",
            "Epoch 47 | D Loss: 1.4754292964935303 | G Loss: 1.0602049827575684\n",
            "Epoch 47 | D Loss: 1.4548826217651367 | G Loss: 1.0764942169189453\n",
            "Epoch 47 | D Loss: 1.5406866073608398 | G Loss: 1.112513780593872\n",
            "Epoch 47 | D Loss: 1.520314335823059 | G Loss: 1.0938739776611328\n",
            "Epoch 47 | D Loss: 1.6066938638687134 | G Loss: 1.078466773033142\n",
            "Epoch 47 | D Loss: 1.540614366531372 | G Loss: 1.0757182836532593\n",
            "Epoch 47 | D Loss: 1.4875330924987793 | G Loss: 1.1202168464660645\n",
            "Epoch 47 | D Loss: 1.499821662902832 | G Loss: 1.1318224668502808\n",
            "Epoch 47 | D Loss: 1.4503692388534546 | G Loss: 1.15175461769104\n",
            "Epoch 47 | D Loss: 1.4472206830978394 | G Loss: 1.1654610633850098\n",
            "Epoch 47 | D Loss: 1.544089674949646 | G Loss: 1.1556625366210938\n",
            "Epoch 47 | D Loss: 1.517689824104309 | G Loss: 1.156073808670044\n",
            "Epoch 47 | D Loss: 1.434685468673706 | G Loss: 1.1748062372207642\n",
            "Epoch 47 | D Loss: 1.3974640369415283 | G Loss: 1.2106332778930664\n",
            "Epoch 47 | D Loss: 1.328683853149414 | G Loss: 1.2462483644485474\n",
            "Epoch 48 | D Loss: 1.3051559925079346 | G Loss: 1.2459096908569336\n",
            "Epoch 48 | D Loss: 1.2001880407333374 | G Loss: 1.2946439981460571\n",
            "Epoch 48 | D Loss: 1.1608123779296875 | G Loss: 1.3459473848342896\n",
            "Epoch 48 | D Loss: 1.1320995092391968 | G Loss: 1.3632597923278809\n",
            "Epoch 48 | D Loss: 1.1064071655273438 | G Loss: 1.36459481716156\n",
            "Epoch 48 | D Loss: 1.0498358011245728 | G Loss: 1.3737456798553467\n",
            "Epoch 48 | D Loss: 1.0805984735488892 | G Loss: 1.3886291980743408\n",
            "Epoch 48 | D Loss: 1.0555027723312378 | G Loss: 1.3801993131637573\n",
            "Epoch 48 | D Loss: 1.0785696506500244 | G Loss: 1.38999605178833\n",
            "Epoch 48 | D Loss: 1.2205698490142822 | G Loss: 1.3159809112548828\n",
            "Epoch 48 | D Loss: 1.2096457481384277 | G Loss: 1.3034052848815918\n",
            "Epoch 48 | D Loss: 1.135393500328064 | G Loss: 1.3207359313964844\n",
            "Epoch 48 | D Loss: 1.1969919204711914 | G Loss: 1.3348568677902222\n",
            "Epoch 48 | D Loss: 1.2091161012649536 | G Loss: 1.344666838645935\n",
            "Epoch 48 | D Loss: 1.2470279932022095 | G Loss: 1.3057411909103394\n",
            "Epoch 49 | D Loss: 1.2414711713790894 | G Loss: 1.2798678874969482\n",
            "Epoch 49 | D Loss: 1.322006106376648 | G Loss: 1.2443203926086426\n",
            "Epoch 49 | D Loss: 1.3351318836212158 | G Loss: 1.2213032245635986\n",
            "Epoch 49 | D Loss: 1.3434839248657227 | G Loss: 1.2105696201324463\n",
            "Epoch 49 | D Loss: 1.2859840393066406 | G Loss: 1.255418062210083\n",
            "Epoch 49 | D Loss: 1.245978832244873 | G Loss: 1.2487757205963135\n",
            "Epoch 49 | D Loss: 1.238372564315796 | G Loss: 1.2597761154174805\n",
            "Epoch 49 | D Loss: 1.1710929870605469 | G Loss: 1.2488231658935547\n",
            "Epoch 49 | D Loss: 1.2023136615753174 | G Loss: 1.2251648902893066\n",
            "Epoch 49 | D Loss: 1.1471707820892334 | G Loss: 1.2820196151733398\n",
            "Epoch 49 | D Loss: 1.147599458694458 | G Loss: 1.3252227306365967\n",
            "Epoch 49 | D Loss: 1.163529396057129 | G Loss: 1.3371775150299072\n",
            "Epoch 49 | D Loss: 1.0291802883148193 | G Loss: 1.3495210409164429\n",
            "Epoch 49 | D Loss: 1.0340157747268677 | G Loss: 1.3878495693206787\n",
            "Epoch 49 | D Loss: 0.9275142550468445 | G Loss: 1.4405343532562256\n",
            "Epoch 50 | D Loss: 0.9704023599624634 | G Loss: 1.4407092332839966\n",
            "Epoch 50 | D Loss: 0.8803673982620239 | G Loss: 1.4450361728668213\n",
            "Epoch 50 | D Loss: 0.844880223274231 | G Loss: 1.505034327507019\n",
            "Epoch 50 | D Loss: 0.8569402694702148 | G Loss: 1.5025383234024048\n",
            "Epoch 50 | D Loss: 0.8719956874847412 | G Loss: 1.4684717655181885\n",
            "Epoch 50 | D Loss: 0.9233654141426086 | G Loss: 1.4596740007400513\n",
            "Epoch 50 | D Loss: 1.0121819972991943 | G Loss: 1.4141570329666138\n",
            "Epoch 50 | D Loss: 1.0191032886505127 | G Loss: 1.3664189577102661\n",
            "Epoch 50 | D Loss: 0.9877126216888428 | G Loss: 1.3845387697219849\n",
            "Epoch 50 | D Loss: 1.1250255107879639 | G Loss: 1.357208251953125\n",
            "Epoch 50 | D Loss: 1.116762399673462 | G Loss: 1.3498283624649048\n",
            "Epoch 50 | D Loss: 1.1248241662979126 | G Loss: 1.2933061122894287\n",
            "Epoch 50 | D Loss: 1.269836187362671 | G Loss: 1.2371093034744263\n",
            "Epoch 50 | D Loss: 1.1739344596862793 | G Loss: 1.233332872390747\n",
            "Epoch 50 | D Loss: 1.1240085363388062 | G Loss: 1.2735984325408936\n",
            "Generated Board 1:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 2:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 3:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 4:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 5:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 6:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 7:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 8:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 9:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 10:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 11:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 12:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 13:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 14:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 15:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 16:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 17:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 18:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 19:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 20:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 21:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 22:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 23:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 24:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 25:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 26:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 27:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 28:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 29:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 30:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 31:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 32:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 33:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 34:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 35:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 36:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 37:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 38:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 39:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 40:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 41:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 42:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 43:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 44:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 45:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 46:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 47:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 48:\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 49:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Generated Board 50:\n",
            "[[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Assuming a simple feed-forward architecture for demonstration\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Tanh()  # Using Tanh to restrict output to range [-1, 1]; adapt as necessary\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        return self.model(img_flat)\n",
        "\n",
        "# Define the Minesweeper Dataset\n",
        "class MinesweeperDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, board_size=10):\n",
        "        self.samples = torch.randint(0, 2, (num_samples, board_size * board_size)).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# Training function\n",
        "def train_gan(generator, discriminator, dataset, latent_dim, epochs=50, batch_size=64):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    loss_function = nn.BCELoss()\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for real_samples in dataloader:\n",
        "            real_samples = real_samples.to(device)\n",
        "            real_labels = torch.ones((real_samples.size(0), 1), device=device)\n",
        "            fake_labels = torch.zeros((real_samples.size(0), 1), device=device)\n",
        "\n",
        "            # Update discriminator with real samples\n",
        "            discriminator.zero_grad()\n",
        "            real_predictions = discriminator(real_samples)\n",
        "            real_loss = loss_function(real_predictions, real_labels)\n",
        "\n",
        "            # Generate fake samples\n",
        "            noise = torch.randn((real_samples.size(0), latent_dim), device=device)\n",
        "            fake_samples = generator(noise)\n",
        "            fake_predictions = discriminator(fake_samples.detach())  # Detach to avoid second backward pass\n",
        "            fake_loss = loss_function(fake_predictions, fake_labels)\n",
        "            d_loss = real_loss + fake_loss\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Update generator\n",
        "            generator.zero_grad()\n",
        "            fake_predictions = discriminator(fake_samples)  # No detach here, we need gradients for G\n",
        "            g_loss = loss_function(fake_predictions, real_labels)\n",
        "            g_loss.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            print(f'Epoch {epoch + 1} | D Loss: {d_loss.item()} | G Loss: {g_loss.item()}')\n",
        "\n",
        "def generate_board(batch_size, latent_dim, generator, device):\n",
        "    # Generate noise vector\n",
        "    noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "    # Generate fake samples\n",
        "    with torch.no_grad():\n",
        "        generator.eval()\n",
        "        fake_samples = generator(noise)\n",
        "        generator.train()\n",
        "    # Convert outputs to 0s and 1s for Minesweeper boards\n",
        "    fake_samples = torch.tanh(fake_samples)\n",
        "    fake_samples = (fake_samples + 1) / 2  # Normalize to range [0, 1]\n",
        "    fake_samples = torch.round(fake_samples)  # Convert to 0 or 1\n",
        "    return fake_samples.view(batch_size, board_size, board_size).cpu().numpy()\n",
        "\n",
        "# Example usage\n",
        "board_size = 10  # Example board size\n",
        "output_dim = board_size * board_size  # 10x10 board\n",
        "generator = Generator(latent_dim=100, output_dim=output_dim)\n",
        "discriminator = Discriminator(input_dim=output_dim)\n",
        "\n",
        "# Example usage with the training function\n",
        "dataset = MinesweeperDataset(num_samples=1000, board_size=board_size)  # Ensure this matches the generator output\n",
        "train_gan(generator, discriminator, dataset, latent_dim=100, epochs=50)\n",
        "\n",
        "# Generate and print some boards\n",
        "generated_boards = generate_board(50, 100, generator, device)  # Generate 50 boards\n",
        "\n",
        "for i, board in enumerate(generated_boards, 1):\n",
        "    print(f\"Generated Board {i}:\\n{board}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27a546c7-f473-4728-8758-e78be596557b",
      "metadata": {
        "id": "27a546c7-f473-4728-8758-e78be596557b"
      },
      "source": [
        "**Bonus Explanation:**\n",
        "\n",
        "To approach the problem of generating good Minesweeper boards to play on, I would use a Generative Adversarial Network (GAN). This network would have two parts: one part that tries to create new Minesweeper boards (the generator) and another part that judges if the boards look like real, playable Minesweeper boards (the discriminator).\n",
        "\n",
        "**In particular, how could a neural network learn to generate minesweeper boards that your previous bots perform well on?**\n",
        "\n",
        "I would train this network by teaching the discriminator to tell the difference between boards made by the generator and real game boards that follow Minesweeper rules. The generator would then learn from the feedback of the discriminator to improve its board creation.\n",
        "\n",
        "**I’ll expand on this more when we talk about generative models, but for now - how could you construct a model that outputs minesweeper boards?**\n",
        "\n",
        "For constructing a model that outputs Minesweeper boards, I'd start by setting up the generator to produce a grid of numbers, where each number represents either an open cell or a mine. The discriminator would then assess these boards against known good examples to ensure the boards are valid according to Minesweeper rules.\n",
        "\n",
        "**How could you evaluate how good that model is?**\n",
        "\n",
        "To evaluate how good this model is, I would check:\n",
        "\n",
        "    • Playability: Are all generated boards valid according to Minesweeper rules?\n",
        "    • Difficulty and Engagement: Are the boards challenging but not impossible, offering a good mix of risk and strategy?\n",
        "    • Bot Performance: How well do previously developed Minesweeper bots perform on these generated boards? If the bots perform better on these than on randomly generated boards, it suggests the model is effective.\n",
        "\n",
        "**how could you approach the problem of generating good boards to play on?**\n",
        "\n",
        "This approach would help me create a model that not only produces new and interesting Minesweeper boards but also tailors them to improve the performance of specific bots, potentially making the game more enjoyable or challenging for other players as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b54c5f3-58a7-4087-94fa-35f071b7cea6",
      "metadata": {
        "id": "4b54c5f3-58a7-4087-94fa-35f071b7cea6"
      },
      "source": [
        "#                                                                                            Writeup and Requirements:\n",
        "\n",
        "***How I Structured My Models***\n",
        "\n",
        "**Logic Bot:** I created a basic Minesweeper solver that uses logic to figure out safe moves. It looks at the numbers on the board to decide where mines are and where it's safe to click next.\n",
        "\n",
        "**Convolutional Neural Network (CNN):** For Task 1, I made a model that sees the game board as layers of information, like which spots are revealed, where the flags are, and what numbers are shown. It predicts which cells are safe to open.\n",
        "\n",
        "**Variable Mines Model**: In Task 2, I adjusted the model to work with different numbers of mines. This made it better at handling games that vary in difficulty.\n",
        "\n",
        "**Fully Convolutional Network:** For Task 3, I designed a model that can work with any size of the game board without needing to change its structure, using layers that adapt to the input size.\n",
        "\n",
        "**Generative Adversarial Network (GAN):** In the Bonus Task, I used a network with two parts: one to create new Minesweeper boards (generator) and another to judge if they're good to play on (discriminator).\n",
        "\n",
        "**How I Generated and Learned from Data**\n",
        "\n",
        "    • I made datasets that mimic different game situations, with boards of varying sizes and mine counts.\n",
        "    • To help my models learn better, I used techniques like adjusting data to a standard format and increasing training variety by adding different board configurations.\n",
        "    • To avoid models just memorizing the data (overfitting), I added dropout techniques in their design and used separate data sets to check how well they generalize.\n",
        "\n",
        "**Improvements and Reducing Overfitting**\n",
        "\n",
        "    • I introduced dropout and regularization in the model designs to prevent overfitting.\n",
        "    • I frequently checked the models' performance on separate validation data to ensure they work well in general, not just on the data they were trained on.\n",
        "    • I experimented with different settings to find the best values for learning rates and network sizes.\n",
        "\n",
        "**Comparing Performance**\n",
        "\n",
        "    • The neural network models usually did better than the logic bot, especially in situations where simple logic wasn’t enough.\n",
        "    • The fully convolutional model from Task 3 could handle any board size, which was a big improvement over the logic bot.\n",
        "    • The GAN model from the bonus task helped create new board configurations, adding variety and new challenges to the game.\n",
        "\n",
        "**Insights from Experiments**\n",
        "\n",
        "    • Sequential vs. Current State: The models that looked at the game board as it is at one moment showed that you might not need to track the whole game history, although knowing the sequence of moves could still add some strategic depth.\n",
        "    • Using Attention: Adding attention mechanisms could help the model focus on important parts of the board, which might be especially useful in tricky situations or larger boards.\n",
        "\n",
        "**Future Directions and Improvements**\n",
        "\n",
        "    • Better Sequential Processing: I could add layers that remember past actions (like RNN or LSTM) to see if understanding the sequence of moves in a game helps make better predictions.\n",
        "    • Incorporating Attention Mechanisms: This could help highlight important features and areas on the board, improving decision-making, especially where there are many mines or the board is big.\n",
        "    • Enhancing Board Generation: I'd like to improve how the GAN creates boards, making them not just playable but also strategically interesting, which could help train better models and offer players more challenging games.\n",
        "\n",
        "This summary covers everything from how I built and trained the models to how they performed and what I learned from them. It outlines the steps I took to create a sophisticated Minesweeper AI."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9623147a-1ecf-45a7-9ed8-3864712fad76",
        "4f0a3408-6b77-4948-8be8-2d8efc08936a",
        "12841d99-003e-40b5-b89e-fdf3f0311390",
        "845f913f-b9a5-49ad-a271-92cf4e0c809a",
        "1443ecf5-464a-4ed4-8be5-55246e86a065"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}